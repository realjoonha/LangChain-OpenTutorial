{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "635d8ebb",
      "metadata": {},
      "source": [
        "# LangGraph : Research Assistant with STORM\n",
        "\n",
        "- Author: [Secludor](https://github.com/Secludor)\n",
        "- Design: [LeeYuChul](https://github.com/LeeYuChul)\n",
        "- Peer Review: \n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "Research is often a labor-intensive task delegated to analysts, but AI holds tremendous potential to revolutionize this process. This tutorial explores **how to construct a customized AI-powered research and report generation workflow** using `LangGraph`, incorporating key concepts from Stanford's STORM framework.\n",
        "\n",
        "### Why This Approach?\n",
        "The STORM methodology has demonstrated significant improvements in research quality through two key innovations:\n",
        "- Outline creation through querying similar topics enhances coverage.\n",
        "- Multi-perspective conversation simulation increases reference usage and information density.\n",
        "\n",
        "The translation is accurate but can be enhanced for better clarity and technical precision. Here's the reviewed version:\n",
        "\n",
        "### Key Components\n",
        "\n",
        "**Core Themes**\n",
        "- Memory: State management and persistence across interactions.\n",
        "- Human-in-the-loop: Interactive feedback and validation mechanisms.\n",
        "- Controllability: Fine-grained control over agent workflows.\n",
        "\n",
        "**Research Framework**\n",
        "- Research Automation Objective: Building customized research processes tailored to user requirements.\n",
        "- Source Management: Strategic selection and integration of research input sources.\n",
        "- Planning Framework: Topic definition and AI analyst team assembly.\n",
        "\n",
        "### Process Implementation\n",
        "\n",
        "**Execution Flow**\n",
        "- LLM Integration: Conducting comprehensive expert AI interviews.\n",
        "- Parallel Processing: Simultaneous information gathering and interview execution.\n",
        "- Output Synthesis: Integration of research findings into comprehensive reports.\n",
        "\n",
        "**Technical Implementation**\n",
        "- Environment Setup: Configuration of runtime environment and API authentication.\n",
        "- Analyst Development: Human-supervised analyst creation and validation process.\n",
        "- Interview Management: Systematic question generation and response collection.\n",
        "- Parallel Processing: Implementation of Map-Reduce for interview parallelization.\n",
        "- Report Generation: Structured composition of introductory and concluding sections.\n",
        "\n",
        "\n",
        "AI has significant potential to support these research processes. However, research requires customization. Raw LLM outputs are often not suitable for real decision-making workflows.\n",
        "\n",
        "A customized AI-based [research and report generation](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag) workflow is a promising solution to address this issue.\n",
        "\n",
        "![10-LangGraph-Research-Assitant-concept](./assets/10-LangGraph-Research-Assitant-concept.png)\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Utilities](#utilities)\n",
        "- [Analysts Generation : Human in the Loop](#analysts-generation--human-in-the-loop)\n",
        "- [Interview Execution](#interview-execution)\n",
        "- [Report Writing](#report-writing)\n",
        "\n",
        "### References\n",
        "\n",
        "- [research and report generation](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag)\n",
        "- [LangGraph `Send()`](https://langchain-ai.github.io/langgraph/concepts/low_level/#send)\n",
        "- [LangGraph - Multi-Agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/#custom-multi-agent-workflow)\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c7aba4",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Setting up your environment is the first step. See the [Environment Setup](https://wikidocs.net/257836) guide for more details.\n",
        "\n",
        "\n",
        "**[Note]**\n",
        "\n",
        "The langchain-opentutorial is a package of easy-to-use environment setup guidance, useful functions and utilities for tutorials.\n",
        "Check out the  [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21943adb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f25ec196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain_core\",\n",
        "        \"langchain_community\",\n",
        "        \"langchain_openai\",\n",
        "        \"tavily-python\",\n",
        "        \"arxiv\",\n",
        "        \"pymupdf\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {},
      "source": [
        "You can set API keys in a `.env` file or set them manually.\n",
        "\n",
        "[Note] If you’re not using the `.env` file, no worries! Just enter the keys directly in the cell below, and you’re good to go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4d6fac1c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables have been set successfully.\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "# Attempt to load environment variables from a .env file; if unsuccessful, set them manually.\n",
        "if not load_dotenv():\n",
        "    set_env(\n",
        "        {\n",
        "            \"OPENAI_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "            \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "            \"TAVILY_API_KEY\": \"\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "# set the project name same as the title\n",
        "set_env(\n",
        "    {\n",
        "        \"LANGCHAIN_PROJECT\": \"LangGraph-Research-Assistant\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa00c3f4",
      "metadata": {},
      "source": [
        "## Utilities\n",
        "\n",
        "These are brief descriptions of the modules from `langchain-opentutorial` used for practice."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "652f3f62",
      "metadata": {},
      "source": [
        "### `visualize_graph`\n",
        "for visualizing graph structure"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc51d6cb",
      "metadata": {},
      "source": [
        "### `random_uuid` , `invoke_graph`\n",
        "- `random_uuid` : for generating a random UUID (Universally Unique Identifier) and returns it as a string.\n",
        "- `invoke_graph` : for streaming and displays the results of executing a CompiledStateGraph instance in a visually appealing format. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c683ef59",
      "metadata": {},
      "source": [
        "### `TabilySearch`\n",
        "\n",
        "This code defines a tool for performing search queries using the Tavily Search API. It includes input validation, formatting of search results, and the ability to customize search parameters.\n",
        "**Methods** :\n",
        "- `__init__` : Initializes the TavilySearch instance, setting up the API client and input parameters.\n",
        "- `_run` : Implements the base tool's run method, calling the search method and returning results.\n",
        "- `search` : Performs the actual search using the Tavily API, taking various optional parameters to customize the query. It formats the output based on user preferences.\n",
        "- `get_search_context` : Retrieves relevant context based on a search query, returning a JSON string that includes search results formatted as specified."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "786c076b",
      "metadata": {},
      "source": [
        "## Analysts Generation : Human in the Loop\n",
        "**Analyst Generation** : Create and review analysts using Human-In-The-Loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "188b28b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the model\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "13850b4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_opentutorial.graphs import visualize_graph\n",
        "\n",
        "\n",
        "# Class defining analyst properties and metadata\n",
        "class Analyst(BaseModel):\n",
        "    # Primary affiliation information\n",
        "    affiliation: str = Field(\n",
        "        description=\"Primary affiliation of the analyst.\",\n",
        "    )\n",
        "    # Name\n",
        "    name: str = Field(description=\"Name of the analyst.\")\n",
        "\n",
        "    # Role\n",
        "    role: str = Field(\n",
        "        description=\"Role of the analyst in the context of the topic.\",\n",
        "    )\n",
        "    # Description of focus, concerns, and motives\n",
        "    description: str = Field(\n",
        "        description=\"Description of the analyst focus, concerns, and motives.\",\n",
        "    )\n",
        "\n",
        "    # Property that returns analyst's personal information as a string\n",
        "    @property\n",
        "    def persona(self) -> str:\n",
        "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
        "\n",
        "\n",
        "# Collection of analysts\n",
        "class Perspectives(BaseModel):\n",
        "    # List of analysts\n",
        "    analysts: List[Analyst] = Field(\n",
        "        description=\"Comprehensive list of analysts with their roles and affiliations.\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "713f52d5",
      "metadata": {},
      "source": [
        "The following defines the state that tracks the collection of analysts generated through the Analyst class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "15e0b45b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# State Definition\n",
        "class GenerateAnalystsState(TypedDict):\n",
        "    # Research topic\n",
        "    topic: str\n",
        "    # Maximum number of analysts to generate\n",
        "    max_analysts: int\n",
        "    # Human feedback\n",
        "    human_analyst_feedback: str\n",
        "    # List of analysts\n",
        "    analysts: List[Analyst]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0d517e",
      "metadata": {},
      "source": [
        "### Defining the Analyst Generation Node\n",
        "\n",
        "Next, we will define the analyst generation node.\n",
        "\n",
        "The code below implements the logic for generating various analysts based on the provided research topic. Each analyst has a unique role and affiliation, offering professional perspectives on the topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1ec68f06",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import END\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Analyst generation prompt\n",
        "analyst_instructions = \"\"\"You are tasked with creating a set of AI analyst personas. \n",
        "\n",
        "Follow these instructions carefully:\n",
        "1. First, review the research topic:\n",
        "\n",
        "{topic}\n",
        "        \n",
        "2. Examine any editorial feedback that has been optionally provided to guide the creation of the analysts: \n",
        "        \n",
        "{human_analyst_feedback}\n",
        "    \n",
        "3. Determine the most interesting themes based upon documents and/or feedback above.\n",
        "                    \n",
        "4. Pick the top {max_analysts} themes.\n",
        "\n",
        "5. Assign one analyst to each theme.\"\"\"\n",
        "\n",
        "\n",
        "# Analyst generation node\n",
        "def create_analysts(state: GenerateAnalystsState):\n",
        "    \"\"\"Function to create analyst personas\"\"\"\n",
        "\n",
        "    topic = state[\"topic\"]\n",
        "    max_analysts = state[\"max_analysts\"]\n",
        "    human_analyst_feedback = state.get(\"human_analyst_feedback\", \"\")\n",
        "\n",
        "    # Apply structured output format to LLM\n",
        "    structured_llm = llm.with_structured_output(Perspectives)\n",
        "\n",
        "    # Construct system prompt for analyst creation\n",
        "    system_message = analyst_instructions.format(\n",
        "        topic=topic,\n",
        "        human_analyst_feedback=human_analyst_feedback,\n",
        "        max_analysts=max_analysts,\n",
        "    )\n",
        "\n",
        "    # Call LLM to generate analyst personas\n",
        "    analysts = structured_llm.invoke(\n",
        "        [SystemMessage(content=system_message)]\n",
        "        + [HumanMessage(content=\"Generate the set of analysts.\")]\n",
        "    )\n",
        "\n",
        "    # Store generated list of analysts in state\n",
        "    return {\"analysts\": analysts.analysts}\n",
        "\n",
        "\n",
        "# User feedback node (can be left empty since it will update state)\n",
        "def human_feedback(state: GenerateAnalystsState):\n",
        "    \"\"\"A checkpoint node for receiving user feedback\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "# Function to decide the next step in the workflow based on human feedback\n",
        "def should_continue(state: GenerateAnalystsState):\n",
        "    \"\"\"Function to determine the next step in the workflow\"\"\"\n",
        "\n",
        "    human_analyst_feedback = state.get(\"human_analyst_feedback\", None)\n",
        "    if human_analyst_feedback:\n",
        "        return \"create_analysts\"\n",
        "\n",
        "    return END"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb884f7",
      "metadata": {},
      "source": [
        "**Explanation of Code Components**\n",
        "\n",
        "- **Analyst Instructions**: A prompt that guides the LLM in creating AI analyst personas based on a specified research topic and any provided feedback.\n",
        "\n",
        "- **create_analysts Function**: This function is responsible for generating a set of analysts based on the current state, which includes the research topic, maximum number of analysts, and any human feedback.\n",
        "\n",
        "- **human_feedback Function**: A placeholder function that can be expanded to handle user feedback.\n",
        "\n",
        "- **should_continue Function**: This function evaluates whether there is human feedback available and determines whether to proceed with creating analysts or end the process. \n",
        "\n",
        "This structure allows for a dynamic approach to generating tailored analyst personas that can provide diverse insights into a given research topic."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77452ab5",
      "metadata": {},
      "source": [
        "### Building the Analyst Generation Graph\n",
        "\n",
        "Now we'll create the analyst generation graph that orchestrates the research workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a903259c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langgraph.graph import START, END, StateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Create graph\n",
        "builder = StateGraph(GenerateAnalystsState)\n",
        "\n",
        "# Add nodes\n",
        "builder.add_node(\"create_analysts\", create_analysts)\n",
        "builder.add_node(\"human_feedback\", human_feedback)\n",
        "\n",
        "# Connect edges\n",
        "builder.add_edge(START, \"create_analysts\")\n",
        "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
        "\n",
        "# Add conditional edge: return to analyst creation node if human feedback exists\n",
        "builder.add_conditional_edges(\n",
        "    \"human_feedback\", should_continue, [\"create_analysts\", END]\n",
        ")\n",
        "\n",
        "# Create memory\n",
        "memory = MemorySaver()\n",
        "\n",
        "# Compile graph (set breakpoints)\n",
        "graph = builder.compile(interrupt_before=[\"human_feedback\"], checkpointer=memory)\n",
        "\n",
        "# Visualize graph\n",
        "visualize_graph(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2299a1d0",
      "metadata": {},
      "source": [
        "**Graph Components**\n",
        "\n",
        "- **Nodes**\n",
        "    - `create_analysts`: Generates analyst personas based on the research topic\n",
        "    - `human_feedback`: Checkpoint for receiving user input and feedback\n",
        "\n",
        "- **Edges**\n",
        "    - Initial flow from START to analyst creation\n",
        "    - Connection from analyst creation to human feedback\n",
        "    - Conditional path back to analyst creation based on feedback\n",
        "\n",
        "- **Features**\n",
        "    - Memory persistence using `MemorySaver`\n",
        "    - Breakpoints before human feedback collection\n",
        "    - Visual representation of workflow through `visualize_graph`\n",
        "\n",
        "This graph structure enables an iterative research process with human oversight and feedback integration."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b57868ee",
      "metadata": {},
      "source": [
        "### Running the Analyst Generation Graph\n",
        "Here's how to execute and manage the analyst generation workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "823b806f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "affiliation='Tech Research Institute' name='Dr. Emily Carter' role='Senior Research Scientist' description='Dr. Carter focuses on the architectural differences between Modular RAG and Naive RAG, with an emphasis on understanding how modularity impacts flexibility and scalability in AI systems.'\n",
            "affiliation='AI Development Group' name='Michael Nguyen' role='Lead AI Engineer' description=\"Michael's primary concern is the practical benefits of implementing Modular RAG at the production level, particularly in terms of efficiency, resource management, and ease of integration.\"\n",
            "affiliation='University of Advanced Computing' name='Prof. Laura Chen' role='Professor of Computer Science' description='Prof. Chen studies the theoretical implications of using Modular versus Naive RAG, analyzing the long-term benefits and potential challenges faced in deploying these systems in real-world scenarios.'\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableConfig\n",
        "from langchain_opentutorial.messages import random_uuid, invoke_graph\n",
        "\n",
        "\n",
        "# Configure graph execution settings\n",
        "\n",
        "\n",
        "config = RunnableConfig(\n",
        "    recursion_limit=10,\n",
        "    configurable={\"thread_id\": random_uuid()},\n",
        ")\n",
        "\n",
        "\n",
        "# Set number of analysts\n",
        "\n",
        "\n",
        "max_analysts = 3\n",
        "\n",
        "\n",
        "# Define research topic\n",
        "\n",
        "\n",
        "topic = \"What are the differences between Modular RAG and Naive RAG, and what are the benefits of using it at the production level\"\n",
        "\n",
        "\n",
        "# Configure input parameters\n",
        "\n",
        "\n",
        "inputs = {\n",
        "    \"topic\": topic,\n",
        "    \"max_analysts\": max_analysts,\n",
        "}\n",
        "\n",
        "\n",
        "# Execute graph\n",
        "\n",
        "\n",
        "invoke_graph(graph, inputs, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd057565",
      "metadata": {},
      "source": [
        "When `__interrupt__` is displayed, the system is ready to receive human feedback. At this point, you can retrieve the current state and provide feedback to guide the analyst generation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b6046767",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('human_feedback',)\n"
          ]
        }
      ],
      "source": [
        "# Get current graph state\n",
        "state = graph.get_state(config)\n",
        "\n",
        "# Check next node\n",
        "print(state.next)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a5981af",
      "metadata": {},
      "source": [
        "To inject human feedback into the graph, we use the `update_state` method with the following key components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bd46b1e2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '77b5f1cc-b454-4a81-9d4d-32949e109a66',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1efd9c73-576d-6e75-8002-4bdd66b12cff'}}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Update graph state with human feedback\n",
        "graph.update_state(\n",
        "    config,\n",
        "    {\n",
        "        \"human_analyst_feedback\": \"Add in someone named Teddy Lee from a startup to add an entrepreneur perspective\"\n",
        "    },\n",
        "    as_node=\"human_feedback\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "346b11de",
      "metadata": {},
      "source": [
        "**Key Parameters**\n",
        "- `config` : Configuration object containing graph settings\n",
        "- `human_analyst_feedback` : Key for storing feedback content\n",
        "- `as_node` : Specifies the node that will process the feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b12bf66a",
      "metadata": {},
      "source": [
        "**[Note]** : Assigning `None` as input triggers the graph to continue its execution from the last checkpoint. This is particularly useful when you want to resume processing after providing human feedback."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e12a3a25",
      "metadata": {},
      "source": [
        "(Continue) To resume the graph execution after the `__interrupt__` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f0cf45e6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "affiliation='AI Technology Research Institute' name='Dr. Emily Zhang' role='AI Researcher' description='Dr. Zhang focuses on the technical distinctions between Modular RAG and Naive RAG, analyzing their architectural differences, computational efficiencies, and adaptability in various AI applications. Her interest lies in understanding how these models can be optimized for better performance and scalability.'\n",
            "affiliation='Tech Innovations Startup' name='Teddy Lee' role='Entrepreneur' description='Teddy Lee provides insights into the practical implications and business benefits of implementing Modular RAG over Naive RAG at the production level. He evaluates cost-efficiency, scalability, and the potential for innovative applications in startup environments, aiming to leverage cutting-edge technology for competitive advantage.'\n",
            "affiliation='Enterprise Solutions Inc.' name='Sophia Martinez' role='Industry Analyst' description='Sophia Martinez explores the benefits of Modular RAG in large-scale enterprise settings. Her focus is on the reliability, integration capabilities, and long-term strategic advantages of adopting Modular RAG systems over Naive RAG in production environments, emphasizing risk management and return on investment.'\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Continue execution\n",
        "invoke_graph(graph, None, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52730608",
      "metadata": {},
      "source": [
        "When `__interrupt__` appears again, you have two options: \n",
        "\n",
        "- Option 1: Provide Additional Feedback\n",
        "    - You can provide more feedback to further refine the analyst personas using the same method as before\n",
        "- Option 2: Complete the Process\n",
        "\n",
        "To finish the analyst generation process without additional feedback:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8a914e38",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '77b5f1cc-b454-4a81-9d4d-32949e109a66',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1efd9c73-86e0-67db-8004-1b34f460d397'}}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set feedback input to None to indicate completion\n",
        "human_feedback_input = None\n",
        "\n",
        "# Update graph state with no feedback\n",
        "graph.update_state(\n",
        "    config, {\"human_analyst_feedback\": human_feedback_input}, as_node=\"human_feedback\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7b879519",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Continue final execution\n",
        "invoke_graph(graph, None, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da31ff4",
      "metadata": {},
      "source": [
        "**Displaying Final Results**\n",
        "\n",
        "Get and display the final results from the graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b3623113",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of analysts generated: 3\n",
            "================================\n",
            "Name: Dr. Emily Zhang\n",
            "Role: AI Researcher\n",
            "Affiliation: AI Technology Research Institute\n",
            "Description: Dr. Zhang focuses on the technical distinctions between Modular RAG and Naive RAG, analyzing their architectural differences, computational efficiencies, and adaptability in various AI applications. Her interest lies in understanding how these models can be optimized for better performance and scalability.\n",
            "\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "Name: Teddy Lee\n",
            "Role: Entrepreneur\n",
            "Affiliation: Tech Innovations Startup\n",
            "Description: Teddy Lee provides insights into the practical implications and business benefits of implementing Modular RAG over Naive RAG at the production level. He evaluates cost-efficiency, scalability, and the potential for innovative applications in startup environments, aiming to leverage cutting-edge technology for competitive advantage.\n",
            "\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "Name: Sophia Martinez\n",
            "Role: Industry Analyst\n",
            "Affiliation: Enterprise Solutions Inc.\n",
            "Description: Sophia Martinez explores the benefits of Modular RAG in large-scale enterprise settings. Her focus is on the reliability, integration capabilities, and long-term strategic advantages of adopting Modular RAG systems over Naive RAG in production environments, emphasizing risk management and return on investment.\n",
            "\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "()\n"
          ]
        }
      ],
      "source": [
        "# Get final state\n",
        "final_state = graph.get_state(config)\n",
        "\n",
        "# Get generated analysts\n",
        "analysts = final_state.values.get(\"analysts\")\n",
        "\n",
        "# Print analyst count\n",
        "print(\n",
        "    f\"Number of analysts generated: {len(analysts)}\",\n",
        "    end=\"\\n================================\\n\",\n",
        ")\n",
        "\n",
        "# Print each analyst's persona\n",
        "for analyst in analysts:\n",
        "    print(analyst.persona)\n",
        "    print(\"- \" * 30)\n",
        "\n",
        "# Get next node state (empty tuple indicates completion)\n",
        "print(final_state.next)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b22fd5",
      "metadata": {},
      "source": [
        "**Key Components**\n",
        "- `final_state`: Contains the final state of the graph execution.\n",
        "- `analysts`: List of generated analyst personas.\n",
        "- `final_state.next`: Empty tuple indicating workflow completion.\n",
        "\n",
        "The output will display each analyst's complete persona information, including their name, role, affiliation, and description, followed by a separator line. The empty tuple printed at the end confirms that the graph execution has completed successfully."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2b277b6",
      "metadata": {},
      "source": [
        "## Interview Execution\n",
        "\n",
        "### Define Classes and `question_generation` Node\n",
        "Let's implement the interview execution components with proper state management and `question_generation` Node:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4c00e51f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import operator\n",
        "from typing import Annotated\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "\n",
        "class InterviewState(MessagesState):\n",
        "    \"\"\"State management for interview process\"\"\"\n",
        "\n",
        "    max_num_turns: int\n",
        "    context: Annotated[list, operator.add]  # Context list containing source documents\n",
        "    analyst: Analyst\n",
        "    interview: str  # String storing interview content\n",
        "    sections: list  # List of report sections\n",
        "\n",
        "\n",
        "class SearchQuery(BaseModel):\n",
        "    \"\"\"Data class for search queries\"\"\"\n",
        "\n",
        "    search_query: str = Field(None, description=\"Search query for retrieval.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6ab79654",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_question(state: InterviewState):\n",
        "    \"\"\"Node for generating interview questions\"\"\"\n",
        "\n",
        "    # System prompt for question generation\n",
        "    question_instructions = \"\"\"You are an analyst tasked with interviewing an expert to learn about a specific topic. \n",
        "\n",
        "    Your goal is boil down to interesting and specific insights related to your topic.\n",
        "\n",
        "    1. Interesting: Insights that people will find surprising or non-obvious.\n",
        "            \n",
        "    2. Specific: Insights that avoid generalities and include specific examples from the expert.\n",
        "\n",
        "    Here is your topic of focus and set of goals: {goals}\n",
        "            \n",
        "    Begin by introducing yourself using a name that fits your persona, and then ask your question.\n",
        "\n",
        "    Continue to ask questions to drill down and refine your understanding of the topic.\n",
        "            \n",
        "    When you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\n",
        "\n",
        "    Remember to stay in character throughout your response, reflecting the persona and goals provided to you.\"\"\"\n",
        "\n",
        "    # Extract state components\n",
        "    analyst = state[\"analyst\"]\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # Generate question using LLM\n",
        "    system_message = question_instructions.format(goals=analyst.persona)\n",
        "    question = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
        "\n",
        "    # Return updated messages\n",
        "    return {\"messages\": [question]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6957dd0a",
      "metadata": {},
      "source": [
        "**State Management**\n",
        "- `InterviewState` tracks conversation turns, context, and interview content.\n",
        "- Annotated context list allows for document accumulation.\n",
        "- Maintains analyst persona and report sections.\n",
        "\n",
        "**Question Generation**\n",
        "- Structured system prompt for consistent interviewing style.\n",
        "- Persona-aware questioning based on analyst goals.\n",
        "- Progressive refinement of topic understanding.\n",
        "- Clear interview conclusion mechanism.\n",
        "\n",
        "The code provides a robust foundation for conducting structured interviews while maintaining conversation state and context."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf3a0fa5",
      "metadata": {},
      "source": [
        "### Defining Research Tools\n",
        "\n",
        "Experts collect information in parallel from multiple sources to answer questions.\n",
        "\n",
        "They can utilize various tools such as web document scraping, VectorDB, web search, and Wikipedia search.\n",
        "\n",
        "We'll focus on two main tools: **`Tavily`** for web search and **`ArxivRetriever`** for academic papers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ac99427",
      "metadata": {},
      "source": [
        "`Tavily Search`\n",
        "- Real-time web search capabilities\n",
        "- Configurable result count and content depth\n",
        "- Structured output formatting\n",
        "- Raw content inclusion option"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "79aea221",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_opentutorial.tools.tavily import TavilySearch\n",
        "\n",
        "# Initialize TavilySearch with configuration\n",
        "\n",
        "\n",
        "tavily_search = TavilySearch(max_results=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "522b7832",
      "metadata": {},
      "source": [
        "`ArxivRetriever`\n",
        "- Access to academic papers and research\n",
        "- Full document retrieval\n",
        "- Comprehensive metadata access\n",
        "- Customizable document load limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "16b9cbca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(metadata={'Published': '2024-07-26', 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks', 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang', 'Summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.', 'entry_id': 'http://arxiv.org/abs/2407.21059v1', 'published_first_time': '2024-07-26', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI', 'cs.IR'], 'links': ['http://arxiv.org/abs/2407.21059v1', 'http://arxiv.org/pdf/2407.21059v1']}, page_content='1\\nModular RAG: Transforming RAG Systems into\\nLEGO-like Reconfigurable Frameworks\\nYunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\\nAbstract—Retrieval-augmented\\nGeneration\\n(RAG)\\nhas\\nmarkedly enhanced the capabilities of Large Language Models\\n(LLMs) in tackling knowledge-intensive tasks. The increasing\\ndemands of application scenarios have driven the evolution\\nof RAG, leading to the integration of advanced retrievers,\\nLLMs and other complementary technologies, which in turn\\nhas amplified the intricacy of RAG systems. However, the rapid\\nadvancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process\\nof “retrieve-then-generate”. In this context, this paper examines\\nthe limitations of the existing RAG paradigm and introduces\\nthe modular RAG framework. By decomposing complex RAG\\nsystems into independent modules and specialized operators, it\\nfacilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a\\nmore advanced design that integrates routing, scheduling, and\\nfusion mechanisms. Drawing on extensive research, this paper\\nfurther identifies prevalent RAG patterns—linear, conditional,\\nbranching, and looping—and offers a comprehensive analysis\\nof their respective implementation nuances. Modular RAG\\npresents\\ninnovative\\nopportunities\\nfor\\nthe\\nconceptualization\\nand deployment of RAG systems. Finally, the paper explores\\nthe potential emergence of new operators and paradigms,\\nestablishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment\\nof RAG technologies.\\nIndex Terms—Retrieval-augmented generation, large language\\nmodel, modular system, information retrieval\\nI. INTRODUCTION\\nL\\nARGE Language Models (LLMs) have demonstrated\\nremarkable capabilities, yet they still face numerous\\nchallenges, such as hallucination and the lag in information up-\\ndates [1]. Retrieval-augmented Generation (RAG), by access-\\ning external knowledge bases, provides LLMs with important\\ncontextual information, significantly enhancing their perfor-\\nmance on knowledge-intensive tasks [2]. Currently, RAG, as\\nan enhancement method, has been widely applied in various\\npractical application scenarios, including knowledge question\\nanswering, recommendation systems, customer service, and\\npersonal assistants. [3]–[6]\\nDuring the nascent stages of RAG , its core framework is\\nconstituted by indexing, retrieval, and generation, a paradigm\\nreferred to as Naive RAG [7]. However, as the complexity\\nof tasks and the demands of applications have escalated, the\\nYunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\\nSystems, Tongji University, Shanghai, 201210, China.\\nYun Xiong is with Shanghai Key Laboratory of Data Science, School of\\nComputer Science, Fudan University, Shanghai, 200438, China.\\nMeng Wang and Haofen Wang are with College of Design and Innovation,\\nTongji University, Shanghai, 20092, China. (Corresponding author: Haofen\\nWang. E-mail: carter.whfcarter@gmail.com)\\nlimitations of Naive RAG have become increasingly apparent.\\nAs depicted in Figure 1, it predominantly hinges on the\\nstraightforward similarity of chunks, result in poor perfor-\\nmance when confronted with complex queries and chunks with\\nsubstantial variability. The primary challenges of Naive RAG\\ninclude: 1) Shallow Understanding of Queries. The semantic\\nsimilarity between a query and document chunk is not always\\nhighly consistent. Relying solely on similarity calculations\\nfor retrieval lacks an in-depth exploration of the relationship\\nbetween the query and the document [8]. 2) Retrieval Re-\\ndundancy and Noise. Feeding all retrieved chunks directly\\ninto LLMs is not always beneficial. Research indicates that\\nan excess of redundant and noisy information may interfere\\nwith the LLM’s identification of key information, thereby\\nincreasing the risk of generating erroneous and hallucinated\\nresponses. [9]\\nTo overcome the aforementioned limitations, '), Document(metadata={'Published': '2024-03-27', 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'published_first_time': '2023-12-18', 'comment': 'Ongoing Work', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI'], 'links': ['http://arxiv.org/abs/2312.10997v5', 'http://arxiv.org/pdf/2312.10997v5']}, page_content='1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\\nWangc, and Haofen Wang a,c\\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\ncCollege of Design and Innovation, Tongji University\\nAbstract—Large Language Models (LLMs) showcase impres-\\nsive capabilities but encounter challenges like hallucination,\\noutdated knowledge, and non-transparent, untraceable reasoning\\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\\nas a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the\\ngeneration, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge with the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.\\nIndex Terms—Large language model, retrieval-augmented gen-\\neration, natural language processing, information retrieval\\nI. INTRODUCTION\\nL\\nARGE language models (LLMs) have achieved remark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or knowledge-intensive tasks [1],\\nnotably producing “hallucinations” [2] when handling queries\\nbeyond their training data or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing chatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions.\\nRAG technology has rapidly developed in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources\\nare\\navailable\\nat\\nhttps://github.com/Tongji-KGLLM/\\nRAG-Survey\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage characteristics.\\nInitially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]–[5].The subsequent arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As research\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate more with LLM\\nfine-tuning techniques.\\nThe burgeoning field of RAG has experienced swift growth,\\nyet it has not been accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. Thi'), Document(metadata={'Published': '2024-05-22', 'Title': 'FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research', 'Authors': 'Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou', 'Summary': 'With the advent of Large Language Models (LLMs), the potential of Retrieval\\nAugmented Generation (RAG) techniques have garnered considerable research\\nattention. Numerous novel algorithms and models have been introduced to enhance\\nvarious aspects of RAG systems. However, the absence of a standardized\\nframework for implementation, coupled with the inherently intricate RAG\\nprocess, makes it challenging and time-consuming for researchers to compare and\\nevaluate these approaches in a consistent environment. Existing RAG toolkits\\nlike LangChain and LlamaIndex, while available, are often heavy and unwieldy,\\nfailing to meet the personalized needs of researchers. In response to this\\nchallenge, we propose FlashRAG, an efficient and modular open-source toolkit\\ndesigned to assist researchers in reproducing existing RAG methods and in\\ndeveloping their own RAG algorithms within a unified framework. Our toolkit\\nimplements 12 advanced RAG methods and has gathered and organized 32 benchmark\\ndatasets. Our toolkit has various features, including customizable modular\\nframework, rich collection of pre-implemented RAG works, comprehensive\\ndatasets, efficient auxiliary pre-processing scripts, and extensive and\\nstandard evaluation metrics. Our toolkit and resources are available at\\nhttps://github.com/RUC-NLPIR/FlashRAG.', 'entry_id': 'http://arxiv.org/abs/2405.13576v1', 'published_first_time': '2024-05-22', 'comment': '8 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.IR'], 'links': ['http://arxiv.org/abs/2405.13576v1', 'http://arxiv.org/pdf/2405.13576v1']}, page_content='FlashRAG: A Modular Toolkit for Efficient\\nRetrieval-Augmented Generation Research\\nJiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou∗\\nGaoling School of Artificial Intelligence\\nRenmin University of China\\n{jinjiajie, dou}@ruc.edu.cn, yutaozhu94@gmail.com\\nAbstract\\nWith the advent of Large Language Models (LLMs), the potential of Retrieval\\nAugmented Generation (RAG) techniques have garnered considerable research\\nattention. Numerous novel algorithms and models have been introduced to enhance\\nvarious aspects of RAG systems. However, the absence of a standardized framework\\nfor implementation, coupled with the inherently intricate RAG process, makes it\\nchallenging and time-consuming for researchers to compare and evaluate these\\napproaches in a consistent environment. Existing RAG toolkits like LangChain\\nand LlamaIndex, while available, are often heavy and unwieldy, failing to\\nmeet the personalized needs of researchers. In response to this challenge, we\\npropose FlashRAG, an efficient and modular open-source toolkit designed to assist\\nresearchers in reproducing existing RAG methods and in developing their own\\nRAG algorithms within a unified framework. Our toolkit implements 12 advanced\\nRAG methods and has gathered and organized 32 benchmark datasets. Our toolkit\\nhas various features, including customizable modular framework, rich collection\\nof pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-\\nprocessing scripts, and extensive and standard evaluation metrics. Our toolkit and\\nresources are available at https://github.com/RUC-NLPIR/FlashRAG.\\n1\\nIntroduction\\nIn the era of large language models (LLMs), retrieval-augmented generation (RAG) [1, 2] has\\nemerged as a robust solution to mitigate hallucination issues in LLMs by leveraging external\\nknowledge bases [3]. The substantial applications and the potential of RAG technology have\\nattracted considerable research attention. With the introduction of a large number of new algorithms\\nand models to improve various facets of RAG systems in recent years, comparing and evaluating\\nthese methods under a consistent setting has become increasingly challenging.\\nMany works are not open-source or have fixed settings in their open-source code, making it difficult\\nto adapt to new data or innovative components. Besides, the datasets and retrieval corpus used often\\nvary, with resources being scattered, which can lead researchers to spend excessive time on pre-\\nprocessing steps instead of focusing on optimizing their methods. Furthermore, due to the complexity\\nof RAG systems, involving multiple steps such as indexing, retrieval, and generation, researchers\\noften need to implement many parts of the system themselves. Although there are some existing RAG\\ntoolkits like LangChain [4] and LlamaIndex [5], they are typically large and cumbersome, hindering\\nresearchers from implementing customized processes and failing to address the aforementioned issues.\\n∗Corresponding author\\nPreprint. Under review.\\narXiv:2405.13576v1  [cs.CL]  22 May 2024\\nThus, a unified, researcher-oriented RAG toolkit is urgently needed to streamline methodological\\ndevelopment and comparative studies.\\nTo address the issue mentioned above, we introduce FlashRAG, an open-source library designed to\\nenable researchers to easily reproduce existing RAG methods and develop their own RAG algorithms.\\nThis library allows researchers to utilize built pipelines to replicate existing work, employ provided\\nRAG components to construct their own RAG processes, or simply use organized datasets and corpora\\nto accelerate their own RAG workflow. Compared to existing RAG toolkits, FlashRAG is more suited\\nfor researchers. To summarize, the key features and capabilities of our FlashRAG library can be\\noutlined in the following four aspects:\\nExtensive and Customizable Modular RAG Framework.\\nTo facilitate an easily expandable\\nRAG process, we implemented modular RAG at two levels. At the component level, we offer\\ncomprehensive RAG compon')]\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.retrievers import ArxivRetriever\n",
        "\n",
        "# Initialize ArxivRetriever with configuration\n",
        "arxiv_retriever = ArxivRetriever(\n",
        "    load_max_docs=3, load_all_available_meta=True, get_full_documents=True\n",
        ")\n",
        "\n",
        "# Execute arxiv search and print results\n",
        "arxiv_search_results = arxiv_retriever.invoke(\"Modular RAG vs Naive RAG\")\n",
        "print(arxiv_search_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "731bab0a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Published': '2024-07-26',\n",
              " 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks',\n",
              " 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang',\n",
              " 'Summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.',\n",
              " 'entry_id': 'http://arxiv.org/abs/2407.21059v1',\n",
              " 'published_first_time': '2024-07-26',\n",
              " 'comment': None,\n",
              " 'journal_ref': None,\n",
              " 'doi': None,\n",
              " 'primary_category': 'cs.CL',\n",
              " 'categories': ['cs.CL', 'cs.AI', 'cs.IR'],\n",
              " 'links': ['http://arxiv.org/abs/2407.21059v1',\n",
              "  'http://arxiv.org/pdf/2407.21059v1']}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# metadata of Arxiv\n",
        "arxiv_search_results[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9c4563b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n"
          ]
        }
      ],
      "source": [
        "# content of Arxiv\n",
        "print(arxiv_search_results[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "088a306d",
      "metadata": {},
      "source": [
        " format and display Arxiv search results in a structured XML-like format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4894dbc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 문서 검색 결과를 포맷팅\n",
        "formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
        "    [\n",
        "        f'<Document source=\"{doc.metadata[\"entry_id\"]}\" date=\"{doc.metadata.get(\"Published\", \"\")}\" authors=\"{doc.metadata.get(\"Authors\", \"\")}\"/>\\n<Title>\\n{doc.metadata[\"Title\"]}\\n</Title>\\n\\n<Summary>\\n{doc.metadata[\"Summary\"]}\\n</Summary>\\n\\n<Content>\\n{doc.page_content}\\n</Content>\\n</Document>'\n",
        "        for doc in arxiv_search_results\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "143d6b5e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
            "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
            "increasing demands of application scenarios have driven the evolution of RAG,\n",
            "leading to the integration of advanced retrievers, LLMs and other complementary\n",
            "technologies, which in turn has amplified the intricacy of RAG systems.\n",
            "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process of\n",
            "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
            "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
            "decomposing complex RAG systems into independent modules and specialized\n",
            "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a more advanced\n",
            "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
            "extensive research, this paper further identifies prevalent RAG\n",
            "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
            "analysis of their respective implementation nuances. Modular RAG presents\n",
            "innovative opportunities for the conceptualization and deployment of RAG\n",
            "systems. Finally, the paper explores the potential emergence of new operators\n",
            "and paradigms, establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment of RAG\n",
            "technologies.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2312.10997v5\" date=\"2024-03-27\" authors=\"Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Retrieval-Augmented Generation for Large Language Models: A Survey\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Large Language Models (LLMs) showcase impressive capabilities but encounter\n",
            "challenges like hallucination, outdated knowledge, and non-transparent,\n",
            "untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\n",
            "emerged as a promising solution by incorporating knowledge from external\n",
            "databases. This enhances the accuracy and credibility of the generation,\n",
            "particularly for knowledge-intensive tasks, and allows for continuous knowledge\n",
            "updates and integration of domain-specific information. RAG synergistically\n",
            "merges LLMs' intrinsic knowledge with the vast, dynamic repositories of\n",
            "external databases. This comprehensive review paper offers a detailed\n",
            "examination of the progression of RAG paradigms, encompassing the Naive RAG,\n",
            "the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\n",
            "tripartite foundation of RAG frameworks, which includes the retrieval, the\n",
            "generation and the augmentation techniques. The paper highlights the\n",
            "state-of-the-art technologies embedded in each of these critical components,\n",
            "providing a profound understanding of the advancements in RAG systems.\n",
            "Furthermore, this paper introduces up-to-date evaluation framework and\n",
            "benchmark. At the end, this article delineates the challenges currently faced\n",
            "and points out prospective avenues for research and development.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Retrieval-Augmented Generation for Large\n",
            "Language Models: A Survey\n",
            "Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\n",
            "Wangc, and Haofen Wang a,c\n",
            "aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n",
            "bShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n",
            "cCollege of Design and Innovation, Tongji University\n",
            "Abstract—Large Language Models (LLMs) showcase impres-\n",
            "sive capabilities but encounter challenges like hallucination,\n",
            "outdated knowledge, and non-transparent, untraceable reasoning\n",
            "processes. Retrieval-Augmented Generation (RAG) has emerged\n",
            "as a promising solution by incorporating knowledge from external\n",
            "databases. This enhances the accuracy and credibility of the\n",
            "generation, particularly for knowledge-intensive tasks, and allows\n",
            "for continuous knowledge updates and integration of domain-\n",
            "specific information. RAG synergistically merges LLMs’ intrin-\n",
            "sic knowledge with the vast, dynamic repositories of external\n",
            "databases. This comprehensive review paper offers a detailed\n",
            "examination of the progression of RAG paradigms, encompassing\n",
            "the Naive RAG, the Advanced RAG, and the Modular RAG.\n",
            "It meticulously scrutinizes the tripartite foundation of RAG\n",
            "frameworks, which includes the retrieval, the generation and the\n",
            "augmentation techniques. The paper highlights the state-of-the-\n",
            "art technologies embedded in each of these critical components,\n",
            "providing a profound understanding of the advancements in RAG\n",
            "systems. Furthermore, this paper introduces up-to-date evalua-\n",
            "tion framework and benchmark. At the end, this article delineates\n",
            "the challenges currently faced and points out prospective avenues\n",
            "for research and development 1.\n",
            "Index Terms—Large language model, retrieval-augmented gen-\n",
            "eration, natural language processing, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE language models (LLMs) have achieved remark-\n",
            "able success, though they still face significant limitations,\n",
            "especially in domain-specific or knowledge-intensive tasks [1],\n",
            "notably producing “hallucinations” [2] when handling queries\n",
            "beyond their training data or requiring current information. To\n",
            "overcome challenges, Retrieval-Augmented Generation (RAG)\n",
            "enhances LLMs by retrieving relevant document chunks from\n",
            "external knowledge base through semantic similarity calcu-\n",
            "lation. By referencing external knowledge, RAG effectively\n",
            "reduces the problem of generating factually incorrect content.\n",
            "Its integration into LLMs has resulted in widespread adoption,\n",
            "establishing RAG as a key technology in advancing chatbots\n",
            "and enhancing the suitability of LLMs for real-world applica-\n",
            "tions.\n",
            "RAG technology has rapidly developed in recent years, and\n",
            "the technology tree summarizing related research is shown\n",
            "Corresponding Author.Email:haofen.wang@tongji.edu.cn\n",
            "1Resources\n",
            "are\n",
            "available\n",
            "at\n",
            "https://github.com/Tongji-KGLLM/\n",
            "RAG-Survey\n",
            "in Figure 1. The development trajectory of RAG in the era\n",
            "of large models exhibits several distinct stage characteristics.\n",
            "Initially, RAG’s inception coincided with the rise of the\n",
            "Transformer architecture, focusing on enhancing language\n",
            "models by incorporating additional knowledge through Pre-\n",
            "Training Models (PTM). This early stage was characterized\n",
            "by foundational work aimed at refining pre-training techniques\n",
            "[3]–[5].The subsequent arrival of ChatGPT [6] marked a\n",
            "pivotal moment, with LLM demonstrating powerful in context\n",
            "learning (ICL) capabilities. RAG research shifted towards\n",
            "providing better information for LLMs to answer more com-\n",
            "plex and knowledge-intensive tasks during the inference stage,\n",
            "leading to rapid development in RAG studies. As research\n",
            "progressed, the enhancement of RAG was no longer limited\n",
            "to the inference stage but began to incorporate more with LLM\n",
            "fine-tuning techniques.\n",
            "The burgeoning field of RAG has experienced swift growth,\n",
            "yet it has not been accompanied by a systematic synthesis that\n",
            "could clarify its broader trajectory. Thi\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2405.13576v1\" date=\"2024-05-22\" authors=\"Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou\"/>\n",
            "<Title>\n",
            "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "With the advent of Large Language Models (LLMs), the potential of Retrieval\n",
            "Augmented Generation (RAG) techniques have garnered considerable research\n",
            "attention. Numerous novel algorithms and models have been introduced to enhance\n",
            "various aspects of RAG systems. However, the absence of a standardized\n",
            "framework for implementation, coupled with the inherently intricate RAG\n",
            "process, makes it challenging and time-consuming for researchers to compare and\n",
            "evaluate these approaches in a consistent environment. Existing RAG toolkits\n",
            "like LangChain and LlamaIndex, while available, are often heavy and unwieldy,\n",
            "failing to meet the personalized needs of researchers. In response to this\n",
            "challenge, we propose FlashRAG, an efficient and modular open-source toolkit\n",
            "designed to assist researchers in reproducing existing RAG methods and in\n",
            "developing their own RAG algorithms within a unified framework. Our toolkit\n",
            "implements 12 advanced RAG methods and has gathered and organized 32 benchmark\n",
            "datasets. Our toolkit has various features, including customizable modular\n",
            "framework, rich collection of pre-implemented RAG works, comprehensive\n",
            "datasets, efficient auxiliary pre-processing scripts, and extensive and\n",
            "standard evaluation metrics. Our toolkit and resources are available at\n",
            "https://github.com/RUC-NLPIR/FlashRAG.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "FlashRAG: A Modular Toolkit for Efficient\n",
            "Retrieval-Augmented Generation Research\n",
            "Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou∗\n",
            "Gaoling School of Artificial Intelligence\n",
            "Renmin University of China\n",
            "{jinjiajie, dou}@ruc.edu.cn, yutaozhu94@gmail.com\n",
            "Abstract\n",
            "With the advent of Large Language Models (LLMs), the potential of Retrieval\n",
            "Augmented Generation (RAG) techniques have garnered considerable research\n",
            "attention. Numerous novel algorithms and models have been introduced to enhance\n",
            "various aspects of RAG systems. However, the absence of a standardized framework\n",
            "for implementation, coupled with the inherently intricate RAG process, makes it\n",
            "challenging and time-consuming for researchers to compare and evaluate these\n",
            "approaches in a consistent environment. Existing RAG toolkits like LangChain\n",
            "and LlamaIndex, while available, are often heavy and unwieldy, failing to\n",
            "meet the personalized needs of researchers. In response to this challenge, we\n",
            "propose FlashRAG, an efficient and modular open-source toolkit designed to assist\n",
            "researchers in reproducing existing RAG methods and in developing their own\n",
            "RAG algorithms within a unified framework. Our toolkit implements 12 advanced\n",
            "RAG methods and has gathered and organized 32 benchmark datasets. Our toolkit\n",
            "has various features, including customizable modular framework, rich collection\n",
            "of pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-\n",
            "processing scripts, and extensive and standard evaluation metrics. Our toolkit and\n",
            "resources are available at https://github.com/RUC-NLPIR/FlashRAG.\n",
            "1\n",
            "Introduction\n",
            "In the era of large language models (LLMs), retrieval-augmented generation (RAG) [1, 2] has\n",
            "emerged as a robust solution to mitigate hallucination issues in LLMs by leveraging external\n",
            "knowledge bases [3]. The substantial applications and the potential of RAG technology have\n",
            "attracted considerable research attention. With the introduction of a large number of new algorithms\n",
            "and models to improve various facets of RAG systems in recent years, comparing and evaluating\n",
            "these methods under a consistent setting has become increasingly challenging.\n",
            "Many works are not open-source or have fixed settings in their open-source code, making it difficult\n",
            "to adapt to new data or innovative components. Besides, the datasets and retrieval corpus used often\n",
            "vary, with resources being scattered, which can lead researchers to spend excessive time on pre-\n",
            "processing steps instead of focusing on optimizing their methods. Furthermore, due to the complexity\n",
            "of RAG systems, involving multiple steps such as indexing, retrieval, and generation, researchers\n",
            "often need to implement many parts of the system themselves. Although there are some existing RAG\n",
            "toolkits like LangChain [4] and LlamaIndex [5], they are typically large and cumbersome, hindering\n",
            "researchers from implementing customized processes and failing to address the aforementioned issues.\n",
            "∗Corresponding author\n",
            "Preprint. Under review.\n",
            "arXiv:2405.13576v1  [cs.CL]  22 May 2024\n",
            "Thus, a unified, researcher-oriented RAG toolkit is urgently needed to streamline methodological\n",
            "development and comparative studies.\n",
            "To address the issue mentioned above, we introduce FlashRAG, an open-source library designed to\n",
            "enable researchers to easily reproduce existing RAG methods and develop their own RAG algorithms.\n",
            "This library allows researchers to utilize built pipelines to replicate existing work, employ provided\n",
            "RAG components to construct their own RAG processes, or simply use organized datasets and corpora\n",
            "to accelerate their own RAG workflow. Compared to existing RAG toolkits, FlashRAG is more suited\n",
            "for researchers. To summarize, the key features and capabilities of our FlashRAG library can be\n",
            "outlined in the following four aspects:\n",
            "Extensive and Customizable Modular RAG Framework.\n",
            "To facilitate an easily expandable\n",
            "RAG process, we implemented modular RAG at two levels. At the component level, we offer\n",
            "comprehensive RAG compon\n",
            "</Content>\n",
            "</Document>\n"
          ]
        }
      ],
      "source": [
        "print(formatted_search_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "678568f9",
      "metadata": {},
      "source": [
        "### Defining Search Tool Nodes\n",
        "\n",
        "The code implements two main search tool nodes for gathering research information: web search via `Tavily` and academic paper search via `ArXiv`. Here's a breakdown of the key components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "621173b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "# Search query instruction\n",
        "search_instructions = SystemMessage(\n",
        "    content=f\"\"\"You will be given a conversation between an analyst and an expert. \n",
        "\n",
        "Your goal is to generate a well-structured query for use in retrieval and / or web-search related to the conversation.\n",
        "        \n",
        "First, analyze the full conversation.\n",
        "\n",
        "Pay particular attention to the final question posed by the analyst.\n",
        "\n",
        "Convert this final question into a well-structured web search query\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "def search_web(state: InterviewState):\n",
        "    \"\"\"Performs web search using Tavily\"\"\"\n",
        "\n",
        "    # Generate search query\n",
        "    structured_llm = llm.with_structured_output(SearchQuery)\n",
        "    search_query = structured_llm.invoke([search_instructions] + state[\"messages\"])\n",
        "\n",
        "    # Execute search\n",
        "    search_docs = tavily_search.invoke(search_query.search_query)\n",
        "\n",
        "    # Format results - handle both string and dict responses\n",
        "    if isinstance(search_docs, list):\n",
        "        formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
        "            [\n",
        "                f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
        "                for doc in search_docs\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    return {\"context\": [formatted_search_docs]}\n",
        "\n",
        "\n",
        "def search_arxiv(state: InterviewState):\n",
        "    \"\"\"Performs academic paper search using ArXiv\"\"\"\n",
        "\n",
        "    # Generate search query\n",
        "    structured_llm = llm.with_structured_output(SearchQuery)\n",
        "    search_query = structured_llm.invoke([search_instructions] + state[\"messages\"])\n",
        "\n",
        "    try:\n",
        "        # Execute search\n",
        "        arxiv_search_results = arxiv_retriever.invoke(\n",
        "            search_query.search_query,\n",
        "            load_max_docs=2,\n",
        "            load_all_available_meta=True,\n",
        "            get_full_documents=True,\n",
        "        )\n",
        "\n",
        "        # Format results\n",
        "        formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
        "            [\n",
        "                f'<Document source=\"{doc.metadata[\"entry_id\"]}\" date=\"{doc.metadata.get(\"Published\", \"\")}\" authors=\"{doc.metadata.get(\"Authors\", \"\")}\"/>\\n<Title>\\n{doc.metadata[\"Title\"]}\\n</Title>\\n\\n<Summary>\\n{doc.metadata[\"Summary\"]}\\n</Summary>\\n\\n<Content>\\n{doc.page_content}\\n</Content>\\n</Document>'\n",
        "                for doc in arxiv_search_results\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return {\"context\": [formatted_search_docs]}\n",
        "    except Exception as e:\n",
        "        print(f\"ArXiv search error: {str(e)}\")\n",
        "        return {\"context\": [\"<Error>Failed to retrieve ArXiv search results.</Error>\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86155692",
      "metadata": {},
      "source": [
        "**Key Features**\n",
        "- Query Generation: Uses LLM to create structured search queries from conversation context\n",
        "- Error Handling: Robust error management for ArXiv searches\n",
        "- Result Formatting: Consistent XML-style formatting for both web and academic results\n",
        "- Metadata Integration: Comprehensive metadata inclusion for academic papers\n",
        "- State Management: Maintains conversation context through InterviewState"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e666219e",
      "metadata": {},
      "source": [
        "### Define `generate_answer`, `save_interview`, `route_messages`, `write_section` Nodes\n",
        "\n",
        "- The `generate_answer` node is responsible for creating expert responses during the interview process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4509e48e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import get_buffer_string\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "answer_instructions = \"\"\"You are an expert being interviewed by an analyst.\n",
        "\n",
        "Here is analyst area of focus: {goals}. \n",
        "        \n",
        "You goal is to answer a question posed by the interviewer.\n",
        "\n",
        "To answer question, use this context:\n",
        "        \n",
        "{context}\n",
        "\n",
        "When answering questions, follow these guidelines:\n",
        "        \n",
        "1. Use only the information provided in the context. \n",
        "        \n",
        "2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
        "\n",
        "3. The context contain sources at the topic of each individual document.\n",
        "\n",
        "4. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1]. \n",
        "\n",
        "5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\n",
        "        \n",
        "6. If the source is: <Document source=\"assistant/docs/llama3_1.pdf\" page=\"7\"/>' then just list: \n",
        "        \n",
        "[1] assistant/docs/llama3_1.pdf, page 7 \n",
        "        \n",
        "And skip the addition of the brackets as well as the Document source preamble in your citation.\"\"\"\n",
        "\n",
        "\n",
        "def generate_answer(state: InterviewState):\n",
        "    \"\"\"Generates expert responses to analyst questions\"\"\"\n",
        "\n",
        "    # Get analyst and messages from state\n",
        "    analyst = state[\"analyst\"]\n",
        "    messages = state[\"messages\"]\n",
        "    context = state[\"context\"]\n",
        "\n",
        "    # Generate answer for the question\n",
        "    system_message = answer_instructions.format(goals=analyst.persona, context=context)\n",
        "    answer = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
        "\n",
        "    # Name the message as expert response\n",
        "    answer.name = \"expert\"\n",
        "\n",
        "    # Add message to state\n",
        "    return {\"messages\": [answer]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4d878d4",
      "metadata": {},
      "source": [
        "- `save_interview`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "42706ff4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_interview(state: InterviewState):\n",
        "    \"\"\"Saves the interview content\"\"\"\n",
        "\n",
        "    # Get messages from state\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # Convert interview to string\n",
        "    interview = get_buffer_string(messages)\n",
        "\n",
        "    # Store under interview key\n",
        "    return {\"interview\": interview}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f8aa4df",
      "metadata": {},
      "source": [
        "### Define `generate_answer`, `save_interview`, `route_messages`, `write_section` Nodes\n",
        "- `route_messages`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "10bb1677",
      "metadata": {},
      "outputs": [],
      "source": [
        "def route_messages(state: InterviewState, name: str = \"expert\"):\n",
        "    \"\"\"Routes between questions and answers in the conversation\"\"\"\n",
        "\n",
        "    # Get messages from state\n",
        "    messages = state[\"messages\"]\n",
        "    max_num_turns = state.get(\"max_num_turns\", 2)\n",
        "\n",
        "    # Count expert responses\n",
        "    num_responses = len(\n",
        "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
        "    )\n",
        "\n",
        "    # End interview if maximum turns reached\n",
        "    if num_responses >= max_num_turns:\n",
        "        return \"save_interview\"\n",
        "\n",
        "    # This router runs after each question-answer pair\n",
        "    # Get the last question to check for conversation end signal\n",
        "    last_question = messages[-2]\n",
        "\n",
        "    # Check for conversation end signal\n",
        "    if \"Thank you so much for your help\" in last_question.content:\n",
        "        return \"save_interview\"\n",
        "    return \"ask_question\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfaaef36",
      "metadata": {},
      "source": [
        "- The `write_section` function and its associated instructions implement a structured report generation system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ca2e1be1",
      "metadata": {},
      "outputs": [],
      "source": [
        "section_writer_instructions = \"\"\"You are an expert technical writer. \n",
        "\n",
        "Your task is to create a detailed and comprehensive section of a report, thoroughly analyzing a set of source documents.\n",
        "This involves extracting key insights, elaborating on relevant points, and providing in-depth explanations to ensure clarity and understanding. Your writing should include necessary context, supporting evidence, and examples to enhance the reader's comprehension. Maintain a logical and well-organized structure, ensuring that all critical aspects are covered in detail and presented in a professional tone.\n",
        "\n",
        "Please follow these instructions:\n",
        "1. Analyze the content of the source documents: \n",
        "- The name of each source document is at the start of the document, with the <Document tag.\n",
        "        \n",
        "2. Create a report structure using markdown formatting:\n",
        "- Use ## for the section title\n",
        "- Use ### for sub-section headers\n",
        "        \n",
        "3. Write the report following this structure:\n",
        "a. Title (## header)\n",
        "b. Summary (### header)\n",
        "c. Comprehensive analysis (### header)\n",
        "d. Sources (### header)\n",
        "\n",
        "4. Make your title engaging based upon the focus area of the analyst: \n",
        "{focus}\n",
        "\n",
        "5. For the summary section:\n",
        "- Set up summary with general background / context related to the focus area of the analyst\n",
        "- Emphasize what is novel, interesting, or surprising about insights gathered from the interview\n",
        "- Create a numbered list of source documents, as you use them\n",
        "- Do not mention the names of interviewers or experts\n",
        "- Aim for approximately 400 words maximum\n",
        "- Use numbered sources in your report (e.g., [1], [2]) based on information from source documents\n",
        "\n",
        "6. For the Comprehensive analysis section:\n",
        "- Provide a detailed examination of the information from the source documents.\n",
        "- Break down complex ideas into digestible segments, ensuring a logical flow of ideas.\n",
        "- Use sub-sections where necessary to cover multiple perspectives or dimensions of the analysis.\n",
        "- Support your analysis with data, direct quotes, and examples from the source documents.\n",
        "- Clearly explain the relevance of each point to the overall focus of the report.\n",
        "- Use bullet points or numbered lists for clarity when presenting multiple related ideas.\n",
        "- Ensure the tone remains professional and objective, avoiding bias or unsupported opinions.\n",
        "- Aim for at least 800 words to ensure the analysis is thorough.\n",
        "\n",
        "7. In the Sources section:\n",
        "- Include all sources used in your report\n",
        "- Provide full links to relevant websites or specific document paths\n",
        "- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n",
        "- It will look like:\n",
        "\n",
        "### Sources\n",
        "[1] Link or Document name\n",
        "[2] Link or Document name\n",
        "\n",
        "8. Be sure to combine sources. For example this is not correct:\n",
        "\n",
        "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
        "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
        "\n",
        "There should be no redundant sources. It should simply be:\n",
        "\n",
        "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
        "        \n",
        "9. Final review:\n",
        "- Ensure the report follows the required structure\n",
        "- Include no preamble before the title of the report\n",
        "- Check that all guidelines have been followed\"\"\"\n",
        "\n",
        "\n",
        "def write_section(state: InterviewState):\n",
        "    \"\"\"Generates a structured report section based on interview content\"\"\"\n",
        "\n",
        "    # Get context and analyst from state\n",
        "    context = state[\"context\"]\n",
        "    analyst = state[\"analyst\"]\n",
        "\n",
        "    # Define system prompt for section writing\n",
        "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
        "    section = llm.invoke(\n",
        "        [\n",
        "            SystemMessage(content=system_message),\n",
        "            HumanMessage(content=f\"Use this source to write your section: {context}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Add section to state\n",
        "    return {\"sections\": [section.content]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc292ede",
      "metadata": {},
      "source": [
        "### Building the Interview Graph\n",
        "Here's how to create and configure the interview execution graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "343f74be",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Add nodes and edges\n",
        "interview_builder = StateGraph(InterviewState)\n",
        "\n",
        "# Define nodes\n",
        "interview_builder.add_node(\"ask_question\", generate_question)\n",
        "interview_builder.add_node(\"search_web\", search_web)\n",
        "interview_builder.add_node(\"search_arxiv\", search_arxiv)\n",
        "interview_builder.add_node(\"answer_question\", generate_answer)\n",
        "interview_builder.add_node(\"save_interview\", save_interview)\n",
        "interview_builder.add_node(\"write_section\", write_section)\n",
        "\n",
        "# Configure flow\n",
        "interview_builder.add_edge(START, \"ask_question\")\n",
        "interview_builder.add_edge(\"ask_question\", \"search_web\")\n",
        "interview_builder.add_edge(\"ask_question\", \"search_arxiv\")\n",
        "interview_builder.add_edge(\"search_web\", \"answer_question\")\n",
        "interview_builder.add_edge(\"search_arxiv\", \"answer_question\")\n",
        "interview_builder.add_conditional_edges(\n",
        "    \"answer_question\", route_messages, [\"ask_question\", \"save_interview\"]\n",
        ")\n",
        "interview_builder.add_edge(\"save_interview\", \"write_section\")\n",
        "interview_builder.add_edge(\"write_section\", END)\n",
        "\n",
        "# Create interview graph with memory\n",
        "memory = MemorySaver()\n",
        "interview_graph = interview_builder.compile(checkpointer=memory).with_config(\n",
        "    run_name=\"Conduct Interviews\"\n",
        ")\n",
        "\n",
        "# Visualize the graph\n",
        "visualize_graph(interview_graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf40202",
      "metadata": {},
      "source": [
        "**Graph Structure**\n",
        "\n",
        "The interview process follows this flow:\n",
        "1. Question Generation\n",
        "2. Parallel Search (Web and ArXiv)\n",
        "3. Answer Generation\n",
        "4. Conditional Routing\n",
        "5. Interview Saving\n",
        "6. Section Writing\n",
        "\n",
        "**Key Components**\n",
        "- State Management: Uses InterviewState for tracking\n",
        "- Memory Persistence: Implements MemorySaver\n",
        "- Conditional Logic: Routes between questions and interview completion\n",
        "- Parallel Processing: Conducts simultaneous web and academic searches\n",
        "\n",
        "Note: Ensure the langgraph module is installed before running this code."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6c487bf",
      "metadata": {},
      "source": [
        "### Executing the Interview Graph\n",
        "Here's how to execute the graph and display the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "cbced19b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Analyst(affiliation='AI Technology Research Institute', name='Dr. Emily Zhang', role='AI Researcher', description='Dr. Zhang focuses on the technical distinctions between Modular RAG and Naive RAG, analyzing their architectural differences, computational efficiencies, and adaptability in various AI applications. Her interest lies in understanding how these models can be optimized for better performance and scalability.')"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Select first analyst from the list\n",
        "analysts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "6377d8b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello, my name is Alex Turner, and I'm an analyst interested in learning about the fascinating world of AI architectures. Dr. Zhang, could you explain the fundamental differences between Modular RAG and Naive RAG, particularly in terms of their architecture and how these differences impact their performance?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
            "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
            "Learn about the key differences between Modular and Naive RAG, case study, and the significant advantages of Modular RAG. ... The rigid architecture of Naive RAG makes it challenging to incorporate custom modules tailored to specific tasks or industries. This lack of customization restricts the applicability of the system to more generalized\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://www.marktechpost.com/2024/04/01/evolution-of-rags-naive-rag-advanced-rag-and-modular-rag-architectures/\"/>\n",
            "The RAG research paradigm is continuously evolving, and RAG is categorized into three stages: Naive RAG, Advanced RAG, and Modular RAG. Naive RAG has several limitations, including Retrieval Challenges and Generation Difficulties. The latter RAG architectures were proposed to address these problems: Advanced RAG and Modular RAG. Due to the\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2406.00944v2\" date=\"2024-10-17\" authors=\"Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng\"/>\n",
            "<Title>\n",
            "A Theory for Token-Level Harmonization in Retrieval-Augmented Generation\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\n",
            "large language models (LLMs). Studies show that while RAG provides valuable\n",
            "external information (benefit), it may also mislead LLMs (detriment) with noisy\n",
            "or incorrect retrieved texts. Although many existing methods attempt to\n",
            "preserve benefit and avoid detriment, they lack a theoretical explanation for\n",
            "RAG. The benefit and detriment in the next token prediction of RAG remain a\n",
            "black box that cannot be quantified or compared in an explainable manner, so\n",
            "existing methods are data-driven, need additional utility evaluators or\n",
            "post-hoc. This paper takes the first step towards providing a theory to explain\n",
            "and trade off the benefit and detriment in RAG. First, we model RAG as the\n",
            "fusion between distribution of LLMs knowledge and distribution of retrieved\n",
            "texts. Then, we formalize the trade-off between the value of external knowledge\n",
            "(benefit) and its potential risk of misleading LLMs (detriment) in next token\n",
            "prediction of RAG by distribution difference in this fusion. Finally, we prove\n",
            "that the actual effect of RAG on the token, which is the comparison between\n",
            "benefit and detriment, can be predicted without any training or accessing the\n",
            "utility of retrieval. Based on our theory, we propose a practical novel method,\n",
            "Tok-RAG, which achieves collaborative generation between the pure LLM and RAG\n",
            "at token level to preserve benefit and avoid detriment. Experiments in\n",
            "real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
            "effectiveness of our method and support our theoretical findings.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "A THEORY FOR TOKEN-LEVEL HARMONIZATION IN\n",
            "RETRIEVAL-AUGMENTED GENERATION\n",
            "Shicheng Xu\n",
            "Liang Pang∗Huawei Shen\n",
            "Xueqi Cheng\n",
            "CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS\n",
            "{xushicheng21s,pangliang,shenhuawei,cxq}@ict.ac.cn\n",
            "ABSTRACT\n",
            "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance large\n",
            "language models (LLMs). Studies show that while RAG provides valuable external\n",
            "information (benefit), it may also mislead LLMs (detriment) with noisy or incorrect\n",
            "retrieved texts. Although many existing methods attempt to preserve benefit and\n",
            "avoid detriment, they lack a theoretical explanation for RAG. The benefit and\n",
            "detriment in the next token prediction of RAG remain a ’black box’ that cannot\n",
            "be quantified or compared in an explainable manner, so existing methods are data-\n",
            "driven, need additional utility evaluators or post-hoc. This paper takes the first step\n",
            "towards providing a theory to explain and trade off the benefit and detriment in\n",
            "RAG. First, we model RAG as the fusion between distribution of LLM’s knowledge\n",
            "and distribution of retrieved texts. Then, we formalize the trade-off between the\n",
            "value of external knowledge (benefit) and its potential risk of misleading LLMs\n",
            "(detriment) in next token prediction of RAG by distribution difference in this\n",
            "fusion. Finally, we prove that the actual effect of RAG on the token, which is the\n",
            "comparison between benefit and detriment, can be predicted without any training or\n",
            "accessing the utility of retrieval. Based on our theory, we propose a practical novel\n",
            "method, Tok-RAG, which achieves collaborative generation between the pure\n",
            "LLM and RAG at token level to preserve benefit and avoid detriment. Experiments\n",
            "in real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
            "effectiveness of our method and support our theoretical findings.\n",
            "1\n",
            "INTRODUCTION\n",
            "Retrieval-augmented generation (RAG) has shown promising performance in enhancing Large\n",
            "Language Models (LLMs) by integrating retrieved texts (Xu et al., 2023; Shi et al., 2023; Asai et al.,\n",
            "2023; Ram et al., 2023). Studies indicate that while RAG provides LLMs with valuable additional\n",
            "knowledge (benefit), it also poses a risk of misleading them (detriment) due to noisy or incorrect\n",
            "retrieved texts (Ram et al., 2023; Xu et al., 2024b;a; Jin et al., 2024a; Xie et al., 2023; Jin et al.,\n",
            "2024b). Existing methods attempt to preserve benefit and avoid detriment by adding utility evaluators\n",
            "for retrieval, prompt engineering, or fine-tuning LLMs (Asai et al., 2023; Ding et al., 2024; Xu et al.,\n",
            "2024b; Yoran et al., 2024; Ren et al., 2023; Feng et al., 2023; Mallen et al., 2022; Jiang et al., 2023).\n",
            "However, existing methods are data-driven, need evaluator for utility of retrieved texts or post-hoc. A\n",
            "theory-based method, focusing on core principles of RAG is urgently needed, which is crucial for\n",
            "consistent and reliable improvements without relying on additional training or utility evaluators and\n",
            "improving our understanding for RAG.\n",
            "This paper takes the first step in providing a theoretical framework to explain and trade off the benefit\n",
            "and detriment at token level in RAG and proposes a novel method to preserve benefit and avoid\n",
            "detriment based on our theoretical findings. Specifically, this paper pioneers in modeling next token\n",
            "prediction in RAG as the fusion between the distribution of LLM’s knowledge and the distribution\n",
            "of retrieved texts as shown in Figure 1. Our theoretical derivation based on this formalizes the core\n",
            "of this fusion as the subtraction between two terms measured by the distribution difference: one is\n",
            "distribution completion and the other is distribution contradiction. Further analysis indicates that\n",
            "the distribution completion measures how much out-of-distribution knowledge that retrieved texts\n",
            "∗Corresponding author\n",
            "1\n",
            "arXiv:2406.00944v2  [cs.CL]  17 Oct 2024\n",
            "Query\n",
            "Wole\n",
            "Query\n",
            "Ernst\n",
            "Soyinka\n",
            "…\n",
            "LLM’s \n",
            "Distribution\n",
            "Retrieved \n",
            "Distribution \n",
            "Fusion\n",
            "Distribution\n",
            "Difference\n",
            "Olanipekun\n",
            "LLM’s \n",
            "Dis\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
            "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
            "increasing demands of application scenarios have driven the evolution of RAG,\n",
            "leading to the integration of advanced retrievers, LLMs and other complementary\n",
            "technologies, which in turn has amplified the intricacy of RAG systems.\n",
            "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process of\n",
            "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
            "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
            "decomposing complex RAG systems into independent modules and specialized\n",
            "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a more advanced\n",
            "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
            "extensive research, this paper further identifies prevalent RAG\n",
            "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
            "analysis of their respective implementation nuances. Modular RAG presents\n",
            "innovative opportunities for the conceptualization and deployment of RAG\n",
            "systems. Finally, the paper explores the potential emergence of new operators\n",
            "and paradigms, establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment of RAG\n",
            "technologies.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2409.11598v2\" date=\"2024-12-03\" authors=\"To Eun Kim, Fernando Diaz\"/>\n",
            "<Title>\n",
            "Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Many language models now enhance their responses with retrieval capabilities,\n",
            "leading to the widespread adoption of retrieval-augmented generation (RAG)\n",
            "systems. However, despite retrieval being a core component of RAG, much of the\n",
            "research in this area overlooks the extensive body of work on fair ranking,\n",
            "neglecting the importance of considering all stakeholders involved. This paper\n",
            "presents the first systematic evaluation of RAG systems integrated with fair\n",
            "rankings. We focus specifically on measuring the fair exposure of each relevant\n",
            "item across the rankings utilized by RAG systems (i.e., item-side fairness),\n",
            "aiming to promote equitable growth for relevant item providers. To gain a deep\n",
            "understanding of the relationship between item-fairness, ranking quality, and\n",
            "generation quality in the context of RAG, we analyze nine different RAG systems\n",
            "that incorporate fair rankings across seven distinct datasets. Our findings\n",
            "indicate that RAG systems with fair rankings can maintain a high level of\n",
            "generation quality and, in many cases, even outperform traditional RAG systems,\n",
            "despite the general trend of a tradeoff between ensuring fairness and\n",
            "maintaining system-effectiveness. We believe our insights lay the groundwork\n",
            "for responsible and equitable RAG systems and open new avenues for future\n",
            "research. We publicly release our codebase and dataset at\n",
            "https://github.com/kimdanny/Fair-RAG.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "Towards Fair RAG: On the Impact of Fair Ranking\n",
            "in Retrieval-Augmented Generation\n",
            "To Eun Kim\n",
            "Carnegie Mellon University\n",
            "toeunk@cs.cmu.edu\n",
            "Fernando Diaz\n",
            "Carnegie Mellon University\n",
            "diazf@acm.org\n",
            "Abstract\n",
            "Many language models now enhance their responses with retrieval capabilities,\n",
            "leading to the widespread adoption of retrieval-augmented generation (RAG) systems.\n",
            "However, despite retrieval being a core component of RAG, much of the research\n",
            "in this area overlooks the extensive body of work on fair ranking, neglecting the\n",
            "importance of considering all stakeholders involved. This paper presents the first\n",
            "systematic evaluation of RAG systems integrated with fair rankings. We focus\n",
            "specifically on measuring the fair exposure of each relevant item across the rankings\n",
            "utilized by RAG systems (i.e., item-side fairness), aiming to promote equitable\n",
            "growth for relevant item providers. To gain a deep understanding of the relationship\n",
            "between item-fairness, ranking quality, and generation quality in the context of RAG,\n",
            "we analyze nine different RAG systems that incorporate fair rankings across seven\n",
            "distinct datasets. Our findings indicate that RAG systems with fair rankings can\n",
            "maintain a high level of generation quality and, in many cases, even outperform\n",
            "traditional RAG systems, despite the general trend of a tradeoff between ensuring\n",
            "fairness and maintaining system-effectiveness. We believe our insights lay the\n",
            "groundwork for responsible and equitable RAG systems and open new avenues for\n",
            "future research. We publicly release our codebase and dataset. 1\n",
            "1\n",
            "Introduction\n",
            "In recent years, the concept of fair ranking has emerged as a critical concern in modern information\n",
            "access systems [12]. However, despite its significance, fair ranking has yet to be thoroughly examined\n",
            "in the context of retrieval-augmented generation (RAG) [1, 29], a rapidly advancing trend in natural\n",
            "language processing (NLP) systems [27]. To understand why this is important, consider the RAG\n",
            "system in Figure 1, where a user asks a question about running shoes. A classic retrieval system\n",
            "might return several documents containing information from various running shoe companies. If the\n",
            "RAG system only selects the top two documents, then information from the remaining two relevant\n",
            "companies will not be relayed to the predictive model and will likely be omitted from its answer.\n",
            "The fair ranking literature refers to this situation as unfair because some relevant companies (i.e., in\n",
            "documents at position 3 and 4) receive less or no exposure compared to equally relevant company in\n",
            "the top position [12].\n",
            "Understanding the effect of fair ranking in RAG is fundamental to ensuring responsible and equitable\n",
            "NLP systems. Since retrieval results in RAG often underlie response attribution [15], unfair exposure\n",
            "of content to the RAG system can result in incomplete evidence in responses (thus compromising recall\n",
            "of potentially relevant information for users) or downstream representational harms (thus creating\n",
            "or reinforcing biases across the set of relevant entities). In situations where content providers are\n",
            "compensated for contributions to inference, there can be financial implications for the unfairness\n",
            "[4, 19, 31]. Indeed, the fair ranking literature indicates that these are precisely the harms that emerge\n",
            "1https://github.com/kimdanny/Fair-RAG\n",
            "arXiv:2409.11598v2  [cs.IR]  3 Dec 2024\n",
            "What are the best running shoes \n",
            "to buy for marathons?\n",
            "𝒅𝟏\n",
            "𝒅𝟐\n",
            "top-k \n",
            "truncation\n",
            "Here are some \n",
            "best options from \n",
            "company A and \n",
            "company B\n",
            "LM\n",
            "Corpus\n",
            "𝒅𝟏 (Company A)\n",
            "𝒅𝟐 (Company B)\n",
            "𝒅𝟑 (Company C)\n",
            "𝒅𝟒 (Company D)\n",
            "𝒅𝟓 (Company B)\n",
            "…\n",
            "Rel\n",
            "Non-Rel\n",
            "Rel\n",
            "Rel\n",
            "Company C and D\n",
            "We are also \n",
            "relevant!\n",
            "🤖\n",
            "🙁\n",
            "🧑💻\n",
            "Non-Rel\n",
            "Figure 1: Fairness concerns in RAG. A simplified example of how RAG models can ignore equally\n",
            "relevant items (d3 and d4) and always consume the fixed top-scoring items (d1 and d2) with the same\n",
            "order of ranking over the multiple user requests. This is due to the deterministic nature of the retrieval\n",
            "process and \n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly, Dr. Zhang. The fundamental differences between Modular RAG and Naive RAG primarily lie in their architectural design and flexibility.\n",
            "\n",
            "1. **Architecture and Design**:\n",
            "   - **Naive RAG**: This approach combines information retrieval with natural language generation, using retrieval models that rank data based on relevance to the input query. It generally has a linear architecture, where the process is \"retrieve-then-generate,\" which may limit its ability to handle complex queries with substantial variability [4].\n",
            "   - **Modular RAG**: This framework transforms RAG systems into reconfigurable, LEGO-like structures by decomposing them into independent modules and specialized operators. It transcends the traditional linear architecture by incorporating advanced design elements like routing, scheduling, and fusion mechanisms. This modularity allows for greater flexibility and customization, enabling the system to adapt more effectively to specific tasks or industries [2][3].\n",
            "\n",
            "2. **Performance and Benefits**:\n",
            "   - **Naive RAG** faces challenges such as shallow understanding of queries and retrieval redundancy, which can introduce noise and reduce the quality of generated responses. Its rigid architecture limits customization and adaptation to domain-specific needs, making it less suitable for complex or specialized applications [5].\n",
            "   - **Modular RAG** offers significant advantages in production settings due to its reconfigurable nature. The modular design allows for the integration of advanced retrievers, LLMs, and other technologies, improving efficiency and scalability. This adaptability makes it suitable for a wide range of applications, from knowledge-intensive tasks to industry-specific solutions, by allowing easy incorporation of custom modules tailored to specific requirements [2][3].\n",
            "\n",
            "Overall, Modular RAG's enhanced flexibility and capability to manage complex tasks make it a more robust choice for production-level deployments, providing a foundation for scalable and efficient AI systems.\n",
            "\n",
            "Sources:\n",
            "[1] http://arxiv.org/abs/2407.21059v1\n",
            "[2] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "[4] http://arxiv.org/abs/2409.11598v2\n",
            "[5] https://www.marktechpost.com/2024/04/01/evolution-of-rags-naive-rag-advanced-rag-and-modular-rag-architectures/\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Thank you for clarifying these distinctions, Dr. Zhang. Could you provide an example of a real-world application where Modular RAG's adaptability significantly enhances performance compared to Naive RAG? Specifically, how does its modularity contribute to better scalability or efficiency in that scenario?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
            "The consolidated table demonstrates that Modular RAG outperforms Naive RAG across all key metrics, making it a more effective and reliable solution for customer support chatbots. By adopting a modular approach, organizations can achieve better relevance, faster response times, greater scalability, and higher customer satisfaction.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
            "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://medium.com/@abhilashkrish/understanding-rag-architectures-naive-advanced-multi-modal-and-graph-rag-for-real-world-1c98da879f76\"/>\n",
            "Understanding RAG Architectures: Naive, Advanced, Multi-modal, and Graph RAG for Real-World Applications | by Abhilash Krishnan | Nov, 2024 | Medium These additions improve retrieval accuracy and relevance, making Advanced RAG suitable for use cases that require nuanced understanding of query intent and better context matching. Cross-modal Understanding: By aligning embeddings from different modalities, Multi-modal RAG can understand and retrieve information even when the query and the data are in different formats (e.g., text query to retrieve image-based results). Multi-modal RAG opens possibilities for cross-modal understanding, and Graph RAG enables retrieval from complex, relational data structures, making it highly valuable in fields requiring structured knowledge and reasoning.\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
            "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
            "increasing demands of application scenarios have driven the evolution of RAG,\n",
            "leading to the integration of advanced retrievers, LLMs and other complementary\n",
            "technologies, which in turn has amplified the intricacy of RAG systems.\n",
            "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process of\n",
            "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
            "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
            "decomposing complex RAG systems into independent modules and specialized\n",
            "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a more advanced\n",
            "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
            "extensive research, this paper further identifies prevalent RAG\n",
            "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
            "analysis of their respective implementation nuances. Modular RAG presents\n",
            "innovative opportunities for the conceptualization and deployment of RAG\n",
            "systems. Finally, the paper explores the potential emergence of new operators\n",
            "and paradigms, establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment of RAG\n",
            "technologies.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2410.12837v1\" date=\"2024-10-03\" authors=\"Shailja Gupta, Rajesh Ranjan, Surya Narayan Singh\"/>\n",
            "<Title>\n",
            "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "This paper presents a comprehensive study of Retrieval-Augmented Generation\n",
            "(RAG), tracing its evolution from foundational concepts to the current state of\n",
            "the art. RAG combines retrieval mechanisms with generative language models to\n",
            "enhance the accuracy of outputs, addressing key limitations of LLMs. The study\n",
            "explores the basic architecture of RAG, focusing on how retrieval and\n",
            "generation are integrated to handle knowledge-intensive tasks. A detailed\n",
            "review of the significant technological advancements in RAG is provided,\n",
            "including key innovations in retrieval-augmented language models and\n",
            "applications across various domains such as question-answering, summarization,\n",
            "and knowledge-based tasks. Recent research breakthroughs are discussed,\n",
            "highlighting novel methods for improving retrieval efficiency. Furthermore, the\n",
            "paper examines ongoing challenges such as scalability, bias, and ethical\n",
            "concerns in deployment. Future research directions are proposed, focusing on\n",
            "improving the robustness of RAG models, expanding the scope of application of\n",
            "RAG models, and addressing societal implications. This survey aims to serve as\n",
            "a foundational resource for researchers and practitioners in understanding the\n",
            "potential of RAG and its trajectory in natural language processing.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "A\n",
            "Comprehensive\n",
            "Survey\n",
            "of\n",
            "Retrieval-Augmented\n",
            "Generation\n",
            "(RAG):\n",
            "Evolution,\n",
            "Current\n",
            "Landscape and Future Directions\n",
            "Shailja Gupta (Carnegie Mellon University, USA)\n",
            "Rajesh Ranjan (Carnegie Mellon University, USA)\n",
            "Surya Narayan Singh (BIT Sindri, India)\n",
            "Abstract\n",
            "This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its\n",
            "evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms\n",
            "with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs.\n",
            "The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated\n",
            "to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in\n",
            "RAG is provided, including key innovations in retrieval-augmented language models and applications\n",
            "across various domains such as question-answering, summarization, and knowledge-based tasks.\n",
            "Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval\n",
            "efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical\n",
            "concerns in deployment. Future research directions are proposed, with a focus on improving the\n",
            "robustness of RAG models, expanding the scope of application of RAG models, and addressing societal\n",
            "implications. This survey aims to serve as a foundational resource for researchers and practitioners in\n",
            "understanding the potential of RAG and its trajectory in the field of natural language processing.\n",
            "Figure 1: Trends in RAG captured from recent research papers\n",
            "Keywords: Retrieval-Augmented Generation (RAG), Information Retrieval, Natural Language Processing\n",
            "(NLP), Artificial Intelligence (AI), Machine Learning (ML), Large Language Model (LLM).\n",
            "Introduction\n",
            "1.1 Introduction of Natural Language Generation (NLG)\n",
            "Natural Language Processing (NLP) has become a pivotal domain within artificial intelligence (AI), with\n",
            "applications ranging from simple text classification to more complex tasks such as summarization,\n",
            "machine translation, and question answering. A particularly significant branch of NLP is Natural Language\n",
            "Generation (NLG), which focuses on the production of human-like language from structured or\n",
            "unstructured data. NLG's goal is to enable machines to generate coherent, relevant, and context-aware\n",
            "text, improving interactions between humans and machines (Gatt et. al. 2018). As AI evolves, the demand\n",
            "for more contextually aware and factually grounded generated content has increased, bringing about new\n",
            "challenges and innovations in NLG.\n",
            "Traditional NLG models, especially sequence-to-sequence architectures (Sutskever et al. 2014), have\n",
            "exhibited significant advancements in generating fluent and coherent text. However, these models tend to\n",
            "rely heavily on training data, often struggling when tasked with generating factually accurate or\n",
            "contextually rich content for queries that require knowledge beyond their training set. As a result, models\n",
            "like GPT (Radford et al. 2019) or BERT-based (Devlin et al. 2019) text generators are prone to\n",
            "hallucinations, where they produce plausible but incorrect or non-existent information (Ji et al. 2022). This\n",
            "limitation has prompted the exploration of hybrid models that combine retrieval mechanisms with\n",
            "generative capabilities to ensure both fluency and factual correctness in outputs. There has been a\n",
            "significant rise in several research papers in this field and several new methods across the RAG\n",
            "components have been proposed. Apart from new algorithms and methods, RAG has also seen steep\n",
            "adoption across various applications. However, there is a gap in a sufficient survey of this space tracking\n",
            "the evolution and recent changes in this space. The current survey intends to fill this gap.\n",
            "1.2 Overview of Retrieval-Augmented Generation (RAG)\n",
            "Retrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address the\n",
            "limitations of pure generat\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2411.03538v1\" date=\"2024-11-05\" authors=\"Quinn Leng, Jacob Portes, Sam Havens, Matei Zaharia, Michael Carbin\"/>\n",
            "<Title>\n",
            "Long Context RAG Performance of Large Language Models\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval Augmented Generation (RAG) has emerged as a crucial technique for\n",
            "enhancing the accuracy of Large Language Models (LLMs) by incorporating\n",
            "external information. With the advent of LLMs that support increasingly longer\n",
            "context lengths, there is a growing interest in understanding how these models\n",
            "perform in RAG scenarios. Can these new long context models improve RAG\n",
            "performance? This paper presents a comprehensive study of the impact of\n",
            "increased context length on RAG performance across 20 popular open source and\n",
            "commercial LLMs. We ran RAG workflows while varying the total context length\n",
            "from 2,000 to 128,000 tokens (and 2 million tokens when possible) on three\n",
            "domain-specific datasets, and report key insights on the benefits and\n",
            "limitations of long context in RAG applications. Our findings reveal that while\n",
            "retrieving more documents can improve performance, only a handful of the most\n",
            "recent state of the art LLMs can maintain consistent accuracy at long context\n",
            "above 64k tokens. We also identify distinct failure modes in long context\n",
            "scenarios, suggesting areas for future research.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "Long Context RAG Performance of Large Language\n",
            "Models\n",
            "Quinn Leng∗\n",
            "Databricks Mosaic Research\n",
            "quinn.leng@databricks.com\n",
            "Jacob Portes∗\n",
            "Databricks Mosaic Research\n",
            "jacob.portes@databricks.com\n",
            "Sam Havens\n",
            "Databricks Mosaic Research\n",
            "sam.havens@databricks.com\n",
            "Matei Zaharia\n",
            "Databricks Mosaic Research\n",
            "matei@databricks.com\n",
            "Michael Carbin\n",
            "Databricks Mosaic Research\n",
            "michael.carbin@databricks.com\n",
            "Abstract\n",
            "Retrieval Augmented Generation (RAG) has emerged as a crucial technique for\n",
            "enhancing the accuracy of Large Language Models (LLMs) by incorporating\n",
            "external information. With the advent of LLMs that support increasingly longer\n",
            "context lengths, there is a growing interest in understanding how these models\n",
            "perform in RAG scenarios. Can these new long context models improve RAG\n",
            "performance? This paper presents a comprehensive study of the impact of increased\n",
            "context length on RAG performance across 20 popular open source and commercial\n",
            "LLMs. We ran RAG workflows while varying the total context length from 2,000\n",
            "to 128,000 tokens (and 2 million tokens when possible) on three domain-specific\n",
            "datasets, and report key insights on the benefits and limitations of long context\n",
            "in RAG applications. Our findings reveal that while retrieving more documents\n",
            "can improve performance, only a handful of the most recent state of the art LLMs\n",
            "can maintain consistent accuracy at long context above 64k tokens. We also\n",
            "identify distinct failure modes in long context scenarios, suggesting areas for future\n",
            "research.\n",
            "1\n",
            "Introduction\n",
            "The development of Large Language Models (LLMs) with increasingly longer context lengths has\n",
            "opened new possibilities for Retrieval Augmented Generation (RAG) applications. Recent models\n",
            "such as Anthropic Claude (200k tokens) [1], GPT-4-turbo (128k tokens) [2], OpenAI o1 (128k tokens)\n",
            "[3], Llama 3 [4] and Google Gemini 1.5 Pro (2 million tokens) [5] have led to speculation about\n",
            "whether long context models might eventually subsume traditional RAG workflows entirely. In this\n",
            "study, we empirically investigate the impact of increased context length on RAG performance and\n",
            "explore the limitations and challenges that arise in long context scenarios.\n",
            "RAG can enhance the accuracy of LLMs by retrieving information from external sources, enabling\n",
            "users to incorporate task-specific or private data into their LLM workflows. Published results using\n",
            "RAG-like methods have demonstrated benefits across many applications [6] including machine\n",
            "∗Equal contribution\n",
            "Workshop on Adaptive Foundation Models, 38th Conference on Neural Information Processing Systems\n",
            "(NeurIPS 2024).\n",
            "arXiv:2411.03538v1  [cs.LG]  5 Nov 2024\n",
            "Figure 1:\n",
            "Long context RAG performance of o1,\n",
            "GPT-4,\n",
            "Claude 3/3.5,\n",
            "Gemini 1.5\n",
            "(gemini-1.5-pro-001 and gemini-1.5-flash-001), Llama 3/3.1, Qwen 2, Mistral and DBRX\n",
            "models on 3 curated RAG datasets (Databricks DocsQA, FinanceBench, and Natural Questions). All\n",
            "values can be found in Table S3. Model versions are listed in Table S1.\n",
            "translation [7], semantic parsing [8], question answering [9, 10, 11, 12], and open-ended text genera-\n",
            "tion [13]. With longer context lengths, LLM developers can feed more documents into their RAG\n",
            "applications. While there has been recent speculation that long context LLMs will replace RAG\n",
            "entirely [14], in this paper we study whether long context LLMs can indeed be used effectively for\n",
            "RAG systems. How well do the best open source and commercial models do on long-context RAG\n",
            "tasks?\n",
            "In this study, we apply a standard RAG approach and evaluate the performance of 20 popular open\n",
            "source and commercial LLMs with varying context lengths from 2,000 to 128,000 tokens (and 2\n",
            "million tokens when possible). We then analyze distinct failure modes for different models across\n",
            "long context RAG scenarios. We show that:\n",
            "• Using longer context does not uniformly increase RAG performance. The majority of\n",
            "models we evaluated first increase and then decrease RAG performance as context length\n",
            "increases. Only a handful of the most recent state o\n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly, Dr. Zhang. A real-world application where Modular RAG's adaptability significantly enhances performance compared to Naive RAG is in the domain of customer support chatbots.\n",
            "\n",
            "In this scenario, customer support chatbots need to handle a wide variety of queries that may require accessing domain-specific knowledge or integrating with other systems. Naive RAG, with its linear \"retrieve-then-generate\" architecture, may struggle to efficiently handle the diversity of queries due to its reliance on straightforward similarity measures and the potential noise from redundant information [5]. This can lead to less relevant or slower responses, impacting customer satisfaction.\n",
            "\n",
            "Modular RAG, on the other hand, excels in this environment due to its reconfigurable nature. By decomposing the RAG system into independent modules, it allows for the integration of specialized operators and advanced retrievers tailored to the chatbot's specific domain or industry requirements. This modularity facilitates better routing and scheduling of tasks, ensuring that the most relevant information is retrieved and generated efficiently. As a result, Modular RAG can deliver more precise and contextually relevant responses, improving both the speed and quality of the customer service experience [3].\n",
            "\n",
            "The adaptability of Modular RAG also enables the chatbot to scale more effectively, as new modules can be added or existing ones refined without overhauling the entire system. This scalability is crucial for businesses looking to expand their support capabilities or integrate new technologies over time [2][3].\n",
            "\n",
            "Sources:\n",
            "[2] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "[5] https://www.marktechpost.com/2024/04/01/evolution-of-rags-naive-rag-advanced-rag-and-modular-rag-architectures/\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "That's a fascinating example of how Modular RAG can offer tangible benefits in a real-world setting. Could you elaborate on any specific computational efficiencies that Modular RAG introduces, and how these might translate to cost savings or enhanced user experience in practice?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
            "Retrieval-augmented generation (RAG) has emerged as a powerful technique that combines the strengths of information retrieval and natural language generation. However, not all RAG implementations are created equal. The traditional or \"Naive\" RAG, while groundbreaking, often struggles with limitations such as inflexibility and inefficiencies in handling diverse and dynamic datasets.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
            "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
            "👩🏻‍💻 Naive RAG, Advanced RAG & Modular RAG. RAG framework addresses the following questions: ... When a user asks a question: The user's input is first transformed into a vector\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2410.07589v1\" date=\"2024-10-10\" authors=\"Mengxuan Hu, Hongyi Wu, Zihan Guan, Ronghang Zhu, Dongliang Guo, Daiqing Qi, Sheng Li\"/>\n",
            "<Title>\n",
            "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-Augmented Generation (RAG) is widely adopted for its effectiveness\n",
            "and cost-efficiency in mitigating hallucinations and enhancing the\n",
            "domain-specific generation capabilities of large language models (LLMs).\n",
            "However, is this effectiveness and cost-efficiency truly a free lunch? In this\n",
            "study, we comprehensively investigate the fairness costs associated with RAG by\n",
            "proposing a practical three-level threat model from the perspective of user\n",
            "awareness of fairness. Specifically, varying levels of user fairness awareness\n",
            "result in different degrees of fairness censorship on the external dataset. We\n",
            "examine the fairness implications of RAG using uncensored, partially censored,\n",
            "and fully censored datasets. Our experiments demonstrate that fairness\n",
            "alignment can be easily undermined through RAG without the need for fine-tuning\n",
            "or retraining. Even with fully censored and supposedly unbiased external\n",
            "datasets, RAG can lead to biased outputs. Our findings underscore the\n",
            "limitations of current alignment methods in the context of RAG-based LLMs and\n",
            "highlight the urgent need for new strategies to ensure fairness. We propose\n",
            "potential mitigations and call for further research to develop robust fairness\n",
            "safeguards in RAG-based LLMs.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "NO FREE LUNCH: RETRIEVAL-AUGMENTED GENERATION\n",
            "UNDERMINES FAIRNESS IN LLMS, EVEN FOR VIGILANT USERS\n",
            "Mengxuan Hu ∗\n",
            "School of Data Science\n",
            "University of Virginia\n",
            "qtq7su@virginia.edu\n",
            "Hongyi Wu ∗\n",
            "School of Management\n",
            "University of Science and Technology of China\n",
            "ahwhy@mail.ustc.edu.cn\n",
            "Zihan Guan\n",
            "Department of Computer Science\n",
            "University of Virginia\n",
            "bxv6gs@virginia.edu\n",
            "Ronghang Zhu\n",
            "School of Computing\n",
            "University of Georgia\n",
            "ronghangzhu@uga.edu\n",
            "Dongliang Guo\n",
            "School of Data Science\n",
            "University of Virginia\n",
            "dongliang.guo@virginia.edu\n",
            "Daiqing Qi\n",
            "School of Data Science\n",
            "University of Virginia\n",
            "daiqing.qi@virginia.edu\n",
            "Sheng Li\n",
            "School of Data Science\n",
            "University of Virginia\n",
            "shengli@virginia.edu\n",
            "ABSTRACT\n",
            "Retrieval-Augmented Generation (RAG) is widely adopted for its effectiveness and cost-efficiency in\n",
            "mitigating hallucinations and enhancing the domain-specific generation capabilities of large language\n",
            "models (LLMs). However, is this effectiveness and cost-efficiency truly a free lunch? In this study,\n",
            "we comprehensively investigate the fairness costs associated with RAG by proposing a practical\n",
            "three-level threat model from the perspective of user awareness of fairness. Specifically, varying levels\n",
            "of user fairness awareness result in different degrees of fairness censorship on the external dataset. We\n",
            "examine the fairness implications of RAG using uncensored, partially censored, and fully censored\n",
            "datasets. Our experiments demonstrate that fairness alignment can be easily undermined through\n",
            "RAG without the need for fine-tuning or retraining. Even with fully censored and supposedly\n",
            "unbiased external datasets, RAG can lead to biased outputs. Our findings underscore the limitations\n",
            "of current alignment methods in the context of RAG-based LLMs and highlight the urgent need for\n",
            "new strategies to ensure fairness. We propose potential mitigations and call for further research to\n",
            "develop robust fairness safeguards in RAG-based LLMs.\n",
            "1\n",
            "Introduction\n",
            "Large language models (LLMs) such as Llama and ChatGPT have demonstrated significant success across a wide range\n",
            "of AI applications[1, 2]. However, these models still suffer from inherent limitations, including hallucinations[3] and the\n",
            "presence of outdated information[4]. To mitigate these challenges, Retrieval-Augmented Generation (RAG) has been\n",
            "introduced, which retrieves relevant knowledge from external datasets to enhance LLMs’ generative capabilities. This\n",
            "approach has drawn considerable attention due to its effectiveness and cost-efficiency[5]. Notably, both OpenAI[6] and\n",
            "Meta[7] advocate for RAG as a effective technique for improving model performance. However, is the effectiveness and\n",
            "efficiency of RAG truly a free lunch? RAG has been widely utilized in fairness-sensitive areas such as healthcare[8, 9],\n",
            "education[10], and finance[11]. Hence, a critical question arises: what potential side effects does RAG have on\n",
            "trustworthiness, particularly on fairness?\n",
            "∗Equal Contribution\n",
            "arXiv:2410.07589v1  [cs.IR]  10 Oct 2024\n",
            "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users\n",
            "Tremendous efforts have been devoted to align LLMs with human values to prevent harmful content generation,\n",
            "including discrimination, bias, and stereotypes. Established techniques such as reinforcement learning from human\n",
            "feedback (RLHF)[12] and instruction tuning[13] have been proven to significantly improve LLMs alignment. However,\n",
            "recent studies[14, 15, 16] reveal that this “impeccable alignment” can be easily compromised through fine-tuning or\n",
            "retraining. This vulnerability arises primarily because fine-tuning can alter the weights associated with the original\n",
            "alignment, resulting in degraded performance. However, what happens when we employ RAG, which does not modify\n",
            "the LLMs’ weights and thus maintains the “impeccable alignment”? Can fairness still be compromised? These questions\n",
            "raise a significant concern: if RAG can inadvertently lead LLMs to generate biased outputs, it indicates that\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2311.17696v6\" date=\"2025-01-22\" authors=\"Chenxi Dong, Yimin Yuan, Kan Chen, Shupei Cheng, Chujie Wen\"/>\n",
            "<Title>\n",
            "How to Build an AI Tutor That Can Adapt to Any Course Using Knowledge Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "This paper introduces KG-RAG (Knowledge Graph-enhanced Retrieval-Augmented\n",
            "Generation), a novel framework that addresses two critical challenges in\n",
            "LLM-based tutoring systems: information hallucination and limited\n",
            "course-specific adaptation. By integrating knowledge graphs with\n",
            "retrieval-augmented generation, KG-RAG provides a structured representation of\n",
            "course concepts and their relationships, enabling contextually grounded and\n",
            "pedagogically sound responses. We implement the framework using Qwen2.5,\n",
            "demonstrating its cost-effectiveness while maintaining high performance. The\n",
            "KG-RAG system outperformed standard RAG-based tutoring in a controlled study\n",
            "with 76 university students (mean scores: 6.37 vs. 4.71, p<0.001, Cohen's\n",
            "d=0.86). User feedback showed strong satisfaction with answer relevance (84%\n",
            "positive) and user experience (59% positive). Our framework offers a scalable\n",
            "approach to personalized AI tutoring, ensuring response accuracy and\n",
            "pedagogical coherence.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1 \n",
            " \n",
            "How to Build an AI Tutor That Can Adapt to Any \n",
            "Course Using Knowledge Graph-Enhanced \n",
            "Retrieval-Augmented Generation (KG-RAG) \n",
            " \n",
            "  \n",
            "Abstract—This paper introduces KG-RAG (Knowledge Graph-\n",
            "enhanced Retrieval-Augmented Generation), a novel framework \n",
            "that addresses two critical challenges in LLM-based tutoring \n",
            "systems: information hallucination and limited course-specific \n",
            "adaptation. By integrating knowledge graphs with retrieval-\n",
            "augmented \n",
            "generation, \n",
            "KG-RAG \n",
            "provides \n",
            "a \n",
            "structured \n",
            "representation of course concepts and their relationships, enabling \n",
            "contextually grounded and pedagogically sound responses. We \n",
            "implement the framework using Qwen2.5, demonstrating its cost-\n",
            "effectiveness while maintaining high performance. The KG-RAG \n",
            "system outperformed standard RAG-based tutoring in a \n",
            "controlled study with 76 university students (mean scores: 6.37 vs. \n",
            "4.71, p<0.001, Cohen's d=0.86). User feedback showed strong \n",
            "satisfaction with answer relevance (84% positive) and user \n",
            "experience (59% positive). Our framework offers a scalable \n",
            "approach to personalized AI tutoring, ensuring response accuracy \n",
            "and pedagogical coherence. \n",
            " \n",
            "Index Terms—Intelligent Tutoring Systems, Large language \n",
            "model, Retrieval-Augmented generation, Generative AI  \n",
            " \n",
            "I. INTRODUCTION \n",
            "INTEGRATING Artificial Intelligence (AI) into education \n",
            "holds immense potential for revolutionizing personalized \n",
            "learning. Intelligent Tutoring Systems (ITS) promise \n",
            "adaptive learning paths and on-demand support, empowering \n",
            "students to learn at their own pace and receive assistance \n",
            "whenever needed. However, realizing this potential is hindered \n",
            "by two key challenges: (1) Information Hallucination: Large \n",
            " \n",
            " \n",
            " \n",
            "Language Models (LLMs), the engines behind many AI tutors, \n",
            "can generate plausible but factually incorrect information [4], \n",
            "eroding trust and hindering effective learning. (2) Course-\n",
            "Specific Adaptation: Existing general-purpose AI tools often \n",
            "lack the necessary alignment with specific course content and \n",
            "pedagogical approaches, limiting their practical application in \n",
            "diverse educational settings [3]. To bridge this gap, we propose \n",
            "KG-RAG (Knowledge Graph-enhanced Retrieval-Augmented \n",
            "Generation), a novel framework that combines knowledge \n",
            "graphs with retrieval-augmented generation. Our approach is \n",
            "grounded in constructivist learning theory [18], emphasizing \n",
            "that learners build understanding through structured, context-\n",
            "rich interactions. The key contributions of our work include: \n",
            " \n",
            "1) A novel KG-RAG architecture that enhances response \n",
            "accuracy and pedagogical coherence by grounding LLM \n",
            "outputs in course-specific knowledge graphs \n",
            "2) An \n",
            "efficient \n",
            "implementation \n",
            "using \n",
            "Qwen2.5, \n",
            "demonstrating superior performance at significantly lower \n",
            "operational costs \n",
            "3) Comprehensive empirical evaluation through controlled \n",
            "experiments and user studies, providing quantitative \n",
            "evidence of improved learning outcomes \n",
            "4) Practical \n",
            "insights \n",
            "into \n",
            "deployment \n",
            "considerations, \n",
            "including cost optimization strategies and privacy \n",
            "preservation measures \n",
            "The remainder of this paper is organized as follows: Section \n",
            "II surveys existing AI tutoring approaches and their limitations. \n",
            "Section III presents the technical foundations of our framework. \n",
            "Chenxi Dong* \n",
            "Department of Mathematics and \n",
            "Information Technology \n",
            "The Education University of Hong Kong \n",
            "Hong Kong, China \n",
            "cdong@eduhk.hk \n",
            "*Corresponding author \n",
            "Yimin Yuan* \n",
            "Faculty of Sciences, Engineering and \n",
            "Technology \n",
            "The University of Adelaide \n",
            "Adelaide, Australia \n",
            "yimin.yuan@student.adelaide.edu.au \n",
            "*Corresponding author \n",
            " \n",
            "Kan Chen \n",
            "School of Communications and \n",
            "Information Engineering \n",
            "Chongqing University of Posts and \n",
            "Telecommunications \n",
            "Chongqing, China \n",
            "ck_linkin123@163.com\n",
            " \n",
            "Shupei Cheng \n",
            "Department of Mechanical and Electronic Engineering \n",
            "Wuhan University of Technology \n",
            "Wuhan, China \n",
            "1248228520@qq.com \n",
            " \n",
            " \n",
            "Chujie Wen \n",
            "Department of Mathematics and Information Technology \n",
            "The Education University of Ho\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2407.10670v1\" date=\"2024-07-15\" authors=\"Yunxiao Shi, Xing Zi, Zijing Shi, Haimin Zhang, Qiang Wu, Min Xu\"/>\n",
            "<Title>\n",
            "Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented generation (RAG) techniques leverage the in-context\n",
            "learning capabilities of large language models (LLMs) to produce more accurate\n",
            "and relevant responses. Originating from the simple 'retrieve-then-read'\n",
            "approach, the RAG framework has evolved into a highly flexible and modular\n",
            "paradigm. A critical component, the Query Rewriter module, enhances knowledge\n",
            "retrieval by generating a search-friendly query. This method aligns input\n",
            "questions more closely with the knowledge base. Our research identifies\n",
            "opportunities to enhance the Query Rewriter module to Query Rewriter+ by\n",
            "generating multiple queries to overcome the Information Plateaus associated\n",
            "with a single query and by rewriting questions to eliminate Ambiguity, thereby\n",
            "clarifying the underlying intent. We also find that current RAG systems exhibit\n",
            "issues with Irrelevant Knowledge; to overcome this, we propose the Knowledge\n",
            "Filter. These two modules are both based on the instruction-tuned Gemma-2B\n",
            "model, which together enhance response quality. The final identified issue is\n",
            "Redundant Retrieval; we introduce the Memory Knowledge Reservoir and the\n",
            "Retriever Trigger to solve this. The former supports the dynamic expansion of\n",
            "the RAG system's knowledge base in a parameter-free manner, while the latter\n",
            "optimizes the cost for accessing external knowledge, thereby improving resource\n",
            "utilization and response efficiency. These four RAG modules synergistically\n",
            "improve the response quality and efficiency of the RAG system. The\n",
            "effectiveness of these modules has been validated through experiments and\n",
            "ablation studies across six common QA datasets. The source code can be accessed\n",
            "at https://github.com/Ancientshi/ERM4.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "Enhancing Retrieval and Managing Retrieval:\n",
            "A Four-Module Synergy for Improved Quality and\n",
            "Efficiency in RAG Systems\n",
            "Yunxiao Shia, Xing Zia, Zijing Shia, Haimin Zhanga, Qiang Wua and Min Xua,*\n",
            "aUniversity of Technology Sydney, Broadway, Sydney, 2007, NSW, Australia.\n",
            "Abstract. Retrieval-augmented generation (RAG) techniques lever-\n",
            "age the in-context learning capabilities of large language models\n",
            "(LLMs) to produce more accurate and relevant responses. Originat-\n",
            "ing from the simple ’retrieve-then-read’ approach, the RAG frame-\n",
            "work has evolved into a highly flexible and modular paradigm. A\n",
            "critical component, the Query Rewriter module, enhances knowl-\n",
            "edge retrieval by generating a search-friendly query. This method\n",
            "aligns input questions more closely with the knowledge base. Our\n",
            "research identifies opportunities to enhance the Query Rewriter mod-\n",
            "ule to Query Rewriter+ by generating multiple queries to over-\n",
            "come the Information Plateaus associated with a single query and\n",
            "by rewriting questions to eliminate Ambiguity, thereby clarifying\n",
            "the underlying intent. We also find that current RAG systems ex-\n",
            "hibit issues with Irrelevant Knowledge; to overcome this, we pro-\n",
            "pose the Knowledge Filter. These two modules are both based on\n",
            "the instructional-tuned Gemma-2B model, which together enhance\n",
            "response quality. The final identified issue is Redundant Retrieval;\n",
            "we introduce the Memory Knowledge Reservoir and the Retriever\n",
            "Trigger to solve this. The former supports the dynamic expansion\n",
            "of the RAG system’s knowledge base in a parameter-free manner,\n",
            "while the latter optimizes the cost for accessing external knowl-\n",
            "edge, thereby improving resource utilization and response efficiency.\n",
            "These four RAG modules synergistically improve the response qual-\n",
            "ity and efficiency of the RAG system. The effectiveness of these\n",
            "modules has been validated through experiments and ablation studies\n",
            "across six common QA datasets. The source code can be accessed at\n",
            "https://github.com/Ancientshi/ERM4.\n",
            "1\n",
            "Introduction\n",
            "Large Language Models (LLMs) represent a significant leap in ar-\n",
            "tificial intelligence, with breakthroughs in generalization and adapt-\n",
            "ability across diverse tasks [4, 6]. However, challenges such as hal-\n",
            "lucinations [32], temporal misalignments [27], context processing\n",
            "issues [1], and fine-tuning inefficiencies [8] have raised significant\n",
            "concerns about their reliability. In response, recent research has fo-\n",
            "cused on enhancing LLMs’ capabilities by integrating them with ex-\n",
            "ternal knowledge sources through Retrieval-Augmented Generation\n",
            "(RAG) [2, 20, 13, 15]. This approach significantly improves LLMs’\n",
            "ability to answer questions more accurately and contextually.\n",
            "The basic RAG system comprises a knowledge retrieval mod-\n",
            "ule and a read module, forming the retrieve-then-read pipeline\n",
            "∗Corresponding author with email: Min.Xu@uts.edu.au.\n",
            "[20, 15, 13]. However, this vanilla pipeline has low retrieval qual-\n",
            "ity and produces unreliable answers. To transcend this, more ad-\n",
            "vanced RAG modules have been developed and integrated into the\n",
            "basic pipeline. For example, the Query Rewriter module acts as a\n",
            "bridge between the input question and the retrieval module. Instead\n",
            "of directly using the original question as the query text, it gener-\n",
            "ates a new query that better facilitates the retrieval of relevant infor-\n",
            "mation. This enhancement forms the Rewrite-Retrieve-Read pipeline\n",
            "[23, 22]. Furthermore, models like RETA-LLM [22] and RARR [10]\n",
            "integrate a post-reading and fact-checking component to further so-\n",
            "lidify the reliability of responses. Additional auxiliary modules such\n",
            "as the query router [21] and the resource ranker 1 [14] have also been\n",
            "proposed to be integrated into the RAG’s framework to improve the\n",
            "practicality in complex application scenario. This integration of var-\n",
            "ious modules into the RAG pipeline leading to the emergence of a\n",
            "modular RAG paradigm [11], transforming the RAG framework into\n",
            "a highly flexible system.\n",
            "Despite significant advancement\n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly, Dr. Zhang. Modular RAG introduces several computational efficiencies that can translate to cost savings and enhanced user experience:\n",
            "\n",
            "1. **Efficient Resource Utilization**: By employing specialized modules and operators, Modular RAG can dynamically allocate resources based on the specific needs of each task. This means that computational power is used more effectively, focusing on retrieving and processing only the most relevant information. This efficiency reduces unnecessary computational load, which can directly lead to cost savings in terms of reduced resource consumption [2].\n",
            "\n",
            "2. **Reduced Redundancy and Noise**: Modular RAG's architecture includes mechanisms to filter out redundant and noisy data before it reaches the language model. This reduces the computational effort required to process extraneous information, allowing the system to operate more quickly and with less strain on computational resources. As a result, users experience faster response times, which enhances the overall user experience [5].\n",
            "\n",
            "3. **Scalability and Flexibility**: The modular structure of Modular RAG allows organizations to scale their systems up or down easily, adding new capabilities or adjusting existing ones without needing to redesign the entire architecture. This flexibility not only optimizes the use of computational resources but also means that businesses can adapt quickly to changes in demand or new technological advancements, maintaining efficiency and cost-effectiveness [3].\n",
            "\n",
            "4. **Improved Query Handling**: By utilizing advanced routing and scheduling mechanisms, Modular RAG can prioritize and route queries more effectively. This ensures that complex or high-priority queries receive the necessary computational attention, while simpler tasks are handled with minimal resource allocation. This targeted approach enhances processing efficiency, thereby improving the user experience with quicker and more accurate responses [2].\n",
            "\n",
            "These computational efficiencies make Modular RAG not only a more powerful option for handling complex, knowledge-intensive tasks but also a cost-effective solution for businesses looking to optimize their AI deployments.\n",
            "\n",
            "Sources:\n",
            "[2] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "[5] https://www.marktechpost.com/2024/04/01/evolution-of-rags-naive-rag-advanced-rag-and-modular-rag-architectures/\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Thank you so much for your help, Dr. Zhang!\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
            "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
            "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://livebook.manning.com/book/a-simple-guide-to-retrieval-augmented-generation/chapter-6/v-4\"/>\n",
            "6 Progression of RAG Systems: Naïve to Advanced, and Modular RAG · A Simple Guide to Retrieval Augmented Generation 6 Progression of RAG Systems: Naïve to Advanced, and Modular RAG Limitations of Naïve RAG approach Advanced RAG strategies and techniques Modular patterns in RAG The basic, or the Naïve RAG approach that we have discussed is, generally, inadequate when it comes to production-grade systems. In this chapter, we will begin by revisiting the limitations and the points of failure of the Naïve RAG approach. Advanced strategies and techniques to address these points of failure will be understood in distinct phases of the RAG pipeline. 6.1 Limitations of Naïve RAG 6.2 Advanced RAG techniques 6.3 Modular RAG\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2406.00944v2\" date=\"2024-10-17\" authors=\"Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng\"/>\n",
            "<Title>\n",
            "A Theory for Token-Level Harmonization in Retrieval-Augmented Generation\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\n",
            "large language models (LLMs). Studies show that while RAG provides valuable\n",
            "external information (benefit), it may also mislead LLMs (detriment) with noisy\n",
            "or incorrect retrieved texts. Although many existing methods attempt to\n",
            "preserve benefit and avoid detriment, they lack a theoretical explanation for\n",
            "RAG. The benefit and detriment in the next token prediction of RAG remain a\n",
            "black box that cannot be quantified or compared in an explainable manner, so\n",
            "existing methods are data-driven, need additional utility evaluators or\n",
            "post-hoc. This paper takes the first step towards providing a theory to explain\n",
            "and trade off the benefit and detriment in RAG. First, we model RAG as the\n",
            "fusion between distribution of LLMs knowledge and distribution of retrieved\n",
            "texts. Then, we formalize the trade-off between the value of external knowledge\n",
            "(benefit) and its potential risk of misleading LLMs (detriment) in next token\n",
            "prediction of RAG by distribution difference in this fusion. Finally, we prove\n",
            "that the actual effect of RAG on the token, which is the comparison between\n",
            "benefit and detriment, can be predicted without any training or accessing the\n",
            "utility of retrieval. Based on our theory, we propose a practical novel method,\n",
            "Tok-RAG, which achieves collaborative generation between the pure LLM and RAG\n",
            "at token level to preserve benefit and avoid detriment. Experiments in\n",
            "real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
            "effectiveness of our method and support our theoretical findings.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "A THEORY FOR TOKEN-LEVEL HARMONIZATION IN\n",
            "RETRIEVAL-AUGMENTED GENERATION\n",
            "Shicheng Xu\n",
            "Liang Pang∗Huawei Shen\n",
            "Xueqi Cheng\n",
            "CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS\n",
            "{xushicheng21s,pangliang,shenhuawei,cxq}@ict.ac.cn\n",
            "ABSTRACT\n",
            "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance large\n",
            "language models (LLMs). Studies show that while RAG provides valuable external\n",
            "information (benefit), it may also mislead LLMs (detriment) with noisy or incorrect\n",
            "retrieved texts. Although many existing methods attempt to preserve benefit and\n",
            "avoid detriment, they lack a theoretical explanation for RAG. The benefit and\n",
            "detriment in the next token prediction of RAG remain a ’black box’ that cannot\n",
            "be quantified or compared in an explainable manner, so existing methods are data-\n",
            "driven, need additional utility evaluators or post-hoc. This paper takes the first step\n",
            "towards providing a theory to explain and trade off the benefit and detriment in\n",
            "RAG. First, we model RAG as the fusion between distribution of LLM’s knowledge\n",
            "and distribution of retrieved texts. Then, we formalize the trade-off between the\n",
            "value of external knowledge (benefit) and its potential risk of misleading LLMs\n",
            "(detriment) in next token prediction of RAG by distribution difference in this\n",
            "fusion. Finally, we prove that the actual effect of RAG on the token, which is the\n",
            "comparison between benefit and detriment, can be predicted without any training or\n",
            "accessing the utility of retrieval. Based on our theory, we propose a practical novel\n",
            "method, Tok-RAG, which achieves collaborative generation between the pure\n",
            "LLM and RAG at token level to preserve benefit and avoid detriment. Experiments\n",
            "in real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
            "effectiveness of our method and support our theoretical findings.\n",
            "1\n",
            "INTRODUCTION\n",
            "Retrieval-augmented generation (RAG) has shown promising performance in enhancing Large\n",
            "Language Models (LLMs) by integrating retrieved texts (Xu et al., 2023; Shi et al., 2023; Asai et al.,\n",
            "2023; Ram et al., 2023). Studies indicate that while RAG provides LLMs with valuable additional\n",
            "knowledge (benefit), it also poses a risk of misleading them (detriment) due to noisy or incorrect\n",
            "retrieved texts (Ram et al., 2023; Xu et al., 2024b;a; Jin et al., 2024a; Xie et al., 2023; Jin et al.,\n",
            "2024b). Existing methods attempt to preserve benefit and avoid detriment by adding utility evaluators\n",
            "for retrieval, prompt engineering, or fine-tuning LLMs (Asai et al., 2023; Ding et al., 2024; Xu et al.,\n",
            "2024b; Yoran et al., 2024; Ren et al., 2023; Feng et al., 2023; Mallen et al., 2022; Jiang et al., 2023).\n",
            "However, existing methods are data-driven, need evaluator for utility of retrieved texts or post-hoc. A\n",
            "theory-based method, focusing on core principles of RAG is urgently needed, which is crucial for\n",
            "consistent and reliable improvements without relying on additional training or utility evaluators and\n",
            "improving our understanding for RAG.\n",
            "This paper takes the first step in providing a theoretical framework to explain and trade off the benefit\n",
            "and detriment at token level in RAG and proposes a novel method to preserve benefit and avoid\n",
            "detriment based on our theoretical findings. Specifically, this paper pioneers in modeling next token\n",
            "prediction in RAG as the fusion between the distribution of LLM’s knowledge and the distribution\n",
            "of retrieved texts as shown in Figure 1. Our theoretical derivation based on this formalizes the core\n",
            "of this fusion as the subtraction between two terms measured by the distribution difference: one is\n",
            "distribution completion and the other is distribution contradiction. Further analysis indicates that\n",
            "the distribution completion measures how much out-of-distribution knowledge that retrieved texts\n",
            "∗Corresponding author\n",
            "1\n",
            "arXiv:2406.00944v2  [cs.CL]  17 Oct 2024\n",
            "Query\n",
            "Wole\n",
            "Query\n",
            "Ernst\n",
            "Soyinka\n",
            "…\n",
            "LLM’s \n",
            "Distribution\n",
            "Retrieved \n",
            "Distribution \n",
            "Fusion\n",
            "Distribution\n",
            "Difference\n",
            "Olanipekun\n",
            "LLM’s \n",
            "Dis\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
            "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
            "increasing demands of application scenarios have driven the evolution of RAG,\n",
            "leading to the integration of advanced retrievers, LLMs and other complementary\n",
            "technologies, which in turn has amplified the intricacy of RAG systems.\n",
            "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process of\n",
            "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
            "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
            "decomposing complex RAG systems into independent modules and specialized\n",
            "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a more advanced\n",
            "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
            "extensive research, this paper further identifies prevalent RAG\n",
            "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
            "analysis of their respective implementation nuances. Modular RAG presents\n",
            "innovative opportunities for the conceptualization and deployment of RAG\n",
            "systems. Finally, the paper explores the potential emergence of new operators\n",
            "and paradigms, establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment of RAG\n",
            "technologies.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2410.12812v1\" date=\"2024-10-01\" authors=\"Sarah Packowski, Inge Halilovic, Jenifer Schlotfeldt, Trish Smith\"/>\n",
            "<Title>\n",
            "Optimizing and Evaluating Enterprise Retrieval-Augmented Generation (RAG): A Content Design Perspective\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented generation (RAG) is a popular technique for using large\n",
            "language models (LLMs) to build customer-support, question-answering solutions.\n",
            "In this paper, we share our team's practical experience building and\n",
            "maintaining enterprise-scale RAG solutions that answer users' questions about\n",
            "our software based on product documentation. Our experience has not always\n",
            "matched the most common patterns in the RAG literature. This paper focuses on\n",
            "solution strategies that are modular and model-agnostic. For example, our\n",
            "experience over the past few years - using different search methods and LLMs,\n",
            "and many knowledge base collections - has been that simple changes to the way\n",
            "we create knowledge base content can have a huge impact on our RAG solutions'\n",
            "success. In this paper, we also discuss how we monitor and evaluate results.\n",
            "Common RAG benchmark evaluation techniques have not been useful for evaluating\n",
            "responses to novel user questions, so we have found a flexible, \"human in the\n",
            "lead\" approach is required.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "Optimizing and Evaluating Enterprise Retrieval-Augmented\n",
            "Generation (RAG): A Content Design Perspective\n",
            "Sarah Packowski\n",
            "spackows@ca.ibm.com\n",
            "IBM\n",
            "Canada\n",
            "Inge Halilovic\n",
            "ingeh@us.ibm.com\n",
            "IBM\n",
            "United States\n",
            "Jenifer Schlotfeldt\n",
            "jschlot@us.ibm.com\n",
            "IBM\n",
            "United States\n",
            "Trish Smith\n",
            "smith@ca.ibm.com\n",
            "IBM\n",
            "Canada\n",
            "ABSTRACT\n",
            "Retrieval-augmented generation (RAG) is a popular technique for\n",
            "using large language models (LLMs) to build customer-support,\n",
            "question-answering solutions. In this paper, we share our team’s\n",
            "practical experience building and maintaining enterprise-scale RAG\n",
            "solutions that answer users’ questions about our software based on\n",
            "product documentation. Our experience has not always matched\n",
            "the most common patterns in the RAG literature. This paper focuses\n",
            "on solution strategies that are modular and model-agnostic. For\n",
            "example, our experience over the past few years - using different\n",
            "search methods and LLMs, and many knowledge base collections -\n",
            "has been that simple changes to the way we create knowledge base\n",
            "content can have a huge impact on our RAG solutions’ success. In\n",
            "this paper, we also discuss how we monitor and evaluate results.\n",
            "Common RAG benchmark evaluation techniques have not been\n",
            "useful for evaluating responses to novel user questions, so we have\n",
            "found a flexible, \"human in the lead\" approach is required.\n",
            "CCS CONCEPTS\n",
            "• Computing methodologies →Artificial intelligence; Natu-\n",
            "ral language generation; • Applied computing →Document\n",
            "management and text processing.\n",
            "KEYWORDS\n",
            "Retrieval-augmented generation, RAG, Large language models\n",
            "ACM Reference Format:\n",
            "Sarah Packowski, Inge Halilovic, Jenifer Schlotfeldt, and Trish Smith. 2024.\n",
            "Optimizing and Evaluating Enterprise Retrieval-Augmented Generation\n",
            "(RAG): A Content Design Perspective. In Proceedings of 8th International\n",
            "Conference on Advances in Artificial Intelligence (ICAAI ’24). ACM, New York,\n",
            "NY, USA, 6 pages.\n",
            "Permission to make digital or hard copies of part or all of this work for personal or\n",
            "classroom use is granted without fee provided that copies are not made or distributed\n",
            "for profit or commercial advantage and that copies bear this notice and the full citation\n",
            "on the first page. Copyrights for third-party components of this work must be honored.\n",
            "For all other uses, contact the owner/author(s).\n",
            "ICAAI ’24, October 2024, London, UK\n",
            "© 2024 Copyright held by the owner/author(s).\n",
            "ACM ISBN 979-8-4007-1801-4/24/10.\n",
            "1\n",
            "INTRODUCTION\n",
            "Retrieval-augmented generation (RAG) is an effective way to use\n",
            "large language models (LLMs) to answer questions while avoiding\n",
            "hallucinations and factual inaccuracy[12, 20, 46]. Basic RAG is sim-\n",
            "ple: 1) search a knowledge base for relevant content; 2) compose a\n",
            "prompt grounded in the retrieved content; and 3) prompt an LLM to\n",
            "generate output. For the retrieval step, one approach dominates the\n",
            "literature: 1) segment content text into chunks; 2) index vectorized\n",
            "chunks for search in a vector database; and 3) when generating\n",
            "answers, ground prompts in a subset of retrieved chunks[13]. Our\n",
            "RAG solutions don’t always use vector databases for search.\n",
            "Wikipedia has long been influenced by and had an influence on\n",
            "scientific research [21, 41]. With respect to RAG, Wikipedia is a dom-\n",
            "inant source of knowledge base content for training data and bench-\n",
            "marks, including: 2WikiMultiHopQA, AmbigQA, ASQA, DART,\n",
            "FEVER, HotpotQA, KILT, MuSiQue, Natural Questions, NoMIRACL,\n",
            "PopQA, SQuAD, StrategyQA, SuperGLUE, TriviaQA, WikiAsp, Wik-\n",
            "iBio, WikiEval, and Wizard of Wikipedia[8, 9, 14–16, 18, 22, 23, 25,\n",
            "28, 29, 31, 34, 39, 40, 42–44, 48]. The knowledge base for our team’s\n",
            "RAG solutions is our own product documentation, which is struc-\n",
            "tured differently from Wikipedia articles.\n",
            "Using common benchmarks to test your RAG implementation\n",
            "involves these steps: 1) index the given knowledge base content in\n",
            "your retriever component; 2) prompt your solution to answer the\n",
            "given questions; and 3) compare generated answers to expected\n",
            "answers, using methods such as exact match, cos\n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "You're welcome! If you have any more questions or need further clarification on any topic, feel free to ask. I'm here to help. Good luck with your article!\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msave_interview\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32minterview\u001b[0m:\n",
            "Human: So you said you were writing an article on What are the differences between Modular RAG and Naive RAG, and what are the benefits of using it at production level?\n",
            "AI: Hello, my name is Alex Turner, and I'm an analyst interested in learning about the fascinating world of AI architectures. Dr. Zhang, could you explain the fundamental differences between Modular RAG and Naive RAG, particularly in terms of their architecture and how these differences impact their performance?\n",
            "AI: Certainly, Dr. Zhang. The fundamental differences between Modular RAG and Naive RAG primarily lie in their architectural design and flexibility.\n",
            "\n",
            "1. **Architecture and Design**:\n",
            "   - **Naive RAG**: This approach combines information retrieval with natural language generation, using retrieval models that rank data based on relevance to the input query. It generally has a linear architecture, where the process is \"retrieve-then-generate,\" which may limit its ability to handle complex queries with substantial variability [4].\n",
            "   - **Modular RAG**: This framework transforms RAG systems into reconfigurable, LEGO-like structures by decomposing them into independent modules and specialized operators. It transcends the traditional linear architecture by incorporating advanced design elements like routing, scheduling, and fusion mechanisms. This modularity allows for greater flexibility and customization, enabling the system to adapt more effectively to specific tasks or industries [2][3].\n",
            "\n",
            "2. **Performance and Benefits**:\n",
            "   - **Naive RAG** faces challenges such as shallow understanding of queries and retrieval redundancy, which can introduce noise and reduce the quality of generated responses. Its rigid architecture limits customization and adaptation to domain-specific needs, making it less suitable for complex or specialized applications [5].\n",
            "   - **Modular RAG** offers significant advantages in production settings due to its reconfigurable nature. The modular design allows for the integration of advanced retrievers, LLMs, and other technologies, improving efficiency and scalability. This adaptability makes it suitable for a wide range of applications, from knowledge-intensive tasks to industry-specific solutions, by allowing easy incorporation of custom modules tailored to specific requirements [2][3].\n",
            "\n",
            "Overall, Modular RAG's enhanced flexibility and capability to manage complex tasks make it a more robust choice for production-level deployments, providing a foundation for scalable and efficient AI systems.\n",
            "\n",
            "Sources:\n",
            "[1] http://arxiv.org/abs/2407.21059v1\n",
            "[2] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "[4] http://arxiv.org/abs/2409.11598v2\n",
            "[5] https://www.marktechpost.com/2024/04/01/evolution-of-rags-naive-rag-advanced-rag-and-modular-rag-architectures/\n",
            "AI: Thank you for clarifying these distinctions, Dr. Zhang. Could you provide an example of a real-world application where Modular RAG's adaptability significantly enhances performance compared to Naive RAG? Specifically, how does its modularity contribute to better scalability or efficiency in that scenario?\n",
            "AI: Certainly, Dr. Zhang. A real-world application where Modular RAG's adaptability significantly enhances performance compared to Naive RAG is in the domain of customer support chatbots.\n",
            "\n",
            "In this scenario, customer support chatbots need to handle a wide variety of queries that may require accessing domain-specific knowledge or integrating with other systems. Naive RAG, with its linear \"retrieve-then-generate\" architecture, may struggle to efficiently handle the diversity of queries due to its reliance on straightforward similarity measures and the potential noise from redundant information [5]. This can lead to less relevant or slower responses, impacting customer satisfaction.\n",
            "\n",
            "Modular RAG, on the other hand, excels in this environment due to its reconfigurable nature. By decomposing the RAG system into independent modules, it allows for the integration of specialized operators and advanced retrievers tailored to the chatbot's specific domain or industry requirements. This modularity facilitates better routing and scheduling of tasks, ensuring that the most relevant information is retrieved and generated efficiently. As a result, Modular RAG can deliver more precise and contextually relevant responses, improving both the speed and quality of the customer service experience [3].\n",
            "\n",
            "The adaptability of Modular RAG also enables the chatbot to scale more effectively, as new modules can be added or existing ones refined without overhauling the entire system. This scalability is crucial for businesses looking to expand their support capabilities or integrate new technologies over time [2][3].\n",
            "\n",
            "Sources:\n",
            "[2] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "[5] https://www.marktechpost.com/2024/04/01/evolution-of-rags-naive-rag-advanced-rag-and-modular-rag-architectures/\n",
            "AI: That's a fascinating example of how Modular RAG can offer tangible benefits in a real-world setting. Could you elaborate on any specific computational efficiencies that Modular RAG introduces, and how these might translate to cost savings or enhanced user experience in practice?\n",
            "AI: Certainly, Dr. Zhang. Modular RAG introduces several computational efficiencies that can translate to cost savings and enhanced user experience:\n",
            "\n",
            "1. **Efficient Resource Utilization**: By employing specialized modules and operators, Modular RAG can dynamically allocate resources based on the specific needs of each task. This means that computational power is used more effectively, focusing on retrieving and processing only the most relevant information. This efficiency reduces unnecessary computational load, which can directly lead to cost savings in terms of reduced resource consumption [2].\n",
            "\n",
            "2. **Reduced Redundancy and Noise**: Modular RAG's architecture includes mechanisms to filter out redundant and noisy data before it reaches the language model. This reduces the computational effort required to process extraneous information, allowing the system to operate more quickly and with less strain on computational resources. As a result, users experience faster response times, which enhances the overall user experience [5].\n",
            "\n",
            "3. **Scalability and Flexibility**: The modular structure of Modular RAG allows organizations to scale their systems up or down easily, adding new capabilities or adjusting existing ones without needing to redesign the entire architecture. This flexibility not only optimizes the use of computational resources but also means that businesses can adapt quickly to changes in demand or new technological advancements, maintaining efficiency and cost-effectiveness [3].\n",
            "\n",
            "4. **Improved Query Handling**: By utilizing advanced routing and scheduling mechanisms, Modular RAG can prioritize and route queries more effectively. This ensures that complex or high-priority queries receive the necessary computational attention, while simpler tasks are handled with minimal resource allocation. This targeted approach enhances processing efficiency, thereby improving the user experience with quicker and more accurate responses [2].\n",
            "\n",
            "These computational efficiencies make Modular RAG not only a more powerful option for handling complex, knowledge-intensive tasks but also a cost-effective solution for businesses looking to optimize their AI deployments.\n",
            "\n",
            "Sources:\n",
            "[2] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "[5] https://www.marktechpost.com/2024/04/01/evolution-of-rags-naive-rag-advanced-rag-and-modular-rag-architectures/\n",
            "AI: Thank you so much for your help, Dr. Zhang!\n",
            "AI: You're welcome! If you have any more questions or need further clarification on any topic, feel free to ask. I'm here to help. Good luck with your article!\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mwrite_section\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Exploring the Architectural Advancements in Modular and Naive RAG\n",
            "\n",
            "### Summary\n",
            "\n",
            "Retrieval-Augmented Generation (RAG) is a promising technique that enhances the capabilities of large language models (LLMs) by incorporating external data through retrieval mechanisms. This report focuses on the architectural distinctions between Modular RAG and Naive RAG, highlighting their respective computational efficiencies and adaptability in various AI applications. The evolution from Naive RAG to Modular RAG marks a significant shift in how LLMs handle knowledge-intensive tasks, prompting a need for a deeper understanding of their optimization for performance and scalability.\n",
            "\n",
            "Naive RAG, foundational in its approach, integrates information retrieval with language generation to produce contextually relevant responses. However, it often struggles with inflexibility and inefficiencies when dealing with diverse datasets, primarily due to its reliance on straightforward similarity calculations for retrieval. In contrast, Modular RAG introduces a reconfigurable framework by decomposing complex RAG systems into independent modules, facilitating more sophisticated routing, scheduling, and fusion mechanisms. This modularity allows for more precise control and customization, making Modular RAG systems more adaptable to specific application needs.\n",
            "\n",
            "The novelty of insights gathered from recent literature reveals a trend towards modular frameworks to overcome the limitations of traditional RAG systems. Notably, the theory of token-level harmonization in RAG presents a novel approach to balancing the benefits and detriments of retrieval, offering a theoretical foundation that could enhance the precision of LLM responses.\n",
            "\n",
            "Key source documents include:\n",
            "1. [1] http://arxiv.org/abs/2406.00944v2\n",
            "2. [2] http://arxiv.org/abs/2407.21059v1\n",
            "3. [3] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "4. [4] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "5. [5] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "The evolution of Retrieval-Augmented Generation (RAG) systems has led to significant advancements in how large language models (LLMs) integrate external information to enhance their generative capabilities. This section delves into the architectural and operational differences between Naive RAG and Modular RAG, emphasizing their impact on computational efficiency and adaptability.\n",
            "\n",
            "#### Naive RAG: Foundations and Limitations\n",
            "\n",
            "Naive RAG represents the initial phase of RAG systems, primarily characterized by its \"retrieve-then-generate\" approach. This model combines document retrieval with language model generation, aiming to produce coherent and contextually relevant responses. However, several challenges undermine its effectiveness:\n",
            "\n",
            "- **Shallow Understanding of Queries**: Naive RAG relies heavily on semantic similarity for retrieval, which can result in inadequate exploration of the query-document relationship. This limitation often leads to a failure in capturing nuanced query intents, affecting the accuracy of the generated responses [2].\n",
            "  \n",
            "- **Retrieval Redundancy and Noise**: The process of feeding all retrieved chunks into LLMs can introduce excessive noise, potentially misleading the model and increasing the risk of generating hallucinated responses. This redundancy highlights the need for more refined retrieval mechanisms [5].\n",
            "\n",
            "- **Inflexibility**: The rigid architecture of Naive RAG restricts its adaptability to diverse and dynamic datasets, making it less suitable for specialized tasks or industries [4].\n",
            "\n",
            "#### Modular RAG: A Reconfigurable Framework\n",
            "\n",
            "Modular RAG addresses these limitations by introducing a highly reconfigurable framework that decomposes RAG systems into independent modules and specialized operators. This modular approach offers several advantages:\n",
            "\n",
            "- **Advanced Design**: By integrating routing, scheduling, and fusion mechanisms, Modular RAG transcends the traditional linear architecture, allowing for more sophisticated data handling and processing [2].\n",
            "\n",
            "- **Customization and Scalability**: The modularity of the system enables organizations to tailor RAG systems to specific application needs, enhancing relevance, response times, and customer satisfaction. This adaptability is crucial for scaling RAG systems across various domains [4].\n",
            "\n",
            "- **Enhanced Retrieval and Generation**: The modular framework supports the inclusion of advanced retrievers and complementary technologies, improving the overall performance of RAG systems in handling complex queries and variable data [3].\n",
            "\n",
            "#### Token-Level Harmonization Theory\n",
            "\n",
            "A significant theoretical advancement in RAG systems is the introduction of a token-level harmonization theory. This approach models RAG as a fusion between the distribution of LLM knowledge and retrieved texts. It formalizes the trade-off between the value of external knowledge (benefit) and its potential to mislead LLMs (detriment) in next token prediction. This theoretical framework allows for a more explainable and quantifiable comparison of benefits and detriments, facilitating a balanced integration of external data [1].\n",
            "\n",
            "#### Implications for AI Applications\n",
            "\n",
            "The transition from Naive to Modular RAG represents a paradigm shift in how LLMs are utilized in AI applications. Modular RAG's adaptability and efficiency make it a preferable choice for tasks requiring high scalability and specialization. The ongoing research into token-level harmonization further enhances the precision and reliability of RAG systems, paving the way for more robust and contextually aware AI solutions.\n",
            "\n",
            "### Sources\n",
            "[1] http://arxiv.org/abs/2406.00944v2  \n",
            "[2] http://arxiv.org/abs/2407.21059v1  \n",
            "[3] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[4] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
            "[5] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "# Set research topic\n",
        "topic = \"What are the differences between Modular RAG and Naive RAG, and what are the benefits of using it at production level\"\n",
        "\n",
        "# Create initial interview message\n",
        "messages = [HumanMessage(f\"So you said you were writing an article on {topic}?\")]\n",
        "\n",
        "# Configure thread ID\n",
        "config = RunnableConfig(\n",
        "    recursion_limit=100,\n",
        "    configurable={\"thread_id\": random_uuid()},\n",
        ")\n",
        "\n",
        "# Execute graph\n",
        "invoke_graph(\n",
        "    interview_graph,\n",
        "    {\"analyst\": analysts[0], \"messages\": messages, \"max_num_turns\": 5},\n",
        "    config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a4090d7",
      "metadata": {},
      "source": [
        "Display completed interview section in markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "2eef4a1f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Exploring the Architectural Advancements in Modular and Naive RAG\n",
              "\n",
              "### Summary\n",
              "\n",
              "Retrieval-Augmented Generation (RAG) is a promising technique that enhances the capabilities of large language models (LLMs) by incorporating external data through retrieval mechanisms. This report focuses on the architectural distinctions between Modular RAG and Naive RAG, highlighting their respective computational efficiencies and adaptability in various AI applications. The evolution from Naive RAG to Modular RAG marks a significant shift in how LLMs handle knowledge-intensive tasks, prompting a need for a deeper understanding of their optimization for performance and scalability.\n",
              "\n",
              "Naive RAG, foundational in its approach, integrates information retrieval with language generation to produce contextually relevant responses. However, it often struggles with inflexibility and inefficiencies when dealing with diverse datasets, primarily due to its reliance on straightforward similarity calculations for retrieval. In contrast, Modular RAG introduces a reconfigurable framework by decomposing complex RAG systems into independent modules, facilitating more sophisticated routing, scheduling, and fusion mechanisms. This modularity allows for more precise control and customization, making Modular RAG systems more adaptable to specific application needs.\n",
              "\n",
              "The novelty of insights gathered from recent literature reveals a trend towards modular frameworks to overcome the limitations of traditional RAG systems. Notably, the theory of token-level harmonization in RAG presents a novel approach to balancing the benefits and detriments of retrieval, offering a theoretical foundation that could enhance the precision of LLM responses.\n",
              "\n",
              "Key source documents include:\n",
              "1. [1] http://arxiv.org/abs/2406.00944v2\n",
              "2. [2] http://arxiv.org/abs/2407.21059v1\n",
              "3. [3] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
              "4. [4] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
              "5. [5] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
              "\n",
              "### Comprehensive Analysis\n",
              "\n",
              "The evolution of Retrieval-Augmented Generation (RAG) systems has led to significant advancements in how large language models (LLMs) integrate external information to enhance their generative capabilities. This section delves into the architectural and operational differences between Naive RAG and Modular RAG, emphasizing their impact on computational efficiency and adaptability.\n",
              "\n",
              "#### Naive RAG: Foundations and Limitations\n",
              "\n",
              "Naive RAG represents the initial phase of RAG systems, primarily characterized by its \"retrieve-then-generate\" approach. This model combines document retrieval with language model generation, aiming to produce coherent and contextually relevant responses. However, several challenges undermine its effectiveness:\n",
              "\n",
              "- **Shallow Understanding of Queries**: Naive RAG relies heavily on semantic similarity for retrieval, which can result in inadequate exploration of the query-document relationship. This limitation often leads to a failure in capturing nuanced query intents, affecting the accuracy of the generated responses [2].\n",
              "  \n",
              "- **Retrieval Redundancy and Noise**: The process of feeding all retrieved chunks into LLMs can introduce excessive noise, potentially misleading the model and increasing the risk of generating hallucinated responses. This redundancy highlights the need for more refined retrieval mechanisms [5].\n",
              "\n",
              "- **Inflexibility**: The rigid architecture of Naive RAG restricts its adaptability to diverse and dynamic datasets, making it less suitable for specialized tasks or industries [4].\n",
              "\n",
              "#### Modular RAG: A Reconfigurable Framework\n",
              "\n",
              "Modular RAG addresses these limitations by introducing a highly reconfigurable framework that decomposes RAG systems into independent modules and specialized operators. This modular approach offers several advantages:\n",
              "\n",
              "- **Advanced Design**: By integrating routing, scheduling, and fusion mechanisms, Modular RAG transcends the traditional linear architecture, allowing for more sophisticated data handling and processing [2].\n",
              "\n",
              "- **Customization and Scalability**: The modularity of the system enables organizations to tailor RAG systems to specific application needs, enhancing relevance, response times, and customer satisfaction. This adaptability is crucial for scaling RAG systems across various domains [4].\n",
              "\n",
              "- **Enhanced Retrieval and Generation**: The modular framework supports the inclusion of advanced retrievers and complementary technologies, improving the overall performance of RAG systems in handling complex queries and variable data [3].\n",
              "\n",
              "#### Token-Level Harmonization Theory\n",
              "\n",
              "A significant theoretical advancement in RAG systems is the introduction of a token-level harmonization theory. This approach models RAG as a fusion between the distribution of LLM knowledge and retrieved texts. It formalizes the trade-off between the value of external knowledge (benefit) and its potential to mislead LLMs (detriment) in next token prediction. This theoretical framework allows for a more explainable and quantifiable comparison of benefits and detriments, facilitating a balanced integration of external data [1].\n",
              "\n",
              "#### Implications for AI Applications\n",
              "\n",
              "The transition from Naive to Modular RAG represents a paradigm shift in how LLMs are utilized in AI applications. Modular RAG's adaptability and efficiency make it a preferable choice for tasks requiring high scalability and specialization. The ongoing research into token-level harmonization further enhances the precision and reliability of RAG systems, paving the way for more robust and contextually aware AI solutions.\n",
              "\n",
              "### Sources\n",
              "[1] http://arxiv.org/abs/2406.00944v2  \n",
              "[2] http://arxiv.org/abs/2407.21059v1  \n",
              "[3] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
              "[4] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
              "[5] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  "
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(interview_graph.get_state(config).values[\"sections\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "eec5b106",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Exploring the Architectural Advancements in Modular and Naive RAG\n",
            "\n",
            "### Summary\n",
            "\n",
            "Retrieval-Augmented Generation (RAG) is a promising technique that enhances the capabilities of large language models (LLMs) by incorporating external data through retrieval mechanisms. This report focuses on the architectural distinctions between Modular RAG and Naive RAG, highlighting their respective computational efficiencies and adaptability in various AI applications. The evolution from Naive RAG to Modular RAG marks a significant shift in how LLMs handle knowledge-intensive tasks, prompting a need for a deeper understanding of their optimization for performance and scalability.\n",
            "\n",
            "Naive RAG, foundational in its approach, integrates information retrieval with language generation to produce contextually relevant responses. However, it often struggles with inflexibility and inefficiencies when dealing with diverse datasets, primarily due to its reliance on straightforward similarity calculations for retrieval. In contrast, Modular RAG introduces a reconfigurable framework by decomposing complex RAG systems into independent modules, facilitating more sophisticated routing, scheduling, and fusion mechanisms. This modularity allows for more precise control and customization, making Modular RAG systems more adaptable to specific application needs.\n",
            "\n",
            "The novelty of insights gathered from recent literature reveals a trend towards modular frameworks to overcome the limitations of traditional RAG systems. Notably, the theory of token-level harmonization in RAG presents a novel approach to balancing the benefits and detriments of retrieval, offering a theoretical foundation that could enhance the precision of LLM responses.\n",
            "\n",
            "Key source documents include:\n",
            "1. [1] http://arxiv.org/abs/2406.00944v2\n",
            "2. [2] http://arxiv.org/abs/2407.21059v1\n",
            "3. [3] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "4. [4] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "5. [5] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "The evolution of Retrieval-Augmented Generation (RAG) systems has led to significant advancements in how large language models (LLMs) integrate external information to enhance their generative capabilities. This section delves into the architectural and operational differences between Naive RAG and Modular RAG, emphasizing their impact on computational efficiency and adaptability.\n",
            "\n",
            "#### Naive RAG: Foundations and Limitations\n",
            "\n",
            "Naive RAG represents the initial phase of RAG systems, primarily characterized by its \"retrieve-then-generate\" approach. This model combines document retrieval with language model generation, aiming to produce coherent and contextually relevant responses. However, several challenges undermine its effectiveness:\n",
            "\n",
            "- **Shallow Understanding of Queries**: Naive RAG relies heavily on semantic similarity for retrieval, which can result in inadequate exploration of the query-document relationship. This limitation often leads to a failure in capturing nuanced query intents, affecting the accuracy of the generated responses [2].\n",
            "  \n",
            "- **Retrieval Redundancy and Noise**: The process of feeding all retrieved chunks into LLMs can introduce excessive noise, potentially misleading the model and increasing the risk of generating hallucinated responses. This redundancy highlights the need for more refined retrieval mechanisms [5].\n",
            "\n",
            "- **Inflexibility**: The rigid architecture of Naive RAG restricts its adaptability to diverse and dynamic datasets, making it less suitable for specialized tasks or industries [4].\n",
            "\n",
            "#### Modular RAG: A Reconfigurable Framework\n",
            "\n",
            "Modular RAG addresses these limitations by introducing a highly reconfigurable framework that decomposes RAG systems into independent modules and specialized operators. This modular approach offers several advantages:\n",
            "\n",
            "- **Advanced Design**: By integrating routing, scheduling, and fusion mechanisms, Modular RAG transcends the traditional linear architecture, allowing for more sophisticated data handling and processing [2].\n",
            "\n",
            "- **Customization and Scalability**: The modularity of the system enables organizations to tailor RAG systems to specific application needs, enhancing relevance, response times, and customer satisfaction. This adaptability is crucial for scaling RAG systems across various domains [4].\n",
            "\n",
            "- **Enhanced Retrieval and Generation**: The modular framework supports the inclusion of advanced retrievers and complementary technologies, improving the overall performance of RAG systems in handling complex queries and variable data [3].\n",
            "\n",
            "#### Token-Level Harmonization Theory\n",
            "\n",
            "A significant theoretical advancement in RAG systems is the introduction of a token-level harmonization theory. This approach models RAG as a fusion between the distribution of LLM knowledge and retrieved texts. It formalizes the trade-off between the value of external knowledge (benefit) and its potential to mislead LLMs (detriment) in next token prediction. This theoretical framework allows for a more explainable and quantifiable comparison of benefits and detriments, facilitating a balanced integration of external data [1].\n",
            "\n",
            "#### Implications for AI Applications\n",
            "\n",
            "The transition from Naive to Modular RAG represents a paradigm shift in how LLMs are utilized in AI applications. Modular RAG's adaptability and efficiency make it a preferable choice for tasks requiring high scalability and specialization. The ongoing research into token-level harmonization further enhances the precision and reliability of RAG systems, paving the way for more robust and contextually aware AI solutions.\n",
            "\n",
            "### Sources\n",
            "[1] http://arxiv.org/abs/2406.00944v2  \n",
            "[2] http://arxiv.org/abs/2407.21059v1  \n",
            "[3] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[4] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
            "[5] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n"
          ]
        }
      ],
      "source": [
        "print(interview_graph.get_state(config).values[\"sections\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4f982d5",
      "metadata": {},
      "source": [
        "### Parallel Interviewing by `map-reduce`\n",
        "Here's how to implement parallel interviews using map-reduce in LangGraph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "ede721fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "import operator\n",
        "from typing import List, Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class ResearchGraphState(TypedDict):\n",
        "    \"\"\"State definition for research graph\"\"\"\n",
        "\n",
        "    # Research topic\n",
        "    topic: str\n",
        "    # Maximum number of analysts\n",
        "    max_analysts: int\n",
        "    # Human analyst feedback\n",
        "    human_analyst_feedback: str\n",
        "    # List of questioning analysts\n",
        "    analysts: List[Analyst]\n",
        "    # List of sections containing Send() API keys\n",
        "    sections: Annotated[list, operator.add]\n",
        "    # Report components\n",
        "    introduction: str\n",
        "    content: str\n",
        "    conclusion: str\n",
        "    final_report: str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3827de29",
      "metadata": {},
      "source": [
        "Let me explain how the `Send()` function is used in LangGraph for parallel interview execution:\n",
        "\n",
        "**Reference**\n",
        "- [LangGraph `Send()`](https://langchain-ai.github.io/langgraph/concepts/low_level/#send)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "d4389d5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.constants import Send\n",
        "\n",
        "\n",
        "def initiate_all_interviews(state: ResearchGraphState):\n",
        "    \"\"\"Initiates parallel interviews for all analysts\"\"\"\n",
        "\n",
        "    # Check for human feedback\n",
        "    human_analyst_feedback = state.get(\"human_analyst_feedback\")\n",
        "\n",
        "    # Return to analyst creation if human feedback exists\n",
        "    if human_analyst_feedback:\n",
        "        return \"create_analysts\"\n",
        "\n",
        "    # Otherwise, initiate parallel interviews using Send()\n",
        "    else:\n",
        "        topic = state[\"topic\"]\n",
        "        return [\n",
        "            Send(\n",
        "                \"conduct_interview\",\n",
        "                {\n",
        "                    \"analyst\": analyst,\n",
        "                    \"messages\": [\n",
        "                        HumanMessage(\n",
        "                            content=f\"So you said you were writing an article on {topic}?\"\n",
        "                        )\n",
        "                    ],\n",
        "                },\n",
        "            )\n",
        "            for analyst in state[\"analysts\"]\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fb746ea",
      "metadata": {},
      "source": [
        "## Report Writing\n",
        "\n",
        "Next, we will define the guidelines for writing a report based on the interview content and define a function for report writing.\n",
        "\n",
        "### Define Nodes\n",
        "- Main Report Content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "8c0d3b46",
      "metadata": {},
      "outputs": [],
      "source": [
        "report_writer_instructions = \"\"\"You are a technical writer creating a report on this overall topic:\n",
        "\n",
        "{topic}\n",
        "\n",
        "You have a team of analysts. Each analyst has done two things:\n",
        "\n",
        "1. They conducted an interview with an expert on a specific sub-topic.\n",
        "2. They write up their finding into a memo.\n",
        "\n",
        "Your task:\n",
        "\n",
        "1. You will be given a collection of memos from your analysts.  \n",
        "2. Carefully review and analyze the insights from each memo.  \n",
        "3. Consolidate these insights into a detailed and comprehensive summary that integrates the central ideas from all the memos.  \n",
        "4. Organize the key points from each memo into the appropriate sections provided below, ensuring that each section is logical and well-structured.  \n",
        "5. Include all required sections in your report, using `### Section Name` as the header for each.  \n",
        "6. Aim for approximately 250 words per section, providing in-depth explanations, context, and supporting details.  \n",
        "\n",
        "**Sections to consider (including optional ones for greater depth):**\n",
        "\n",
        "- **Background**: Theoretical foundations, key concepts, and preliminary information necessary to understand the methodology and results.\n",
        "- **Related Work**: Overview of prior studies and how they compare or relate to the current research.\n",
        "- **Problem Definition**: A formal and precise definition of the research question or problem the paper aims to address.\n",
        "- **Methodology (or Methods)**: Detailed description of the methods, algorithms, models, data collection processes, or experimental setups used in the study.\n",
        "- **Implementation Details**: Practical details of how the methods or models were implemented, including software frameworks, computational resources, or parameter settings.\n",
        "- **Experiments**: Explanation of experimental protocols, datasets, evaluation metrics, procedures, and configurations employed to validate the methods.\n",
        "- **Results**: Presentation of experimental outcomes, often with statistical tables, graphs, figures, or qualitative analyses.\n",
        "\n",
        "To format your report:\n",
        "\n",
        "1. Use markdown formatting.\n",
        "2. Include no pre-amble for the report.\n",
        "3. Use no sub-heading.\n",
        "4. Start your report with a single title header: ## Insights\n",
        "5. Do not mention any analyst names in your report.\n",
        "6. Preserve any citations in the memos, which will be annotated in brackets, for example [1] or [2].\n",
        "7. Create a final, consolidated list of sources and add to a Sources section with the `## Sources` header.\n",
        "8. List your sources in order and do not repeat.\n",
        "\n",
        "[1] Source 1\n",
        "[2] Source 2\n",
        "\n",
        "Here are the memos from your analysts to build your report from:\n",
        "\n",
        "{context}\"\"\"\n",
        "\n",
        "\n",
        "def write_report(state: ResearchGraphState):\n",
        "    \"\"\"Generates main report content from interview sections\"\"\"\n",
        "    sections = state[\"sections\"]\n",
        "    topic = state[\"topic\"]\n",
        "\n",
        "    # Combine all sections\n",
        "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
        "\n",
        "    # Generate report from sections\n",
        "    system_message = report_writer_instructions.format(\n",
        "        topic=topic, context=formatted_str_sections\n",
        "    )\n",
        "    report = llm.invoke(\n",
        "        [\n",
        "            SystemMessage(content=system_message),\n",
        "            HumanMessage(content=\"Write a report based upon these memos.\"),\n",
        "        ]\n",
        "    )\n",
        "    return {\"content\": report.content}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "978dbdf4",
      "metadata": {},
      "source": [
        "- Introduction Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "be9672e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "intro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}\n",
        "\n",
        "You will be given all of the sections of the report.\n",
        "\n",
        "You job is to write a crisp and compelling introduction or conclusion section.\n",
        "\n",
        "The user will instruct you whether to write the introduction or conclusion.\n",
        "\n",
        "Include no pre-amble for either section.\n",
        "\n",
        "Target around 200 words, crisply previewing (for introduction),  or recapping (for conclusion) all of the sections of the report.\n",
        "\n",
        "Use markdown formatting.\n",
        "\n",
        "For your introduction, create a compelling title and use the # header for the title.\n",
        "\n",
        "For your introduction, use ## Introduction as the section header.\n",
        "\n",
        "For your conclusion, use ## Conclusion as the section header.\n",
        "\n",
        "Here are the sections to reflect on for writing: {formatted_str_sections}\"\"\"\n",
        "\n",
        "\n",
        "def write_introduction(state: ResearchGraphState):\n",
        "    \"\"\"Creates report introduction\"\"\"\n",
        "    sections = state[\"sections\"]\n",
        "    topic = state[\"topic\"]\n",
        "\n",
        "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
        "\n",
        "    instructions = intro_conclusion_instructions.format(\n",
        "        topic=topic, formatted_str_sections=formatted_str_sections\n",
        "    )\n",
        "    intro = llm.invoke(\n",
        "        [instructions, HumanMessage(content=\"Write the report introduction\")]\n",
        "    )\n",
        "    return {\"introduction\": intro.content}\n",
        "\n",
        "\n",
        "def write_conclusion(state: ResearchGraphState):\n",
        "    \"\"\"Creates report conclusion\"\"\"\n",
        "    sections = state[\"sections\"]\n",
        "    topic = state[\"topic\"]\n",
        "\n",
        "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
        "\n",
        "    instructions = intro_conclusion_instructions.format(\n",
        "        topic=topic, formatted_str_sections=formatted_str_sections\n",
        "    )\n",
        "    conclusion = llm.invoke(\n",
        "        [instructions, HumanMessage(content=\"Write the report conclusion\")]\n",
        "    )\n",
        "    return {\"conclusion\": conclusion.content}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbf3d5e8",
      "metadata": {},
      "source": [
        "- Final Report Assembly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "29bbf7e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def finalize_report(state: ResearchGraphState):\n",
        "    \"\"\"Assembles final report with all components\"\"\"\n",
        "    content = state[\"content\"]\n",
        "\n",
        "    # Clean up content formatting\n",
        "    if content.startswith(\"## Insights\"):\n",
        "        content = content.strip(\"## Insights\")\n",
        "\n",
        "    # Handle sources section\n",
        "    if \"## Sources\" in content:\n",
        "        try:\n",
        "            content, sources = content.split(\"\\n## Sources\\n\")\n",
        "        except:\n",
        "            sources = None\n",
        "    else:\n",
        "        sources = None\n",
        "\n",
        "    # Assemble final report\n",
        "    final_report = (\n",
        "        state[\"introduction\"]\n",
        "        + \"\\n\\n---\\n\\n## Main Idea\\n\\n\"\n",
        "        + content\n",
        "        + \"\\n\\n---\\n\\n\"\n",
        "        + state[\"conclusion\"]\n",
        "    )\n",
        "\n",
        "    # Add sources if available\n",
        "    if sources is not None:\n",
        "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
        "\n",
        "    return {\"final_report\": final_report}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d83bbe53",
      "metadata": {},
      "source": [
        "Each function handles a specific aspect of report generation:\n",
        "- Content synthesis from interview sections\n",
        "- Introduction creation\n",
        "- Conclusion development\n",
        "- Final assembly with proper formatting and structure"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ef82f56",
      "metadata": {},
      "source": [
        "### Building the Report Writing Graph\n",
        "Here's the implementation of the research graph that orchestrates the entire workflow: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "c59ca60f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj0AAAKgCAIAAAD/C1vLAAAAAXNSR0IArs4c6QAAIABJREFUeJzs3XdcVfX/B/D35XIvl8ve47KHAspeIu49cs/MNHNk01FaaWZlmVY21DLTcuTOWYoLN6CiIAgKgmxk73XhXrj398cpvv4UFRTu4cDr+UePy7nnfO7rQvLic9blKZVKAgAA4Ag1tgMAAAC0AHoLAAC4BL0FAABcgt4CAAAuQW8BAACXoLcAAIBL+J999hnbGQA4QK5QhBZkJlaVFtbVXirO0RUIDQQa4cW57fyxoYZIT10YU15Y01BvIBSx/V0EaAWYbwE8kVKpPJh9f0l8eHW9vKBOmlBZWiqrq6iXV9fLy2SyYllt+39cUldbKq+7XV78R/rdm6X5CqUyrryY7e8rwAvh4bpjgMfJFQ1lchmfePsfJAcamtmKddlO1AqUSiWPxzuQnXy3suR7994CNfzZCpyE3gJ41L2q0o0pt5d28RXz1dnO0iZKZbW6AmGFXGakITIQYOchcAz+4AL4fxqUypSq8s9cAztqaRGRgVDE56lpqQvW3IsqqJOyHQegZTDfAvif39LuTJQ4dqp/EnEVRX2NJDwej+0gAM2F+RbAv7ak3zEWijpVaRGRu67xhaIHOdJqtoMANBfmWwBERAqlslwuU1An/eew/n7sa7au9lod4fQT6PDQWwCUXVMZU1EcZGjOdhDWKJTKmoZ6W7EO20EAng37CQHox5RYD11DtlOwSY3HI6ISWS3bQQCeDb0FnV1RnXS+g7uWupDtICzTVhesSrxRLq9jOwjAM2A/IXR2tQ311Q31bKdoF2LKCvlqav2MJWwHAXgazLegU4sqLVh976YqX7GhoSHi8rkX+Xuxuroq6npYq4b6l5e+SZBB5z3IB1yB3oJOLaw4113XSJWvuOz9eRt+WPXc10splcrxwwMunAtp7Vz/Ci/JLZNhVyG0ax32jgAAzTHb1rVOqVDlK96Ji+7Ze8BzbNjQ0MDn87Mz08tKS7p7+LZBNCKijJrK6nr5KAv7Nhof4MVhvgWdmlqb3Vu2sCDvk6Xzh/R2HdLb9ZMl86qrK6sqywPdzQsLco8d2h3obv7BuzOZNaurq376duVLA717ekmG93Nf/sHc8rJSIjq4d1ugu/nVsAuzXxkR7G0Vfvns5QunJr4UREQrP3470N18z85fWz12d11DPu6dAe0b5lvQeWXVVH557+bqbkFtMfiHC18vLMh9a8HyqqqK6MgILS0dqbTmzQXLNv20euVXGyytbIxNzImopqb6rdkTCvJy5sx/38LS+sjBP0NP//PRp98SUVpaEp/P/+3nb95450N5vczHt0dNTc3AIaOuRVz6/uc/icjO3rnVYzto6Xnrm7b6sACtCL0FnZdUUa+jLmiLkSsryu7ERb/6+jtjJ04noumvvUVEmprihvp6gUAwePhYgeDf1928YW1qcuLOv87aO3QhoksXT0msbHV09YkoLSVJQ6S55oetZub/nuCnpa1bVl7q4ubu5RPYFrGJqLJeFlWaP8TMto3GB3hx2E8InVcXbYOVrm1SADq6+uaWVkcP7jp94vDDyxPv3nbs4tZYWuVlpYcP7BgxehJTWswKLm4ezOO0lKR+A4c3lhbj3t04F1ePtsjMqJbXhxXntt34AC8OvQWdl0zRUNJmn+Kx8be/XNw8Pv3orTdmjiktKWIWJt6NbawlIoq8dkUmqxsyfBzzpVwuT01K6OrqwVRaSXGhW3fvh8fMykyvqix36ebZRpmJSKTO72ti1XbjA7w49BZ0XjKFYtW9G200uLWt/cYtBz78ZG1M9PUDu7cSUUlxYUF+rouLe+M62VlpRGQpsWG+vB19XSaXdXV1J6LUlHtE5ODY9eExE+/eJqKuLt3bKDMR6Qs0cN0xtHPoLei8tNUFBgKNtrizkey/S6BGjJnM4/FkcjkRpSQnEJGx2f8u7GV2GAqE/95iav+e34nIzMKSiNJTkojI3rHLw8OmJt8lImPTNrw0OKas6EZpftuND/DicF4GdGrfuPdqi5vJLpj/ssTK1ssn8EJoiLq6+qBho4lIW1uXiPbs+LWqokKNzx86YpyHVwAR7di6YfyUGceP7Lt0/iQRSWuqiSg1JUnfwNDQyOThYbV0dIlow/dfdHf3sbSy9fFr/TMho8vyh5jipAxo1zDfgk6tul5eLpe17pi1tVKJlW345dDvvl5eUVH606a9rm6eROTa3WvUuJfjY6PWfvlhUmI8EXl4+b37/qfnQ4/PmT7qTlz0og+/IKKkxDtElJaa1HiyRqOXxkz18PL75/Ce9es+Ly8rbt3YzKeZuOkaeeobt/rIAK0I99WFzm7erfNfd+vJdor2Ql+ggeuOoZ1Db0FndyY/k8fj+eibPGmFEf09amub2Jfo7ukTFxv9+HJ9fYPDJ6+3dswmhF0+++mHbzf5lJWNbXZmxuPL+w4YuvKrDU8acFvG3Zk2riYamq0aE6CVobcAqKahXvrkjzLJfZDV5D8TnhpPqWhiOZ/PN7NQxSl5UmlNaXFR08+p8aipbJpisYFh07sBrxbnptdUvuPYhheHAbQK9BYA3S4vzpZWBhp27o/wUCqNMNMCLsB5GQDkoWd0t7IkpqyQ7SCsyaipIBzVAo7AfAvgXxVyWVW9TIPf6S4OOZGXpi/QGG3hwHYQgGZBbwH8T2hBloFAw05Ll+0gqlMqr9NU40s0tdkOAtBc2E8I8D+DTK1P5WdU1svZDqIKMkXDrsxEB7EuSgu4BfMtgEfl10nrFQ3Z0ionbX22s7QVHtFHdyI+cPbuom3AdhaAlkFvATRBrlCsSYoy1dCcKHFSKpW8jnIpboVcFlacYyPW6WMs6SBvCTof/meffcZ2BoB2h8/j9TWWmGhommiILxU9OJGbJlMorMU6ubVVKdUVClLqqAtza6vvVZUTUTt/nF5TEVmSL1XUSzS1T+dnaKkLBpna4KYYwF04vgXwRLZiHXUeb6S53VhLRytNbQOhqKa+/k5FcV5dja5AWCirDSt60CqPz2bdX79ze+uO+e9jdWGFXMbn8ezFurrqwpm2rhMkTgI1/MMHDsN+QgD2JSUlrVy5cu/evWwHAeAA/NkFAABcgt4CAAAuQW8BsE9NTc3WFp/WCNAs6C0A9ikUioyMJj52BAAeh94CaBe0tXHTCoBmQW8BtAtVVVVsRwDgBvQWAPt4PJ6xcdMf5wgAj0BvAbBPqVQWFT3hk4sB4P9DbwGwj8fjOTjg468AmgW9BcA+pVKZmprKdgoAbkBvAQAAl6C3ANjH4/H09PTYTgHADegtAPYplcry8nK2UwBwA3oLgH08Hk9fv8N+tjJA60JvAbBPqVSWlZWxnQKAG9BbAADAJegtAPbxeDyJRMJ2CgBuQG8BsE+pVD548IDtFADcgN4CAAAuQW8BsI/H49nb27OdAoAb0FsA7FMqlWlpaWynAOAG9BYAAHAJeguAfbgfPEDzobcA2If7wQM0H3oLAAC4BL0FwD41NTVbW1u2UwBwA3oLgH0KhSIjI4PtFADcgN4CAAAuQW8BtAva2tpsRwDgBvQWQLtQVVXFdgQAbkBvAbCPx+NZW1uznQKAG9BbAOxTKpVZWVlspwDgBvQWAABwCXoLgH08Hs/IyIjtFADcgN4CYJ9SqSwuLmY7BQA3oLcA2If76gI0H3oLgH24ry5A86G3ANinpqaGzzsGaCb0FgD7FAoFPu8YoJnQWwDs4/F4ZmZmbKcA4AaeUqlkOwNAJzV16tTq6moiqq+vr6ioMDQ0JCKZTHb69Gm2owG0X5hvAbBm1KhR+fn5ubm5hYWFdXV1ubm5ubm5Ojo6bOcCaNfQWwCsmTRpko2NzcNLeDxe37592UsEwAHoLQDWCIXCsWPH8vn8xiU2NjYTJ05kNRRAe4feAmDT5MmTJRIJ85jH4/Xv39/CwoLtUADtGnoLgE1CoXDChAnMlMvGxmbSpElsJwJo79BbACybPHmypaUlM9nC2fAAz6TOdgCA51FQJ82oqZB3lKs4Ame+XHPhgsOIQREleWxnaR266gJHsa6muoDtINAB4fot4JjkqrIt6XcyairddY2KZbVsx4GmNZAyvaq8t7FkSRcftrNAR4PeAi7JrKlcfvfqq9ZddQUabGeBZ4suK0ytLv+2ezCPx2M7C3Qc6C3gjHJ53ayoc0vx9zunxJcXpUurvnTrwXYQ6DhwXgZwxo7MxFEWdmyngJbprmesVCpvlRWyHQQ6DvQWcEZMeZGhQMR2CmgxgZpaanUF2ymg40BvAXcoyVCIw1rcY6qhiTNooBXhPHjgjEKZVIGjsRwkVyjrefVsp4COA/MtAADgEvQWAABwCXoLAAC4BL0FAABcgt4CAAAuQW8BAACXoLcAAIBL0FsAAMAl6C0AAOAS9BYAAHAJegugY7p27tTmLz6qKi9jOwhAK0NvAbyQ/KyMhOhItlM04cCmdVdOHq2Xy59jW7lcdv38KVkdboYL7RF6C+D5XQsNeX/y0JuXQtkO0sqWvzp2w/KFclkd20EAmoDeAnh+0uoqtiO0CWl1NdsRAJ4In2MCHVlFWcmRrT9Hh52vKCk2NLfoPWLcS9PnZKcmfTJzvMTB2a6LS0zEZZlUuvTHra4+AeUlxfs3fX8r7FxtdY3EwfmlV+f2GDiMGWf/pu/DT/5dXlqkpavn2aPPtHeX6ugbhJ089vuaT4no9IGdpw/sNJVYf3/wLBHV19f/s/O3S8cPlRUVGJqY9x45btSMeerqz/i3lpWS9PvXK7LT7tfX11vZO42aMTdwwDAiSk+6+8nM8cOmzszNTEu+HSMUifz6Dpz61hKRWPyUrR5258bVr9+b5eLt/8kvfzJL4iLD1y6Y7eYXtGzDttBDe0/u21ZckG9katbnpQljZr6xeOLg0qJ8InpjSCARzf90ba/hY+5GXd/383fZaclibZ3u/kGvL/1cKNJsmx8awDNgvgUdVmVZ6Wdzppw9tFsmq7N3c6+pLI+NuNTYHw9Sk+Ouhfn2GeQR1MfF27+qvOzzeVMvHz8k1ta1d3N/kJq88ZOF54/tZ1auLi/T0Tfo4uFDCsWVkCO/fbWMiEwsJfau3YnI3Maux6Dh3sH9iUipVG5YvvDQlvV1tVLHbp411ZWHtqzfvOqjZ6YV6+jk52TZdnG1sndKv3dn4yeLUu/GNz57at+O/OzMwIHDNESi0EN7d69f05ytGG5+PUwl1om3buRnZTBLrp45QUS9h4+OvxGx/bvPy0uKvIL6iMTaxfk5ROQd3F+gISIiv76DewwabmIpqamqWLdkfmpCnKtPgKWtQ3riXZQWsAjzLeiwjm7bVPAgyz0weNGajUKRpqxWWl5S3Pismprasp93Wjk4M18e2fZLwYOsAeOmzFryGY/Hy0pJ+uS18Qc2/dD3pYl8Pn/Wh5/zeDwiqq2pWTJleEz4xZrqqq6efgPGTP49Id6zR59XFy1jxom6fC7qcqhtF7dPf92loSmuqa769PWJV88cH/nK63Zd3J6S1sjU4pcT4cyrnNy3ffdPa66fP+ng1p151sza9qvthzU0xRVlJQtG97sScuS1JSv5fP7Tt2LweLy+oyb+9esPF48fmvLmYrms7ualUKFI5Nd3yMV//iKigP7D5n2ymnl3RPTqomWR50+X1tXOXf6llo4eM+erk0pNLa2XrPutcTUAtqC3oMOKDjtPRBPmvsdMDoQiTRNLq8ZnJQ7OjaVFRNFXzjO/kfdu+IZZoqmlXVVeVpCdaWFrn5Zw59iOX9MT71SUlyoVDUqlsjgvR+zYpYkXvXKeiERi8aEtG5glGhqaRJR6N+7pvSWrlZ49uDvs9D9FOQ+UpCCiggdZjc/qGhhpaIqJSFff0NhSkpuRVlqYZ2wuefpWjfqMGHdoy/orIUcnzlsQe/VyTVVFz6GjNLW03AN78dXVw04dE4o0hr/8upnEuslsEjtHU0vrgpysbxfPHT3zja6efs349gO0FfQWdFilRYVEZPqE38UisdbjK0ec/ueR1YQijaTb0V+9PUOpVLoHBhuZWURfOV9WVFhXK21y2LLiAiK6F3PzXszNh5cLhKKnp/1p+YLYiMvGFhL/AUMrSotjwi/W1TY9rREINYioQV7f/K0MTEy9evaNvnL+9rUrV8+eIKJew8cQkZW909Lvt2z77vPQQ3vPHz0wfvY7Y2e92eQrfrxh29avP429eiX26hXfPoPe/uI7ocYz3hFAG0FvQYelpaNTXlxXVligq2/4zJXF2toVJXXf7A2xtHN45Km/Nv/YUF8/Y/HyIZNeJaK8rMyyokKlUtm4glKheGgcHSKatfTzgeOmND9q/oOs2IjLhibma3f/o6Epvhd7Myb84sMv8eJb9Rs9KfrK+dP7dybFResbm3T378ks7+YftHbPiSshR7Z/t+rgbz95BvW2d+n+3/v631AmllYfb/gj4daNzas+iroceu7wvuEvv9b8NwjQinBeBnRYrt4BzFEu5jokuVyWlvDoOQsPrezPHOWSy2VEVC+Xp9yNY56SVtcQkbGFFXOCePb9RCJSNNQTkaaWDhHlZqYRkUKhqK+vd/EKIKLT+3dUlJYwmyfFRj0zam1NFRHpGf27MzD59i0iamhQvPhWzNshIs+gPvrGpvE3ImS1tT2HjFJT+/fffl52Jp/P7zdqontATyLKz84kIk0tLSLKyUxrHCH/QRbzXRoyaToR5WalPfNNAbQRzLegwxo3++2YiIuRF04n3oo0s7LNz84QCEXrDp1teuXX346JuHT1zPG7UddMLa3zs9J5fP4Ph0KFGiIXL7+oy6FbVi938fRLTYyvKCslotyMtK6efg5u3dX4/LjI8I+mj5ZWVS7bsL33iDFnD+56kJ6yeOIgK3vnitKSgpysVdsP2Xft9pSoFjb2OgaGaYl3vnp7hrq6IP5GBBHlZ6Y/fcr1lK14PB6zIzQ24vKgCS8TEZ/PDxw47PT+nUTUe/gYZoS87MylU4Y5dvfS1Te4fe2KulDD0c2DiJw9fHIyUr9b/IaZtY21Y9c5H69a894sgUAosXdKjIkkIjefwBf74QA8P8y3oMOS2Dmu3LzXu1d/uUyefu+uSKwdPGwUM096nJWD84pfd3v17CuT1qYmxInE2sFDRzM7AAdPmj785dfU1NRir1226+K2+JtftHT17sVEEZGppfWcj1cZmVnkZqQqFUqBSENDU7x805/9x0wWijRTE+Jqa2t6DBqhpaP79KhCDdGitT87unncv3M7Pztz9kdf9Bw6qqa6Kjsl6bm36j18rKZYOyv1XuP63fyCiMjG2cXaqSuzpKFe3s0/KCPpbvyNCLsubh98t4k5dWXy/EVePfs2NMhzM1L1DA3rpFJXn8Dy0uJb4Re0dPVnLF7eY9CI5/qZALQC3jP3oQO0E6OuHv/AyVuDz2c7CCcplcp/dm458Ov30977cMTLs1T50tdL8nk8es/RU5UvCh0Y9hMCqEJS3K0jv2980rOvLfnsSeegt4rzx/b/9esPlWVlRmYW/UdPbLsXAlAB9BaAKlSUFMVdD3/Ss9LqyjZ99ZqqKoFA1H/s5Amz32XOJQHgLuwnBM7AfkKOwn5CaF04LwMAALgEvQUAAFyC3gIAAC5BbwEAAJegtwAAgEvQWwAAwCXoLQAA4BL0FgAAcAl6CwAAuAS9BQAAXIL7EwJnOGrpKQi3JeMePo+npy5kOwV0HJhvAWfwiHJrq9lOAS2WUVNpKdZiOwV0HOgt4Iw+RpY50iq2U0CLVTfI/fRM2U4BHQd6CzhjnMSxQFYbWZrPdhBogT1ZSRMsHfWEGmwHgY4Dn2MCHLPw9hWJSGwoFEk0tYl4bMeBptU0yHOk1ddK8960cw82tmA7DnQo6C3gnpN5GVdL8+RKRVp1OdtZnpOiQVFbWyvWEj9phYryCl09XdWGak2mGmI7se4kiaO1Jj6mEloZeguABceOHYuNjf3000+bfHbPnj0bN26cMmXKggULVB4NoL3D8S0AFty9e9fNze1Jz0ZERMhkstOnT4eEhKg2FwAHoLcAWJCQkODq6trkU+Xl5bm5uURUUFDwxx9/JCcnqzwdQLuG3gJggaamZrdu3Zp8Kj4+vqysjHmcnp7+pH2JAJ0WegtA1e7du1dZWfmkZ2/cuNHYW0SUkpKyYsUKVUUD4AD0FoCqpaWl9ejR40nPxsTE8Hj/O79foVBERET88ccfqkoH0N6htwBULTY21tzc/EnPlpSUND5WKpVCoVAgELz++uuqSgfQ3uG+ugCqVl1d/aSDW0RUVlZmamoaEhISHh5uZGTk4uKi2nQA7R3mWwCqdv78eQcHhyc9e/nyZeb097y8vMOHD6s2GgAHYL4FoFI5OTne3t6amprPXLNXr14NDQ0qCQXAJZhvAajU/fv31dWb9feimZnZ5MmT2z4RAMegtwBUKjU19Sk7CR+xY8eOh8+JBwD0FoCqlZaWPulOGY+LjY2NjY1t40QAHIPeAlCpmJiYp5wE/4jp06cbGhq2cSIAjsF5GQAqlZGRYWtr28yVfXx82jgOAPdgvgWgOuXl5VpaWjo6zf1Iquzs7F27drVxKACOQW8BqM6DBw9atN9PIBDs2bOnLRMBcA96C0B1cnNzHR0dm7++mZkZ7gcP8Aj0FoDq5OXl6erqtmiTp9yBF6BzQm8BqE5hYaGJiUmLNlm9enV6enqbJQLgHvQWgOrU1dU1/yR4Rm5ubk5OTpslAuAe9BaA6mRnZ2tpabVok3feecfJyanNEgFwD67fAlCdioqKlh7f6tq1a5vFAeAkzLcAVMfQ0LClvXX+/Pnw8PA2SwTAPegtANVJTk4WCAQt2uTevXsJCQltlgiAe7CfEEB15HJ5S3urR48e9fX1bZYIgHvQWwCq4+Li0tLe8vb2brM4AJyE/YQAqnP37t2WfoRxTExMTExMmyUC4B70FoDqqKmpKRSKFm1y5coV9BbAw7CfEEB1nJ2dW3qwyt3dvfn3jwfoDNBbAKqTm5tbW1vbok369evXZnEAOAn7CQFUx9jYuKW9FRkZmZWV1WaJALgHvQWgOmpqahUVFS3aZOfOndnZ2W2WCIB70FsAqqOvr19WVtaiTfz9/e3s7NosEQD34PgWgOrY2dnV1dW1aJOZM2e2WRwATsJ8C0B1+Hx+S3f67dq1q6WXfAF0bOgtANWxtLRs0fqVlZVbt27l8/ltlgiAe9BbAKqjp6fXopvkyuXy2bNnt2UiAO5BbwGojoWFhUgkav76hoaGr776alsmAuAe9BaA6lhYWERERDR//dTU1OvXr7dlIgDuQW8BqI6mpqavr29JSUkz1z916lR8fHwbhwLgGJ5SqWQ7A0An0rt3by0tLYVCUVVVZWRk9M8//zxl5fDwcIlEguu3AB6G67cAVMHX11epVPJ4PB6PJ5VKmYUDBgx4+lbBwcEqSQfAJdhPCKAK48ePV1dX5/F4jUt0dXX79u379K1OnDjR0vsZAnR46C0AVVi2bJmNjU3jl0ql0tDQ0MfH5ymbSKXSr7/+ukXnHwJ0BugtAFXg8XgffPCBoaFh45JevXo9fROZTLZ69eq2jwbAMegtABXp0aPH4MGDmZtf6OnpPbO39PT0+vTpo6p0AJyB3gJQnSVLltjb2yuVSgMDAz8/v6evHBERcf78eVVFA+AMnE8IHFAsq63vKBdszP9o6cqVK/0GDcivkz59zeNhl93c3J65GmcolUZCkboa/laGF4Xrt6Bd25Qad74w21qsnVNbzXYWVZPLZOrq6ryO8oteSPxCmdRZW3+ixLGPsYTtOMBh6C1op2QKxbxb54ONLOzEOjrqQrbjQOsoltWeK8jqaWQxUeLEdhbgKvQWtFOzokKHm9raaOmwHQRa3+GclAADM1QXPJ8OsgsCOpjDOSkeesYorY5qvKXjtZK8EhkuqYbngd6C9ii2vEhHXcB2CmhDdYqGlOpytlMAJ6G3oD1SKJVmGmK2U0AbshXr5NV2lFMlQbXQW9Ae5dRWKwhHXjuymoaGWkU92ymAk9BbAADAJegtAADgEvQWAABwCXoLAAC4BL0FAABcgt4CAAAuQW8BAACXoLcAAIBL0FsAAMAl6C0AAOAS9BZ0HEm3o49u2ySX1bEdhH0KhSL+RsTx3b8TkbS68lpoyLVzp1plZGl15fHdvx/Y9H2rjAbwHNBb0HF8u/iNg7/9VC/n2F3vSgrzfvr4vXmDA+YP6xF54XSrjCmtrlzz3uun9+0kopiIyxtXLI67HtYqI+c/yNq38duEmJutMhrAc1BnOwBAZ/fjh++mJsQ5uLoLRSJHN3e24wC0d+gtADblZaanJsQ5u3uv/G0v21kAuAG9BR3Nvl++vXkpVF4nc+ru8erC5Ra29kR0ct/23T+tGTPzjUnzFxGRtLp67iBfXUOjX06EE9HcQf5dvXzF2joxEZfV+WpO7t7+/QZf+PtgZnKitp7+sCkzhk2ZyQy+f9P34Sf/Li8t0tLV8+zRZ9q7S3X0DYjohw/fTo6PeWn6nHOH95YVF1raOUx984Nu/kFPj/r3jt8O/Po9ESXH3Zoe5DJryWcDx08lomvnTv2zY3NOeopIW9s7uP/Ut97XNTBkNkm7d+fApu+TbkfzeGpdPLwnzV9k37Ub81RNVcX+Td9HXjhTW1Nj69T1kddKvh39/uShJXm5pta2w6fM6Dd6ErP8WujJQ1vXF+bmCNQFTu6eU9/+wNbZlXmqoqzkyNafo8POV5QUG5pb9B4x7qXpcx4es05a8/kb0zKTEyfMfW/c62+1xk8P4NlwfAs6mvNH9huZWqgLBbevhX2zeK6srlkfBh8TfvHOzasB/Yfw1YXRV85vXvWxtLoqoP/QqvLSXT9+HR12gVmturxMR9+gi4cPKRRXQo789tWyxhEqSor3bfzWrqubR2Dv9MS7373/RkFO1tNf1MzaxqmbJxHpGBh69exrbG5JRKf279j4ycKczDQHN3dNTa3Lxw+tevMVaXU1ESXHx3zxxiuDjSf/AAAgAElEQVRx18Mt7RzNre1uXwtbNf+VjOQEIpLLZV+/9/q5w/vkdXU2jl1yMtIeea2cjFSRSGxqbfsgNXnr1yuObv+VWV4vlzXU13dx99IxMIi7Hr524RxZrZSIKstKP5sz5eyh3TJZnb2be01leWzEJXX1//eX7pbVyzOTE3sMGo7SAlXCfAs6mmU/73T19q+tqfn09Yk5GakJ0Tc8g3o3Z8MVv+4xt7K5fyf2szlTdPUNVm7eKxKLHVzdt3/3+a2wCz69+hPRrA8/5/F4RFRbU7NkyvCY8Is11VViLW1mhFlLP+s/ZjIR7Vm/NmTvtojTx8fOevMprxg4YJiOnsHqd2Y6urp/sG4zEZUXF+3/eZ1IrLXqj4MWtvZKpXLT50sjTv9z8Z+/hk99bfs3n8vrat/+Yl3Q4JFEdP7o/j/Wrjy8deOitT9fPHYwLSHeyrHLsg3bdQ0Mi/NzFowd8PBr9Rg88p0v1hFRXGT42gWzj/3xy4Axk3UNDIOHje41fAyzzg8fvhN1OfRudKRXz75Ht20qeJDlHhi8aM1GoUhTVistLyl+eMC/d/x2LfSkvWu3ectXt/ynBPD80FvQ0dh1cSMikVjsEdQ7JyO14MEzJj2NmOmOsZklEYm0tEViMRFZ2toTUWlRAbNOWsKdYzt+TU+8U1FeqlQ0KJXK4rwcsWMX5lkTSyvmgb1rdyJq/ks3ir0eJpfL9E1MLxw7wCyRVlcRUcrduKK8BxnJCXx19bSE+LSEeCKSyWqJKOXubSKKuXqRiEZMe53ZoygSaz0ysoZIk3ngHhDcxdM3KTYq6XaUX9/BpUX5f+/4LS4yvKQgn8cjImKmidFh54lowtz3hCJNIhKKNBvfHRHlpKfcj7tFRC+/vVT438gAqoHegg5LXSAkovp62QuNwvwuVyqZ68O+enuGUql0Dww2MrOIvnK+rKiwrlb6+EYCIfPS8pa+WnlRIREV5mSH7N328HKhhqisuIiIGurrH31KKCKisqIiIjKTWD02ZBP0DIyIqKa6urqyfOXrU0qL8h1c3bv5BKYkxGck3a2rkRJRaVEhEZlKrJscobqinHlwdPsmN9/Alr5NgBeB3oJOQU1NjYgUSuWLDHL+6L6G+voZi5cPmfQqEeVlZZYVFSpfbMxHiLV1iKjHoBHvrHr0wt4H6SlEpG9ssvGfK49vqG9klEFUWljYnFcpys8hIkMT0xsXz5YW5fv1HbxwzQYiOrptU0bSXeYdaenolBfXlRUW6OobPj6CulBj0dqNW75afvfmtYjT//QcOup53zFAi+G8DOgUdA2MiCg98Q7z5dXQ488xiLS6hoiMLayYMxKz7ycSkaKhNS9zdvHxJ6KoK+dT7sYxS9Lu3amT1hCRhY29npFxWVHhmYO7mafKS4rzMtOZx8xJgCf2bK0qLyOiutpHz0ZhBiGiqMvn0hLixdq6zt29amuqicj0vx2AyXHRRKRQNBCRq3cA02TM/Ufkchmzc5Jh79LNs0fvae8uJaLdG9ZKqytb8ZsA8HSYb0Gn0NXLV12oERcZ/uHLIxvnLi3l4uUXdTl0y+rlLp5+qYnxFWWlRJSbkdbV06+1ckrsHHsPH3vl5NHP506xcXatr5fnpN1/+d2lw6e+pqamNuXNxb99uWznulVn/vpTU0s7Jz2lu3/PRWt/JqJhL7929tDetIQ7C8cPsLC1z8969NDatdCQnIzUulppflYGEU15630NTXFXD18iOnNwV/6DzJKCvLTEO0SUm5lKRONmvx0TcTHywunEW5FmVrb52RkCoWjdobMPj9lzyEsX//7rbtT1v379acb7n7TWNwHg6TDfgk7B0MT8nS/WWdo65Odk8wWCGYuXP8cggydNH/7ya2pqarHXLtt1cVv8zS9aunr3YqJaN+qc5V9Nmr/QxNIq835icW6Oi0+ArZML81SfkePfW/2TvWv34tycrJRkcys7j8B/T5XU1Tdc9vN2N78eDQ2KkoJ8n979Hh5TQ1NzxMuzKktLS/Jybbu4vbPq+4HjpjDnj8xd/pWRmcXtq1eIx1vywxZLW4fUhHi5XCaxc1y5ea93r/5ymTz93l2RWDt42KjHJ5cz3l/BV1cPPbwn7d6d1v0+ADwJr3X3zgO0itnR50Zb2JtpiNkOAm3lTEGWq47BJIkT20GAe7CfEKANJcXdOvL7xic9+9qSz8yecMIeADwJegugDVWUFMVdD3/SszidAeA5oLcA2pBf38G7riaynQKgQ8F5GQAAwCXoLQAA4BL0FgAAcAl6CwAAuAS9BQAAXILeAgAALkFvAQAAl6C3AACAS3DdMXCerEaafeOWQIxP3WWZiZ2NyMSI7RTQ8aG3gPN4DQ0GWtqOLl3ZDtKpCdTVG/hqRWzHgM4AvQWcJ9QS2/l7N7Ado5PjEU+uVBAp2A4CHR96CzhPqaZWrkRtAXQWOC8DAAC4BL0FAABcgt4CAAAuQW8BAACXoLcAAIBL0FsAAMAl6C0AAOAS9BYAAHAJegsAALgEvQUAAFyC3gIAAC5BbwEAAJegtwAAgEvQW9BJnT24Z+5Avzs3rj5zTaVSqZJErfnSm1d9PKuPh0LR3E8VKSsq/H7pW3MG+qxb8ubzvSKAyqC3oJNSkqK+oV6hfNpv9vysjB8+fDv00F4V5vrXyb3bV8ya8NybZ6clmdvaq6k19x/4L58vTYyJeuW9j/qPmfzcLwqgGugt6KSGTJy+7WKMe0DwU9a5cTk06vI5Bzd3FeYiZpp1aOsGXUMjHo/3HJsrFIoHaSkSO4dmrv8gPeXuzauDJ07rP2ayT6/+zUz4HMEAWgUP//9BOzQ7+txoC3szDXEbjX/4942Ht24kot9Cb5YXFa5Z8HpXL7+C7MzMlHuGpubzlq/u4uFzbMfmv379gVnf0MR8/d8XlUrl2YO7zh3eV/AgS0tPf+ikV0fNmFteXPT2S71su7jV18tz01OW/7LT3MrukSXRYRdP7Nr6w+FzJhaSvOzMDyYNGT/nnbGz3lrx2gSxjo5CoUhLjDcys5g4b0HggGENDQ3zBvvVSaXMS8/5eFW/0ZMakzc0NMhqax9+L+pCgUAgfHhJflbG+5OHGplbSquq1IWCXsPGTH5zsbq6OhHdvh52bNumtHt3+HyBb5/+ry/9POLsia2rP3nkbZ4+sDP08N7i3BwjC8uJc9/rMWgEEf2+9tMLRw/49hmUEB1p59Jt2YZtuRlpf23+Mf7mNbmsztHN/d1VP+gZGTfzR3CmIMtVx2CSxOnFfpLQGeHzjqEz8u87+NI/h9QFArGWtry2tjg/Nybi8qjpcwIHDt+9fs3Rbb8s/WGrT6/+J3b9bmZtPXHuQm09fSLasW5V6KE9vn0GjZ75xpWQo/s3rQsaMuJBeioRVZWVvrp4WUVZiVN37/gbEY8sOfrHJrG2romFhIiyU+4RkY1TVzU1tfwHmQ0NDUMmTffs0ef4rq2/frbU1TtAJBb3GTH+7KHd0xd+bGHj4NjN4+HkNy+d3bB84cNLxsx8Y9L8RQ8vyUq9R0TWjl38+gy8fv5UyJ4/jC0sh0ycfi305MYVi6wcu8xasjIt4c6Zg7vc/Hq6eAd49uwTG3H5rc+/Mza3JKI9G9ae3Lt94PipLl7+//y5ZdPnSx27eZpYSLKS7xGRgYnpe6t/JKK8zPSVc6YQjzdx7jvE4+368evIC2cGT5ymqp8hdF7oLeiMLOwcK0qKvXv1J6KC3GwimvLm4oHjphDRgc0/1tc3EJFYR6emqsIzsLdnUG9mZ1rooT3mNnYzP1hRXlRYVysVaIg0tbQzkxKJaM7yLxt3OT6+JCPpro1zV+ZxVkoyEdk4u9TW1NTWVI+Y9vrLb39ARA318kNbNzxIT3H19pfJ6vjq6gPGTBaKNB9J7uYTuOLXXQ8vMTKzeGSdrPvJRDRryadGZpaBA4fNHeQfdz180Phpu35aLRAI5y77SqylnXz7FhHpGxmbW9nI6+pEYnHQ4JE8Hu9BesrJvdt7jxg3a8lnRCStqfpjzcrM5EQjM4us1CRnd+/XPviUeZWNKxbVVFXMWvKZV6++MWGXFA0NWjo6bfPjAvh/0FvQGeVmpMrlMpsurkSUcS+BiGy7uBBRcX6OvK7Wys6RiJjf7M7uPswmibduMpOM90b3JSITS6t3V32vpaOXcT9RoCFy8+3ROPgjS0oK8yrKSns4uTBfZt2/p6mlY2ppnRwfQ0R2Xd2Y5TJZHRFpamkRUdLtKFtn18dLi4jEOro2Tq4PL1EXCh5ZJys1WUdf38jMkoj4fHUej6dUKPIy08uKColo5exJRKSppTPxjYUegb2IKDslycLWgTmWdvvqFSIKHjaKGaqyrIx50fysjDqp1Cu4X+OrJNy6QUTbvv2MviU1Pj942Ci/foNb44cD8AzoLeiMMpMTicjW2YWIMpITeDyetYMzEWUkJRKRbVdXIkq6fYuI7N26M5uoC9SJ6N2vfjQ2l4i1tM2sbZmz9TKTEqwcnPh8/v8G//9LmAmWlaMTEcnlsnu3o2ycuhBRZvI9IrLv2o1ZHnn+lL6xibVj16rystyMtEETmt7h1pz9hOmJd2yd/63Dq6EhSqXSPTCYqbcRL8/qNXwMEZlb2zK9WFFaUlFW6tnz30KqrakmIi0dXeZY2rXQEG09fUc39+grFxq/Y/9+Q9QFTt0853+6tqaqylRixexKBVAB9BZ0Rpn37zXOsdKT7pjb2Gloiokog+mzLq5EJK2uIqLQQ3uUSuWEOe+6+QSqCzWObts0bMqM8uJiNTXeS6/OldXV5mWl9x01sXHkx5cw85j4yKtmljZnDu4uLy4K7D+MiLJSEokoKuw8L5wXfurv/OzMd778kc/nS2uqiSg5Lub80f2GpuZePfs+nPyZ+wmL83MKcrL4AvXLJw5npySf/utPRzePAWOnqAuElrYOl04cNjA1Ewg0zhzcPefjVcz8j4is7B2ZzR3cPIjo4Jb1PQaOvHbuRNb9e/NWrBFqiDKTE5jdm40v5BHU+8LRAxePH7KwsQvZ+8fM91fo6Bu0wc8K4FHoLeiMMpMTdfT1DU3M6+vrs1Lu+/UZ2Licr64usXcioiETX0m4FXls+6+GpuYT5rxrYmn13lc/Hvhl3bZvPtPWM5j27lIiyk65r1AorJ26No78+BI33x6+fQZFh11Iuh1tZmVDRMyxrozke9p6+iG7ttbU1Nh3cV2y7jfPnn2IyMRCMmDM5CshR/du+HbKW+8/klxH36Crvt9T3lrk+TOGJua2zq7bv/tCS1tvyKTpE+a8JxBqENGCNRt2fPfFX5t/VBdo9Bk59t/AaclEZGn/73l9nkG9J81fGHpw790b1yQOzgvXrPfrO4Rpeh19fUMTs8YXmvbOkvo62YVjB+SyOlsnF5QWqAzOg4f2qEXnwSsUiqqKsiaWNyjU+E1coagp1mJ+j7NIqVTOG+QfOGg4M+lh0W9fLrt84vCPR84Zm0tU+bo4Dx6eG+ZbwHnlxUXvju7z+HIrB+fs1OTHl8//dC1zjIdFBTnZ0poqC1t7dmPcvHQm4uwJp26eKi4tgBeB3gLO09bT+2j9H48vb6iv56s38X+4lb2zSnI9Tdb9BCKytG3uLS3ayJWQv7t6+M5ZxvKcD6BFsJ8Q2qO2vl8GsA77CeG54f6EAADAJegtAADgEvQWAABwCXoLAAC4BL0FAABcgt4CAAAuQW8BAACXoLcAAIBL0FsAAMAl6C0AAOAS9BYAAHAJegsAALgEvQUAAFyC3oL2SKKppcbjsZ0C2pCYry7i43OU4Hmgt6A9EvD4udJqtlNAG0qrLpeItNhOAZyE3oL2yEffpKpeznYKaEMaavyu2npspwBOQm9BezTS3C61puJORTHbQaBN7MtKGmJmo6UuZDsIcBI+7xjaKYVSufD2FRcdfStNbVN88HGHIFM0FNRKLxRlT5A4DTCxYjsOcBV6C9q13Vn3zhVmi/nqWTVVbGdpQ0pSNjQ0qHfo8xQ0+GrShgZPPeOJEkcvPRO24wCHobeAA+oUDXKlgu0Ubej+/ftr1qzZunUr20HakpK01QVsh4COoCP/fQcdhoYaX4P4bKdoQxJD4xH9B2rz8Wsd4Nkw3wIAAC7B+YQA7CstLT137hzbKQC4Ab0FwL7CwsIOfnALoPWgtwDYZ2ZmNm/ePLZTAHADjm8BAACXYL4FwL6ysrKTJ0+ynQKAG9BbAOwrKCjYuXMn2ykAuAG9BcA+ExOTV155he0UANyA41sAAMAlmG8BsK+oqOjAgQNspwDgBvQWAPtKSkqOHDnCdgoAbkBvAbDPyMhowoQJbKcA4AYc3wIAAC7BfAuAfUVFRbt372Y7BQA3oLcA2FdSUnL8+HG2UwBwA3oLgH24fgug+XB8CwAAuATzLQD2FRUV7du3j+0UANyA3gJgX0lJybFjx9hOAcAN6C0A9hkaGo4bN47tFADcgONbAADAJZhvAbCvtLT09OnTbKcA4Ab0FgD7CgsLt2/fznYKAG5AbwGwT0dHJzAwkO0UANyA41sAAMAlmG8BsK+2tvb+/ftspwDgBvQWAPsyMzNXrFjBdgoAbkBvAbBPQ0PD1taW7RQA3IDjWwAAwCWYbwGwD8e3AJoPvQXAPhzfAmg+9BYA+3R1dXv27Ml2CgBuwPEtAADgEsy3ANhXWVkZFhbGdgoAbkBvAbAvNzf3559/ZjsFADegtwDYh+NbAM2H41sAAMAlmG8BsK+ysvLq1atspwDgBvQWAPtyc3PXr1/PdgoAbkBvAbBPV1e3V69ebKcA4AYc3wJgzapVq44dO0ZEzD9DHo/HPI6KimI7GkD7hfkWAGumTZtmZWXFNBZTWkSEDz4GeDr0FgBrHB0d/f39H97noaur+9prr7EaCqC9Q28BsGnq1Kk2NjaNX7q5uQUEBLCaCKC9Q28BsMnR0dHPz495bGRkNGvWLLYTAbR36C0Alk2bNs3a2pqIXFxcfH192Y4D0N6htwBYZm9v7+fnp6OjM2PGDLazAHAAzoOH9uXwg/thJXlEypSqCrazqI5CqZDL5BoaGmwHUSl9odBZS/8V6672WrpsZwEuQW9BO7I0PtxEqGkh0rIQafHVeGzHgbZVKZcV1EkvFT14x8EjwNCM7TjAGegtaC/ejwuz09Txx++vzufPzMTxlo4DTa3ZDgLcgONb0C6cyEs31xCjtDqnV21cjuamShvkbAcBbkBvQbtwrSTPSChiOwWwhsfjxZWXsJ0CuAG9Be1CAyktNLXYTgGscRDrPqitZjsFcAN6C9qFjOpKtiMAm6SKhsp6GdspgBvQWwAAwCXoLQAA4BL0FgAAcAl6CwAAuAS9BQAAXILeAgAALkFvAQAAl6C3AACAS9BbAADAJegtAADgEvQWAABwiTrbAQDao5rqqtvXwhQN9T2HvMR2FgD4fzDfAmjC3ZtXN36yMCbi0osMkp+VkRAd2cyVb18PmzvI/8zBXa0+ckt9/e7rH0wZJsWdjqG9Qm8BtIlroSHvTx5681JoM9fPSk6UVlem3o1r9ZFbpKGhIeVubF5mekVZWVuMD/DisJ8QoE1Iq6tatP6Qya8aWVh19+vR6iO3CJ/P//TX3VUV5WYS67Z7FYAXgd4CrmpoaDi5b3vYiSN5D7J0dPU9gnpPeXOxroFhTVXF/k3f37h4VlpZaWZlM+zl1/qNmkhE6Ul3P5k5ftjUmbmZacm3Y4QikV/fgVPfWiISi5kBH6Sn7N34bUL0dT5fYGJp1fhCJ/dt3/3TmjEz35g0fxERSaur5w7y1TU0+uVEOLPCzUuhJ/duy0hOUOMLnLq5T35zcXZK8u9rPiWi0wd2nj6w01Ri/f3Bs095L0e3bTr4209ENHTyjFcXLXtK1LCTx5ocubykeP+m72+FnautrpE4OL/06tweA4c1vmuJg7NdF5eYiMsyqXTKW+/v+ulrU0vrdQfP8Hg8IroScmTzqo99+wxctPbnGcFuCoWCiDafua6lo0dEaffuHNj0fdLtaB5PrYuH96T5i+y7dkuOj/l87lQbZ5fVO48yb2H5zPGT3ljg1bMv85388OWRk+YvGjPzjbb8XwA6KewnBE5SKpXrly3Yt/Hbgtxs+65uAqEw8twp4lG9XL7mvdnnDu8TCITOnr75OdlbV39yav+Oxg1P7duRn50ZOHCYhkgUemjv7vVrmOV52Zmfz5sWE35RJNaysLHLTk1qZpJT+3f8+NE7Sbejza3tTcwtb18LqywrNbGU2Lt2JyJzG7seg4Z7B/d/+iDmNvbWTl0fHbmpqE2OXFVe9vm8qZePHxJr69q7uT9ITd74ycLzx/Y3DvUgNTnuWphvn0EeQX2GTplh5dilICfrXuxN5tnzRw8Q0ZCJ04nIp/dAdYGgccPk+Jgv3ngl7nq4pZ2jubXd7Wthq+a/kpGc4Nzdy9TSOjM5MS87k+m2jKS7F44dYLa6FhpCRM7dPJv5PQRoEcy3gJOiLodGXQ41NDH/9LfdxuYS5m98XX3DKyFHUhPibLu4rdy8WyjSTLod/cUb0w5v/XnguKnMhmbWtl9tP6yhKa4oK1kwut+VkCOvLVnJ5/MPbPq+prI8eNiouctWqwsEV0KObV714TNjlBUV7v95HY/H+/Cn37v792RiSOwciWjAmMm/J8R79ujz6qJlzxynx8BhFaVFO9d9+fDCJqN29fR7fOQj234peJA1YNyUWUs+4/F4WSlJn7w2/sCmH/q+NJFZQU1NbdnPO60cnJkvB094Zds3K6+EHHPx8s9Ou58cd0vi4NzNP4iIFq7ZMH9Yj6ryfw9ubf/mc3ld7dtfrAsaPJKIzh/d/8falYe3bly09uegwSOO7dgcdfHMyOlzLv1zkIhuhV8sKcw3NDG7Fhqixufbu7q38KcK0CyYbwEnRV+5QESDJ77ClBYRMW0RFxlBRH1HTRCKNImoi4ePha19TVVF5v1/50+6BkYammIi0tU3NLaU1MvlpYV5SqUy9uolIpr0xkJmtqGpJW5OjNuR4XK5zD0wmCmtxhitosmoTa4ZfeU8EdXW1Ozd8M2e9WuvnDiiqaVdVV5WkJ35byoH58bSIqLgoaM0tXQiz52S1UovHD1AREMnTX982KK8BxnJCXx19bSE+D3r1+5ZvzbzfiIRpdy9TURBQ0cR0Y2LZ2W10ojTJ7T19BUNDZePH85ITsjNSHMPDNbU0mqtbwXAwzDfAk4qKy4gIlOrR88dqCwrISIDY5PGJTr6hrkZaVUVZXqGRo+sLBBqEFGDvL62pqpOKlXj8xtbsJnKiwqJyFRi8wJvpVkaozb5bGlRIRFFnP7nkeVCkUZdnZSIROL/VyEisbjPyHGnD+yMOHsi7NQxLV294KGjHh+2rLiIiBrq60P2bvt/wwpFRGRl72Tj7HL/TuzpA7tqqirmfbL6nz+3XPj7r1ppDRH1HIzr3qCtoLeAk8TaukRUVlTwyHIdfUMiqigpaVxSVlhARLp6Bk8ZTVNLRygSyWpry0uKH683NTU1IlIolU3E0NElotLCR2M0UioUzX5PLfPwyGJt7YqSum/2hljaOTyyWmV506ezDxr/8ukDO3f/tFZaXTly+hxmYvcITS1tItI3Ntn4z5UmBwkaNDIzOfHQ1vXaevo9Bo2oldbsXPflqf07hSKRT++BL/b+AJ4I+wmBk1x9AonozMHdjZ2RFHeLiNx8ApgT5OSyOuaIS0FOlo6+/uNnPTzC1tmViA7+9lN9fT0RyWprG5/SNTAiovTEO8yXV0OPNz7l4u1HRDERF5lXZ85QkNXVMl1IRLmZaUSkUCiYYVvF4yO7evszR7nkchkR1cvlKc+6DszC1t49IFhaXammpjZ4wstNr2Njr2dkXFZUeObgbmZJeUlxXmZ64wpBQ0YwL9d31EShhqjX8LEisVa9rM6nV3/sJIS2g/kWcFLv4aPPHNz1IDX5gylDJXZOVeVlBTlZ3+wN6Tl01Mn9O+/fiV0ydYSxueX9+Bgimjhv0cPnyDVp/Jx31i6YfeHYgagr54zMzLPu/+98wq5evupCjbjI8A9fHsmcedH4lMTOsc9LEy4fP/Tl/FckDs48Hi87Jem1JSsHjJ3i4NZdjc+Piwz/aPpoaVXlsg3bzaxtW+W9Pz7yuNffjom4dPXM8btR10wtrfOz0nl8/g+HQoUaoqeMM2jCtLjIcJ/eA5+0d1RNTW3Km4t/+3LZznWrzvz1p6aWdk56Snf/novW/sysYGwu6eLpm3w7etC4qUQk1tLuPXzs2UO7g7CTENoS5lvASUKR5ic/7+w/drJIrJWRnCCT1QYPG6Uh1hRqiJZt2N57xLjamur78TFm1nbzVqwZOG7KMwd0Dwh+Z9X3EgfnmsqKmspKz6A+jU8Zmpi/88U6S1uH/JxsvkAwY/Hyhzec/dEXU95830RinZOeUpyf6+ITyJwBYWppPefjVUZmFrkZqUqFUiDSaK33/vjIVg7OK37d7dWzr0xam5oQJxJrBw8d/cxdlN69+htbSIZOfvUp6/QZOf691T/Zu3Yvzs3JSkk2t7LzCOz98Ao9B7/k3at/4+Vugye+oqWr5xHU+wnjAbQCnrKpvfYAKvbKjTPTbboaCFrtlztwy4WiBxKR1gwbF7aDAAdgPyGAKpw7sv/mpTNNPiXS1Frw9XqVJwLgKvQWgCrkpKfEXQ9v8inmPAsAaCb0FoAqvLpoWXNunAEAz4TzMgAAgEvQWwAAwCXoLQAA4BL0FgAAcAl6CwAAuAS9BQAAXILeAgAALkFvAQAAl6C3AACAS3C/DGBZXl5ebGysQFTHIx7bWYA1Gmp8kRqf7RTADegtYEFiYmJsbGxMTExsbCyPx/P09BS+1LtYJtUXCNmOBuzIq6320jNmOwVwAz7HBFShtra2sahiY2Pt7Ow8PT29vLw8PT3NzMyIaGdmYqVc5mtgynZSYMfhnJSY1et9bR18fX19fX3t7e3ZTgTtF3oL2mzBtjQAACAASURBVAqzA5DpqoyMjMai8vT0FIma+Bze0VePv+/krcHHzqJO52Jhto5AOFVPEvWf8vJyHx8fPz8/Hx8fR0dHtgNC+4Legtb0+A5Apqu6du36zG0r5LLZ0ecmSZxsxPhcj85CrlCcL8zWURcsdvZ+eHlJSUl0dPTNmzejo6OLi4t9/+Pk5MReWGgv0FvwQhp3AMbFxd26devxHYAtUl0v/yklNqw4x0fftFhW2zaR2yOFUiGrkzU5De3AKutlCqVytIX9VKsuT1mtrKyscR5WWFjY2GHOzs4qDAvtCHoLWqzJHYBeXl4eHh6t8ptXpmhIqa6QKRpaIyw3ZGdnb9u2bcWKFWwHUSkjochCpMXnteA80vLy8sYOy8vL8/X1DQgI8Pb27tLlac0HHQx6C5rl3r17TFHFxMS0dAcgPFNSUtLKlSv37t3LdhAuqaysjIqKiomJuX79em5urq+vr5+fn6+vLzqsw8N58NA0hUIRExNz69Yt5r82NjZeXl79+/dfsGDBc+wABGh1Ojo6/fr169evX2OH3bx58++//2Y6LCgoyNPTE/sSOyTMt+B/KisrG4sqPj7ey8vL29ub+a+mpibb6TqypKSkb775ZuvWrWwH6QiYDouPjw8LCysoKPD7j4ODA9vRoHWgtzq7wsLCW7duRUdH37p1y9TUVF1dnSkqDw8PtqN1IthP2EbKy8tv/qe0tNTPz8/f39/X19fOzo7taPD8sJ+wM8rOzmaKKjo6uq6uztvb28fHZ+LEiTjJmC1qamq2trZsp+iA9PT0Bg4cOHDgQCIqLS29efPmjRs39uzZIxQKnZ2dAwIC/P39sd+bczDf6ixSU1OjoqKio6Ojo6NFIpGPjw9TV1ZWVmxHA8y3VK2wsDAyMjIyMvLGjRsikcjf3z8gICAgIEBHB9cOcgB6qyNjuurGjRtRUVESicTNzc3Hx8fHx8fYGDeCa1+Sk5M3b9783XffsR2kM8rIyLhx4wZTYxKJJCAgoGfPnv7+/mzngidCb3U0aWlpzIlVN2/eNDAw8PX1ZXbo6+vrsx0NngjzrXYiMTExMjIyNTX1+PHjAQEBQUFBPXr0wEmJ7Q16qyPIzMy8cePGzZs3o6KidHV1mQtZ/Pz8DAwM2I4GzYLeaoeuX79+9erVa9eulZSU9PiPoaEh27kAvcVZJSUlkZGR169fj4yMNDMzc3JyYi66NDIyYjsatFhSUtK33367ZcsWtoNAE4qLi6/9x9jYuH///t7e3tiRyCL0FpfI5XKmq65fv15SUhIQEBAYGBgQEGBubs52NHghmG9xRVJS0q1bty5cuBAbGxv8H1NTfP6OSqG3OCA2Npapq/j4eKarAgMDcc56R5KcnLxly5ZvvvmG7SDQXDKZLPw/enp6PXv27NWrl4+PD9u5OgX0VjtVUFBw9erV8PDwzMxMsVjM1JW3t3czNgXuwXyL05KTkyMiIsLCwhISEoKDgwcMGBAUFKSrq8t2rg4L1x23L1FRUcxfcBUVFUFBQUOHDg0ODu5sH28BwC3Ozs7Ozs4zZ86USqXh4eGJiYlr1qxxdHTs379/3759cYlkq8N8i315eXkRERHh4eEREREeHh49e/YMDg7GbsBOJTk5efv27V999RXbQaDVMIfBLl26pKGh0a9fv759+3br1o3tUB0Eeos18fHxFy9evH//fnJyMtNVPXv2FAqFbOcCFmA/YQeWkpJy8eLFS5cu5efn9+3bd/DgwTgX8QWht1Tt2rVrFy5cuHjxorm5eb9+/fr3749bfAJ6qzMoKiq6dOlScnLyyZMnhw4dOnToUF9fX7ZDcRJ6SxXq6+svXrzI1BXzKVb9+vXDzZagEe7z1KlUVVWdPn36zJkzaWlpgwcPHjp0KD5+oUXQW21ILpefO3cuJCTk+vXrzNSqX79+OMkCHof5VudUXFx89uzZ06dPFxQUMDMwfIB4c6C32sSlS5dCQkIuXbo0bdo0X1/f4OBgthNBu4be6uTy8vJOnz4dExOTl5c3duzYcePG4VD3U6C3WlNMTExISMjJkyf9/f1HjBgxaNAgthMBNyQnJ69fv37Dhg1sBwGWJSUlHT169MiRI4MHDx43bhwu2WwSeqsVZGdn//333ydPnjQ1NR0xYsTw4cPFYjHboYBLMN+CR5w4ceLIkSOlpaXjxo0bO3astrY224naEVx3/EIuXbq0f/9+gUDg4eGxefNmS0tLthMBQEcwcuTIkSNHpqenHzlyZOHChTY2NjNmzMC5xwz01nPavXv3n3/+6ebmNnPmzMDAQLbjAOdpaWmxHQHaHTs7u0WLFhHRsWPH3n//fWtr6zlz5nTv3p3tXCxDb7VMWVnZjh07/vzzz9mzZ//5558mJiZsJ4IOorq6mu0I0H6NGTNmzJgxV65cWb9+vUgkevvttzvzmYforeaqqan55ZdfMjIy/P39b968yXYc6Ggw34Jn6t27d+/evcPDw7/66itra+ulS5fq6emxHYoFamwH4IYdO3YMHTpUIpFs2LDh/9q77/imqvcP4E+SNk3SdO89oFAKpXvQMlp2gbK3bAVEUUBFcSAo6g9FRJYMRRThi0xBFMoqq2WUtnSwuvfeTZu0Wff3x8WIpQtIepL0eb948Wpubm4+TU/y5Jx77r3z5s0jHQdpIexvoU4KDQ09cODAoEGDJk2a1D3n8mDd6sDly5fDw8M5HM6NGzdmzZpFOg5CCAEAjB49Ojo6mqKoKVOmpKenk47TpXCcsE1isXjNmjU2NjanT5/GS+kglWIymU5OTqRTIM0ze/bskJCQ3bt3e3h4dJ+hIOxvte7WrVvTpk2bMGHC6tWrsWghVZPL5Xl5eaRTII3k7Oy8cePGmpqajRs3ks7SRbC/1YrDhw/HxsaePn2adBCEEOqUFStWxMXFTZ8+/ejRo6SzqBz2t1r64osvKIrasWMH6SCoG2EwGHZ2dqRTIM0WGBj4wQcfvPrqq6SDqBzWrf84duyYsbHx7NmzSQdB3QtFUUVFRaRTII3n5+f3+eefr169mnQQ1cK69a+ffvpJR0dn+fLlpIOgbgf7W0hZ7Ozsxo8fv2nTJtJBVAjr1hNXrlwpKyubNGkS6SCoO8L+FlKiQYMGyeXyCxcukA6iKjgv44kvv/wyKiqKdArUTTEYDAaDQToF0h5vvPHGjBkzRo4cSTqISmB/CwBg3759kydP1tHBKo7IoCgKryiElMjAwCAkJOTUqVOkg6gE1i0AgD///HP8+PGkUyCEkNKMHDny/PnzpFOoBNYtKCws1NfXt7e3Jx0EIYSUxtvbOycnh3QKlcC6Bfn5+X369CGdAnVrTCbT2tqadAqkVdhstoODQ0VFBekgyod1C0QiEZ7JCZEll8tLS0tJp0DaxtTUVCQSkU6hfIxuuzd4woQJhYWF9C5xekIXAJibm2vriDBSQ8uWLbt79+7TLZCeoJGYmEg6GtJgvr6+LaanUhQVEhKiNacB6r79rUWLFnE4HAaDwWQymUwmg8GgKMrf3590LtSNLF261Nzc/OlJ8AwGoztfxxYphbu7u+LgCpq5ufmSJUtI51Ka7lu3JkyY0OIMBba2tnPnziWXCHU73t7effr0eXrMg81md5+rUSAVmTVrFofDUdykKMrLy6t///5EQylT961b9F9XT0+P/pmiKB8fH/p7CkJdZt68eebm5vTPFEU5OztHRESQDoU0W2RkpKOjo+KmmZnZ/PnziSZSsm5dtyZNmqTocllZWeHpdFHX8/HxUXS5eDzenDlzSCdC2mD27Nn0l3KKojw9Pfv160c6kTJ167pFd7nYbDa9ZwtnwyMi5s2bZ2ZmBgA9evQYM2YM6ThIG0RGRjo5OVEUZWZmtmDBAtJxlKy7161JkyY5OTlhZwsR5Ovr6+npyeFwXnnlFdJZkPaYN28eh8Px9PT09PQknUXJOp4H/3thRlpDTY2kuasidbWqyiqBQODs4kw6iKpY6/HM2Zwh5nZufGPSWTp2rjT3gaBaJJVWSbW2yT1L2CgsKipy6+VGOkiXstXTN9Jlh5rZ9DM0I52lYxfK8u8LqppksnKxxhwRlZGeYW9vz+VxSQfpLFNdvd58kxn2HbwR2qtbWQ11y5KvhpnbWehx+Tq6KgiJuoKcgiKRoLipMdzcfoKtK+k4bWqWyZYnX+vJN9Jn6VrqcWXQTY8s7FYKhQ0VYpGnodlcR/WdEiWj5CtSbjhw+TyWjqUeT44tU2UaJJIKsehaZdFu7zAXfaO2Vmuzbj2qr96WlTLfSX0bE3pefxRnBZpYTbHrSTpIK+QU9WripbHWLvZcPuksqKudKclxNzBRz9JFUdQbSVcHmFr3MjAhnaW7kFPUb/mPV/b07t3Ga976/i0ZRW3JSppur44fcOiFTbLtcb2yOKOhlnSQVmzJvDfQzA6LVvcUaeOSUleZVFtOOkgrdufc9zI2x6LVlZgMxjS7nt9lJsnb6Fa1XreS6yp1GUwuC69HpW0cePwrFYWkU7TiQnlBH/xo6MaceIbRFep4xeeL5QW99DVgx7CW4enoshiMlPrKVu9tvW4ViBoceXiqWS1kz+GXqd9e5ZzG+n6GZky84G83ZsfRr5I0kU7RUnmz0I6rz8O9+yQ48wzzhIJW72q9btVLxTJKruJUiAAdJrNUJCSdoqVmuUwgkZBOgUjSYTILRY2kU7TULJfXSsSkU3RTMqDq2njxu/vxWwghhDQL1i2EEEKaBOsWQgghTYJ1CyGEkCbBuoUQQkiTYN1CCCGkSbBuIYQQ0iRYtxBCCGkSrFsIIYQ0CdYthBBCmgTrFkIIIU2CdUsJMh8k/7blq4cJd0gHQd1XXsajvw/+VF9TTToIQiqnFnXr/95a9N6M0aLGJ6f+LSvIe5QYRzrUc7hy+tj5owfqqls/5X6HUu7ELB4ecOH4QWXnQq37Y9/OpaOCsh6m0Dcb6mrjr114+c3WVlasmBi+Y+2qzqysrCdV2LPhw8M7vxU1tH7+7M6QSMR3oqPEzf+elB1bZhdTUcsk69l29fLI1y2ZTJb1MLk0P7e+thYAbl86++70UfHXLpHO1XUKMh6LGgXZD1NJB+kuMh8mN9bXFWZnAEBVWfFbkYNP7vvh5TdbVV5aVVaSkZrU8ZrKe1Il+njuxO0fr5SImxVLsGV2MRW1TLKebVcvj/yVIVks1qe7DzXU11nZOQCAqLGBdKKuNnL6XDMb+37+waSDdBeLP/wy4/4930HDAEAqlkiUdKGKHh6eq7f8aG5t2+GaSnxSJRI1tryMCLbMLqailknWs+3q5Smnbn298tXUO7ErN273HzICABKuXz71y64NPx+n792xdtXtS+c+3LY/Pyvt0NaNfoOHCxvqsx6mcDjczccvLB0ZJJfLAWDPhTv3Yq7u2/gpAJw/euD80QOWdg7fHb8IAHXVVUd2fXcv5nJTo9DO1W3c3MXBw0Z3Jlj8tUvnDu/Py3jEZOn27Os5fdk7zr08ACDp5rUTP20vzExnc7megaGz3lptZmkDAFs+eDPjftK4Oa9dPnm4tqrC1tl15rL3+gYMoLcmk8nO/f5LzN9/lBYVGBga9x8waMaydwxNTJ9+xtz0h5/Mn+zW33fdnv/RS9YunJLz+MHXh/+2c+5x6cThc7/vryovM7O0GjxuyoT5S0/t33V871YAGDV93txVHwGAsKH+yK7v7l69KBIIrOwdR89aEBY5VbHl0TPnl+TnZKQksTkc/yHDZr6xmsPjKeWPqEFuXzq3Y+0qxStWX1v97pSRW05e4hsZA8Dlk7/v37R+wsJlAWEjPpk/2c7VzbmXe9LN62KR6P3vf/pj/66H8bcAYOXG7S59+r07fRQA5Gc8njPAHQC2nr5iZmkjlUrPHNh77a8TtZXlphbWg8ZOipy3REenvTdLWnL8htfnAICjm/tXB04BwOLhAW6eXha29gnXo8VNTb36+8x79xNLW4eq8pJWn3Tx8ABRo2DCwmUxf5+qqSqf/OryiQuXtdUYAICiqL//93P0H7/XlJdZ2TtWV1YowtCb+uXGfTrz/7Z9ffbw/oWr1w+bPJN+uf74aWdiTHR9dZWptc2gMZPGzXnt/ZkRNZVlALB0ZBAAvP7p15WlxZ1sme38sipuCGon/tql79cs77BlTluy4tk/98PEOFW0TAA49/svz37wcvUNctIeHN31XXpKIoPB7NXfZ9rrq1x69wWAfV9/euXU0b7+IcW5WYL6WhtH5zGzFgwaM4neWvsfUC3ebj9+9XGLdjUwYsLLv87KGSccMDISAO5efTIUe/XM8ZxH93PTHwJAs0h4L/aqsblFH78g+t6E65cENdXBw8aEjZ/G1TfwHTRMR/fJ5UQtbO1c+vQDAGtH5+DhET6h4fQg72dLZl7/6wSPb+ji4VmUnbHjk5XRp490mCrqyK/fr1menpJo7eBiYW2bcjtGUFsDAPHXLmx+7/W89Edu/X0MTUxvXzq74fU5woZ6+lH11VW/79jk3Nujf9Cg3McPv313aXlxAf0xse2jFb/v2FReUujS20OXzY67HAXPc4Xe+3dv/vLtZ3XVld4DBnN4/KqyYgCwdnRx6NlbsY5UItn49quXT/6uq8t28/IrKy786atPoo78+u8v9fuvZYX5QcNG63E4l04cPrRt43Mk0BZ9fAMBIP7qRfpmzLnTImHDjXOn6Ju3Lv0NACEjx9E3i7IzUm/H+A0e3n/AYHefgF79fYzNLem79PS43iFDAIDHNwweHhE8PEJPj0tR1PaPV574cVtzk6hHXy9ho+DEj9v2bFjTfiS+kYnHM/2SlNsxty6e6x88yM61Z9LNa5vffV0qlbb6pIqHnDmwt7ePfx+foEFjJ7bfGH7b8tXvOzZVlhbbuvQUCRuFgrrOvHSC2pr1r824eOKQWNzs4uEpFNQl37ymo6PjExquq8cBAP8hI4KHR1jY2j1vy2z1l+1MJG3Sy8u38y2zxZ9bRS1TocUHb8b9pM+XvpJ6J9bWuYe1g3PK7ZgNr7+Sl/FIsX5O2oO+AcEevgGFWel7Nnx49c9jnWkGLd5uz7YrZbzMSupv+Q8Zvv8bzr2Ya1KJRFBbk3zrOgBcOX104er192KvNotEYeOnMZlPaqSFrf3nPx9jc568V1du3P766OCGuloA6O3lP3TC9H2P7nsFD6a/sADAH/t/KC8qGDppxsLV6xkMRkFW+icLJh/dtWXIuKksFqutSLWVFUd2bmYwGB9s3dcvIAQAinKz7Jx7AMChbd9QFPXm+k3Bw8fIZLLN7y1NuR1z+eSRyHmL6ccufH99+ITpii+qN8//NXHhsoTrlxKuXzK1sP507yFzazt6g4bGpm0FeFZBVjoABIaPXvLJVwDQJBQCQPCw0fU1lQc2f0Gvc+viX9mPUp16eazbc4jN4aanJH6+dPbJn3YOmzSTXsHKwenLX07qcXn1tdUrxofdOPvHgtXr2nkdtJKRqZlbf9+MlMSsh6k9PDyvnzkBAFdPH4uYuaCmojwtKd7JrY+dcw/6mxOTyfxo5wF7Vzf6sVMXv12UnUl/x+IbGc9d+VHSzWvmNrbLN2yhV4i/dinh+iWnXh6f7j6ox+UJGxs+XTT11oW/xr6yiO6st8rOucfclR99OGd8i+Ub9h21cnBSdLuzHiT19vJ/9kkV5r+zlu4VAcCNs3+01RjKigouHPtNV4/z6e6DLu79ZDLZB7PHlubndvjSndq/q7yowDModNXGHWwOV9wkqquuAoC5qz6Kiz5f09y0+OMv9A2M6JU72TJ12Xrt/LKd+5NqCUNj0860TMX6T/+5VdQyFVp88P7yzWeS5qY3P988YMRYAIg+deTnr9ed/GnHqq930ivMWfHB4LGTAeDm+TM/rF/954G9YeOndfgB1eLt1mq7ennKqVs8fb5PaFhcdNSDhNu5aQ/lMhnfyPhm1F+zl79/6+JZAAgZGalY2Sc0XPHadUbijWj6U/7w9m/oJVx9fkNdbXlhvo2TS1uPSomLlUjE/YMH0kWL/mShJytWFBcaGpsEDYug964NGjMp5XbM4+S7kfCkblnY2tM/0J2/8qICAEi8cQUARkx9hS5aig12nmfQQJaOTkzUaTZHL2LWInp/XgupcTcBYEjkFPol6tXf18bJpSQvJz8znaXDAgBDEzM9Lo9+h5jb2pXk5dRUlCoidR8hI8ZlpCTGX7sgk0kLczL5RsZFuVlpyfE5jx9QFBUycqxiTTtXN8W7qDPo9sbh8U78uJ1eQveHsh+mdubToQUzmyd/Gmf3vjmPH5QVFbb/UR40PELxczuN4XHiHQAYMHyMi3s/uhmz9Tid+u1iogFgyuK36W2yOVxFa29fO2F6eHi+2C+rlTrfMlv8uTv0ki3z6Q/eytKivIxHLB2dnEf3cx7dBwCxuAkAFFMZAYDJfPJteMDIcXu//Ki8qEBQW9PhB9Tzvt1ejNLmZYSMHBsXHZVw9eKDhNsWtvbTlqz4Yf3q6NNHU25dt7J3VLRsAOA+5/6YmsoKuua3WM7m6LXzqLrKCgCwtHNssby+rgYADM0sGIwnY3wGxiYA0FjXyjCLLpsNAFKpBABqq8oBwNL+xYfs7V16vv/dj/u//ezSicPRp47SOzBarCOorQYAE3MLxRIDY9OSvJyG+lojU7Nn4ukBgEzS7UZjACBo+Ojfvv8y/tql+poaBoOx4qtt//f2wiunj5YV5ANA8IgxijU5PP3n2jL9h05Lik9Lin96uS67U4WhLWw2BwBkHe1pfzptO42hpopu3s/dGmsqX/CB7YR5duVO/rJaqfMt83kb50u2zKc/eGurKgFAJpWePbz/6XXYrW2KwWDoGxrVVVU2Ngg6/IB63rfbi1Fa3fIaMJjHN7x+9g+pRDJz+erAoaP/t+ObI7u2SCXi4BFjO7GB/6DkcsXPPD6/vrr5m8NnbZ1dO78FnoEhANRUlLdYbmhkAgD1NVWKJTUVFQDANzbpYIN8QwCorWy5wRaYDCYAwFP5n9Y3YMDX//v7xtk/fvl2w/G9W70GDKK/LysYGJsCQH31v0eP1laUK2IjBUNj034BA1Jux1QUF3oNGNzHN9Bv0LA7l6IkEnFvLz8zq44n9T1N/p/2ZgAAC9//bNikGSoI3vqTtqqdxmBsag4A9B7vZzGYTACgqFa2r29gUFfVXFtR3tYQNyWnnjdM+79Fd6MRLZOrzwcAY3OLHWdudLiyuLlJUFMNAPp8gxduBm21qxejtOO3dNl6/mEjpBKJrh4nbNwUHV3doRNnSMXNLfZDdoirbwAAJfk59N9MKpX28Qmg93LRs0KlEklWJw4ocffxB4Ckm1fTU+/RS3LSHoibmyztHc0sbeqrqxKuX6aPiaOnePT162Cybx/fIAC4cPyQohYqtkyTSsT0OB4AFOfnCBsb6CctzstWrFNamM9iscIip3oGhgBAWWF+i2fx8A2kd2zQhzvci71aXlxgYGz89B5yRBswIpJuDyOmvAIAI6fNoVvI04PSHeLo8wGgqrRE3CSi24O7dyAAnD/yq+LcE+nJCcpN/uyTtrpaO43BqbcHANyM+oveaUpR1NPHxxiZmgIAPf5TX1udevem4q4+PoH0Xi56fYlETK8GAFx9fbrpthoJW2bnqX/LtHF0MTIzr62suHD8EL2krrqqxf5R+g9NUdTp/bvkcrmdcw8DY5MXaAbtt6sXo8zjt0JGjr3+14kBI8bSkz6HTphx+pfd9q69nms/kKtHPyaLlRoXu2bOeFGD4KPtv0xa9GbSzWu3Lvz1MOG2pa1DWUEug8XacuJS+wP6ds49Bo+bcv2vE1+8/oqdqxuDwSjMSl+wet3QiTOmvb5y9+cfbP9kZc9+3pWlxZUlRVb2jmHjp7UfbFDE+AvHDxZlZ7w3Y5Sdc8+Gutry4gK6F8jh8gAg+db1QWMmGZtb2PfoVZiV/v7MMaYWVrlpDxTfmEoL89+fMbpHP29DY5OU2zd02Ho9PPq3fA1HRZ47ciDzQfLqmWPMrW0z7ycBwNQlqxRTLpGC/5DhP3/DMbWw7D9gED3J0L5Hr5K87MChozq/ESNTM0s7h/KigtUzxnANDEZPnztozISLxw8W5Wa9M3W4vYtbfU11eXHBhl9O0FOEleLZJ221+bXTGDwDQ+n9/x/Pn2Tn0lMoqK8qK1E80DMgtCQv55tVrzn06F2Qld4k/PcAmkmvvpl082rclfOP78VZ2TuVFebpsjmbT1zU0dFx6+9bnJf97TtLrRwcHXr0XvLxl50Mo6yXRWuof8tkMpkzlr2z94uPDmzecOHYb1x9fnFuVr+AEMWkDAD4dfOG6NNHaysq6G79tNdXvlgzaL9dvRhlni/Dwy/Y2NxixNTZ9E1jc4uA8FEDRj7fIKGlrcNrH24ws7Ipycum5JQuR8/e1W3t7kPeIUPEoqbsR6kcHj901HiqozEWAHh1zeczlr1rYedQnJtVVVbi7htE7zAcGDFh+YYtds49M+8nCRsaQkZFfvzDb/SXgnawOdxPdh4Inzidw9PPy3gkFjeFjo7U43EBIGjYaJ6BUU1FOX2qquUbvuvt7S9qFNRVV46b85q9S096CzKppG/AgLz0h/fv3nTu5fHet7ue3SXO1uN8tP2XQWMmNQkbM+8nWTk4L1m7UdUDVhqKq6/vOzBs+OTZil2VI6e+4hkYatDRkG8Lb37+nVMvj7qaypqKMr6RiR6X9/Gu38InTGdzuNmPUpuahMHDx+gbGCo3fIsnbXWd9hvDqo07BkaM5/D4lcVF9q496QMQaVOWvBUyKpKlo1uUm+0/ZFjQUwc72jn3WLfnsM/AcIlYkpv2kMPjh46OlMukADD99VXeIUNkMklJXjbdY+t8GPQ0jWiZg8dOfvurrS59+lWVFBdkZVjbO/cPGvT0ClYOzqX5eQ2COjdPn9Wb99LH5r5AM2i/Xb0YBkW1Muz4W0FagVAw1KJTE42QBikUNVypKNrpPYR0YKLXdgAAIABJREFUkP94LKj5LiNpkXMf0kEQMRXNouPFWb/6DScd5D8KRA0fPbj1pqtnJ9bVHvRxx8o6RviFXakssuPoz3N0f/Yu8ud5emHpqff+2LejrXsXrF7f6kRzhF5Mk1C49aO32rp32KSZ9BdShLpYN2yZGly36qsrU+/EtnWv4uzyCCmFTCZpp731Dx7U1l0IqVQ3bJkaXLf8h4w4eOsx6RSou9A3MML2htSQ0lvmqx98/uoHnytxg0pH/jomCCGEUOdh3UIIIaRJsG4hhBDSJFi3EEIIaRKsWwghhDQJ1i2EEEKaBOsWQgghTYJ1CyGEkCbBuoUQQkiTtF63mBQw/zmTMdImDIrSZarhlxX1TIW6DgNA758Lw6sPCoCNLZMQJgVMaL0Mtf4nMWVz6rvlNba1Xq1UbKh+F0wyY3MrxCLSKRBJdVKxgY7atUwLNqe8GVsmGXVSsXkbF1lsvW458wyEMqmKUyECqpub+vKVcwkcJTJlcwx0dEXY5LqxymZRXwO1a5kcJsuRx6+TNHdiXaRkjVKJC6/1q4u1Xrf6GJpyWazHghoVB0NdSiKXX60qnuHQi3SQllgMRqS1y6XyAtJBEDFR5flzW7vSElkMBmOSjetFbJld7kF9lYEuu7dB61fabHPo9kuPAXdryx7UV6syG+o6tZLmA/mPf/QdSjpI6ybaurrqG/1VkkM6COpqjVLJT7kPdnqFqec+zuGWjoEmVieLs0gH6Ubu11cl11V94RHc1gqtX+9YYd2jO0WiBmNdPZ76DT0rC0VRFEUx1fI9oxQclk5mQ62Bju77br52XD7pOO05VJCWWFMhpmR2XH73GqmmQCaXsVhqNzFBpbgsnayGWh5L9+0e/XvyjUnHac/xosxb1aXNcpkjl9+gOS1TJpOxmKw2JjeoI6FUUicV23P46/oEtrNaB3ULAAqEgqzG+mpJk7ITqotHjx7l5OSMGTOGdBBVMdDRc+bx3dT7c0GhokmYKxKUNQvFcjnpLF2noqIiKipq7ty5pIN0Kb6OrhPXoK2xIHVTI27KFtaXNgmb5TLSWTrr119/HTt2rLm5OekgnWXG5rjyDB14Bu2v1vF1Ix14Bh1uRaOdS04X5ZZNtu1BOggCALDg8Cw4PNIpulp6g+xqUtrkD7ARqi8TNseP3fr0NrV1JClt6LR5PbXuw01rB8cQQghpJaxbCCGENAnWLdDR0TEw0OaBUKQR+Hy1njKDNJGZmZlWzjjTwl/pebFYLKlUYyYIIW3FZrNJR0Daprm5WSvbFdYtMDIyqq2tJZ0CdXfV1XisJFIygUBgaNj6KSc0GtYtcHFxefz4MekUqLvj8brdLEqkUrW1tY2NjVi3tJOpqSmbzc7NzSUdBHVrQqGQdASkVVJSUnr1UruTuikF1i0AgLFjx964cYN0CoQQUpq4uLjw8HDSKVQC6xYAwMSJE+Pi4kinQN0Xg8Gws7MjnQJplfT09IiICNIpVALrFtBDhb6+vseOHSMdBHVTFEUVFRWRToG0x7FjxwYMGKCrfhfbUwqsW08sXLjw4MGDhYWFpIOg7ojBYGjl/nNEhEAg2Llz58KFC0kHURWsW//as2fPe++9RzoF6o4oiqqvryedAmmJtWvXbt68mXQKFcK69S9ra+uvv/767bffJh0EIYRe0P/+97+goCA/Pz/SQVQI69Z/ODk5vfHGG++//z7pIKh7YTAYLi4upFMgjbdly5b8/PxZs2aRDqJaWLdacnd3nzVrVmRkJJ78CXUZiqJycvBaz+ilHDhwoGfPnmvWrCEdROWwbrXCx8dnz549ixcvjomJIZ0FIYQ69u2339bU1ERGRpIO0hWwbrXO1tZ2//79x44d++abb0hnQdqPwWC4urqSToE0UlNT07x581xdXVesWEE6SxfButWerVu3urq6Dh8+HDteSKUoisrOziadAmme8+fPT5gw4YMPPpg8eTLpLF1Hh3QAdTd16tRhw4atX7/+zp07c+fOtbS0JJ0IIYSgsbHxxx9/LC8vP3/+POksXQ37Wx0zMTHZunWrn5/f/PnzN2/eLJfLSSdC2obJZDo5OZFOgTTGoUOHIiIivL29v/rqK9JZCMC61VlhYWHnzp2zsbGZO3fuDz/8IBaLSSdC2kMul+fl5ZFOgTTA5cuXly5dWlZWdv369bCwMNJxyMC69Xxmz5596NAhPT29IUOGbNmyRSAQkE6EEOoWEhIS5s6de/78+U8//fSdd94hHYckBkVRpDNoqoMHD8bGxpqbm8+aNcvDw4N0HKTBMjIy9uzZ8+2335IOgtTR1atXf/755169ek2ePBk/arBuKcHZs2cPHz6sq6s7f/78IUOGkI6DNFJ6evq6desOHz5MOghSL6dOnTp48KCTk9OiRYv69u1LOo66wPmEL2vMmDFjxoxJTk6+cuXKmjVrIiMjx40b179/f9K5EEKaKisr6/jx4ydOnJg4ceKmTZvwHGAtYN1SDi8vLy8vrzfeeOPMmTNbtmypra2lCxjOm0edxOfzSUdA5EVFRR0/fry+vn7q1Km3bt1isVikE6kjrFvKxGazp0yZMmXKlPz8/DNnzsyfP9/Z2TkyMnLMmDGkoyF119DQQDoCIqakpOTYsWMnTpwYOHDgm2++6ePjQzqRWsP9W6oVFxd35syZqKio6dOnBwQEdNt5q6h96enpW7du3blzJ+kgqEsJBIKoqKioqCgDAwMfH58pU6Zgt7szsG51BblcfvHixQsXLly/fj08PHzYsGHh4eFsNpt0LqQucF5GtyKTyc6fPx8VFZWSkjJ69OjRo0d7e3uTDqVJsG51KblcfuXKlcuXL1+5ciUgIGDYsGFDhw41MDAgnQsRhnWrm7h27dq5c+eio6NHjRo1evTo0NBQ0ok0EtYtYmJjYy9fvhwdHd27d++hQ4cOGzbM3NycdChERmZm5s8//9w9z9nTHdy8eTM+Pv7IkSNBQUEREREjRowgnUizYd0iLz4+Pjo6uri4uLy8PDQ0NDQ0FAcNuhvsb2kfiURy9erVK1euXLlyxd/fPyIiYujQoRwOh3QubYDzCcnz9/f39/cHgLS0tNjY2O3bt2dmZob+w9jYmHRA1BX09fVJR0BKUFtbe/Xq1ejo6Li4uLCwsPDw8PXr1+PObOXC/pY6amhoiP2Hvb39wIEDQ0ND8fwuWgz7W5quqKiI7lrl5uaGhYUNHToU912pDtYtdXf//v2YmJjY2NiSkpLQ0NCBAwcGBARgJ0zLYN3SUAkJCTdu3IiNjTUyMurbt294eDgO8ncBrFsao6amJjY2Ni4uLjY21srKKigoKDg4ODAwkMFgkI6GXlZGRsa2bdu2b99OOgjqWE1NTcw/+vXrRw+HuLq6ks7VjWDd0khpaWl37ty5fft2XFxcQEBAcHBwUFCQu7s76VzoBWF/S/09ePAgNjY2JiamuLh44D9wngURWLc0Xlxc3O3bt+/cuVNcXEx3woKCgmxsbEjnQs8B+1vqqba29s6dO+np6adPn7a1taUH6vG87MThfEKNFxgYGBgYCAD19fV0J+ynn34yMjJyd3enZypaWFiQzog6QFFUXV0d6RToiYSEhJs3b96+fbu0tDQoKCg8PHzOnDkmJiakc6EnsL+lnQoLC+P/wePx/P39/fz8/P39zczMSEdDrcBxQuJyc3Pv3LlDlytvb+8BAwYEBwfj2Lt6wrql/fLy8uLj4xMSEuLj4w0NDf3/gZMSiZs2bVpmZiaTyQQABoMhl8sZDAZFUYmJiaSjdQt1dXVxcXH0nEAOhxMUFBQSEhIcHKyjgwNRag3rVveSk5Oj6IeZm5uHhIT07dvXx8cH+2FEREdHf/bZZ42NjU8vdHNzw46X6kil0jt37sTFxd29e7e0tDQwMHDAgAFBQUHW1tako6HOwrrVfWVmZiYnJ8fFxd27d4++jIKvr6+vry++gbvSggUL7t+/r7ipp6f39ttvz5gxg2goLZSUlBQXFxcXF5eamhoUFBQYGBgQENC7d2/SudCLwLqFgB7cv3fvXmJiYmJiIoPB8PX19fPz8/HxcXR0JB1Ny126dOnzzz8XCoX0TTc3twMHDujq6pLOpQ0eP34cHx9Pd608PDzoGUx4SUYtgHULtVRSUpKYmJiQkHDv3j2hUDh69GhbW1sfH59evXqRjqadFi1alJKSAgAsFuvdd9+dPn066UQaLDMzMz4+/u7du/Hx8fb29v7+/nTXCs8QqE2wbqH2VFZW3r9/nx5LLCws9Pb29vHxof/H83Qoi6LL5ejoeOzYMRaLRTqRhsnNzVXUKnNzc39//4CAAH9/f7x2sLbCuoU6SygUJiUl3bt3j/6/f//+dAHz8fHBD4iXtHDhwgcPHqxatWrWrFmks2iG3NzchISEhISErKwsqVSqqFU4S7Y7wLqFXlBycjJdwB4+fGhsbOzl5eXt7e3l5WVvb99lGe5Ul+YKBdWS5i57RhWhj7cbP348PSdec/FZusa67B76hh6Gyp+hqqhVCQkJfD7fz88Pj0rsnrBuISXIysqiy1hycnJTU5PXP/r166eiZ6xoFq1KuWHG5thx+boa/lmvTdhMVmlTIwAY6+q966aEGRA5OTmJiYnx8fFVVVVVVVV+/8CLg3dnWLeQklVWVib/4/Hjx6NGjbK2tqbLmLKGE8uahF+k3Y2wcjJl41lN1VRMZTEw4D033xbL165dm5qaeurUqXYem52drehXGRkZ+fr6+vv7+/r6Yq1CNKxbSIWkUmlqampiYiJdxqysrJ53OHHFihVbt25tsXBO/IXZ9r2waKm5S+UFLvqGrzj8e4zU0qVLExMTeTzetWvXWqycmZlJ96sSExNNTEwU/SpTU9MuD47UHdYt1HWeHk4UiUSKGtbOcGJwcLCnp+emTZsU+9tvVpUcL8qcbu/WhcHRixBKJbty7h8LigCAioqKpUuX5ubmMplMiqISEhLoWqXoV5mbmyv6VXgGW9Q+rFuIjKqqKkUNe/ToET2QSJcxAwMDxWr+/v4URbm6uq5bt44ub4cK0opEDYPN7YjGR52yIytli9eg/NSHGzZsKCwspBdSFBUeHp6QkGBpaanoV+E8QNR5WLcQeTKZjB5IpMuYhYUFXcN++OGH8vJyeh0rK6vXXntt0qRJ27KTKYoKMsGTUWmAvbkPBhTVn/xhb0VFxdPLTU1Njx8/bmhoSC4a0mB42mNEHovFok+NSN/Mzs5OTk6+e/fu0x92ZWVl27Zty8nJ0Zk4jFxS9NwOHTpUX1bGYDCePlBdLBZj0UIvDPtbSH35+vq2OJ5JR0fHZvHMsLAw7G9phL25D4ZUNBUlJKekpAgEgrq6OqFQSBcwehcXQi8A+1tI3dFfrfT19U1MTMzNzSV8g048CKmLgQMH9hg1lp6akZmZSZ++WbGvC6EXgHULqakxY8ZwuVwjIyMrKytPT09vb+/evXvb2dnR+7dIp0PPzcLCwsLCYsCAAaSDII2HdQupqbNnzwJAaWkpXg8MIfQ0PEEOUmtYtBBCLWDdQgghpEmwbiGEENIkWLcQQghpEqxbCCGENAnWLYQQQpoE6xZCL0Lc3BR15NcD331JOghC3Q7WLdQtpKfe+2zxzEXh3ismDa0oLvxj386lo4KyHqa88AYFdTUHv/+/1LgY+mZtZcWKieE71q5SXmQ1UlaQ9ygxjnQKhJ7AuoW0X01F+aZVS7Ieprh7+zv27G1ha5/5MLmxvq4wO0NZT1FVXlpVVpKRmqSsDaqP25fOvjt9VPy1S6SDIPQEni8Dab97N6+IGgXj5y2ZvuwdesniD7/MuH/Pd5DSTi3fw8Nz9ZYfza1tlbVB9SFqbCAdAaH/wLqFtNzXK19NvRMLAH8e2Pvngb1fH/77181fPIy/BQArN273HzLi3O+/HNq6cc7KD2PP/1mcm21sbjlq2pyR0+bSD7996dyJn7ZVlBTr6uj29PSa+eZ7Tm59WjxFWnL8htfnAICjm/tXB05dPvn7/k3rW6xjZmWz9dQVAKirrjqy67t7MZebGoV2rm7j5i4OHja6w99i8fAAUaNgwsJlMX+fqqkqn/zq8okLlwHA7ctRZ37dU5ybxeHzfULDZ77xrqGJKQCsnhlRkpfjN3j4o8Q4uVzq6tF/6pIVvTx96K0V5Wb9vvPbR4lxcrmsh0f/aUtW9PLyAwD6pfAbPFzYUJ/1MIXD4U5duvLnr9cBwPmjB84fPWBp5/Dd8YtK+ssg9IJwnBBpuR59vawcnADA1snVO2QIh8fr1d/H2NyyxWoHv/8/PQ4vaGhEfXX1ge++vHn+DL1cKhHLpNJent4GJiapd2K/XvmauEnU4rF8IxMP/2DFTUNTUxf3vvQ/59596YVTFr8FAA11tZ8tmXn9rxM8vqGLh2dRdsaOT1ZGnz7Syd/lzIG9vX38+/gEDRo7EQCijvy645OVxfk5rh6eXK7+9b9ObFj2iqixUbF+YU6G3+BhNk4uD+Nv/9+b8+lx0Yriws+WzL4Xc8XK3snJrc+jxLiv3lqQ9TBV8aiE65cENdXBw8aEjZ9m59LDpU8/ALB2dA4eHuETGv6cLz9Cyof9LaTlpi5+m8Fg/LFv5+DIKeNeeZVeUpSdeffqhadXCxkV+cb6TQDgHzbiu9XLrv51MmRUJACEjh4/MGICvc6WD5YnXL/0MDHOO2TI04+1c+4xd+VHH84ZT98MCBsZEDaS/vns4f25aQ+8Q8MGj50MAH/s/6G8qGDopBkLV69nMBgFWemfLJh8dNeWIeOmslisDn+X+e+sHTZ5Jv1zXVXlkZ2bOTz9DT8ft3FyoShq12fv3zx/5uqZYxEzF9DrrPl+n4WtPQD8vPHT6NNHz/3+6+KPvji5b6dQUDd00oxF738GAKd/3X1s9/cnftz6/paf6EdZ2Np//vMxNodL3xw6Yfq+R/e9ggfPXfXRy/0pEFIOrFsIAQBY/LNrytW9HwBUFBfQN2sqy/78dW9qXGx1eRl9wd7yf+7qUEFW+rFdW3gGRq9+8Dm9JPFGNAA0CYWHt39DL+Hq8xvqassL822cXDrcYNDwCMXPyXdiJBKxsYXlldNH6SX0jqine05MnSe1cGDEhOjTRzMfJAPA/bibADBy6hz6riFjpxzb/f3jpHjFo3xCwxVFCyE1hHULof/Q1WMDgFQsAYBGQd26RTNqKstc+3j29Q3KenQ/L/1hs7DlOGGrJBLxrs/el0jEiz/+0sTiybBkTWUFACgGIRXYHL3ObJPD01f8XFdZQQ/6nT28/z+b0uM8+0C+kQkAiAQCAGioqwEAYzML+i4DE1MAEDc1ScTN9BIuj9eZMAiRgnULoTbdvXqxprLMf8iIlRu3A8Cp/bvy0h928qqVx/duy8947D9kBD3eSOPx+fXVzd8cPmvr7PqS2Xh8AwAIHj5m+YbvOly5qrwEALgGBnQNq6ksq6up4hsZA0BtZRkAcHg8XXZ7tZOSy18yMELKgvMyEGpTk7ARACxt7embGamJACCXyxQrSMTiVh/4OOnu2UP7DIyNF77/n4mFfXwC6L1cEokYAKQSydPDes/F3TcAABJuRCu2kJP2oFkkfHodabOYHpb8++A+AOjj4w8AHv6BAKAYXTx/9CAAePgFt/YkAABcfQMAKMnPAQC5XC6VSl8sMELKgv0thNrUu78fAFw4frCsKL+6vDTn8QMAKMnPBgAOlwcAlSVFhdkZ9q5uTz9K1CjY/fkaulv27btL6IVsPc7a3YcmLXoz6ea1Wxf+ephw29LWoawgl8FibTlxqdXBvfbZOfcYFDHxxrlTny2e4ejWRyqVFOdkznrrfcWkDABY++p0K3uHsoJ8YUM9z8Bo7OxXAWDC/GXx1y5F/f7r43vxDAbkPH6gw9ab/Nrytp7I1aMfk8VKjYtdM2e8qEHw0fZf6PmZCJGC/S2E2uTSp9/ij780s7JJuXUDGIzVW360dXLNfnRfIhHrGxgFhI3kGxk/e7Koy6eOVpYUAYCgtjbn8QP6X276QwCwd3Vbu/uQd8gQsagp+1Eqh8cPHTX+hYfgXvv4y2mvr7Swtc/PfFxVUuzuG+jU0/3pFazs7AuzMwHAb/DwdXv/R88ttHV2/eSH3/oFhJTkZxflZnn4BX3ywwHnXh5tPYulrcNrH24ws7Ipycum5JRu53bFIaQ6jE4O1iOkJrZlJ1MUFWRiTTqIWqOPO956+oqZpQ3BGHtzH3zc27+HvhHBDEj74DghQuSlp977Y9+Otu5dsHq9lZ1D1yZCSH1h3UKIvPrqSvpkVK0SNQq6Ng5Cag3rFkLk+Q8ZcfDWYyVucNPv55S4NYTUCs7LQAghpEmwbiGEENIkWLcQQghpEqxbCCGENAnWLYQQQpoE6xZCCCFNgnULIYSQJsG6hRBCSJNg3UIIIaRJsG4hhBDSJFi3kIYx1eGIZXjtXc3AAODr6JJOgbQN1i2kYVz0DYubGkmnQB0TyqTV4iYrPR7pIEjbYN1CGibE1Lq0qVEgFZMOgjoQX1023tqFdAqkhbBuIQ3DYDA29Rv4R3F2g1RCOgtq0+3qUqFcutC5zcsoI/TC8HrHSCMVixrfSrnmyjO05fI5LLwcj7rQZTCLmxrkFLAYjLXuAaTjIO2EdQtpsKsVhRmNdeXNQtJBXlZjY2NaWpqvry/pIC/LUEfPQo/jpm/sZ2JJOgvSWli3ECIvPT193bp1hw8fJh0EIQ2A+7cQQghpEqxbCCGENAnWLYTIYzAYdnZ2pFMgpBmwbiFEHkVRRUVFpFMgpBmwbiFEHoPB0NXF8yEh1ClYtxAij6IoiQQPo0aoU7BuIUQeg8EwNDQknQIhzYB1CyHyKIqqr68nnQIhzYB1CyHyGAyGq6sr6RQIaQasWwiRR1FUdnY26RQIaQasWwghhDQJ1i2E1AKXyyUdASHNgHULIbUgEolIR0BIM2DdQog8BoNhaYkX/kCoU7BuIUQeRVHl5eWkUyCkGbBuIYQQ0iRYtxAij8lkOjk5kU6BkGbAuoUQeXK5PC8vj3QKhDQD1i2EEEKaBOsWQuQxmUwXFxfSKRDSDFi3ECJPLpfn5OSQToGQZsC6hRBCSJNg3UKIPBwnRKjzsG4hRB6OEyLUeVi3EEIIaRKsWwiRx2Aw7OzsSKdASDNg3UKIPIqiioqKSKdASDNg3UIIIaRJsG4hRB6DwdDV1SWdAiHNgHULIfIoipJIJKRTIKQZsG4hRB6eDx6hzsO6hRB5eD54hDoP6xZCCCFNgnULIbXA5/NJR0BIM2DdQkgtNDQ0kI6AkGbAuoUQeTgvA6HOw7qFEHk4LwOhzsO6hRB5DAbD2dmZdAqENAPWLYTIoygqNzeXdAqENAPWLYTIYzAYJiYmpFMgpBkYFEWRzoBQNzV9+nSxWAwAYrG4vr7e3NwcAEQi0fnz50lHQ0h9YX8LIWKmTJlSUlJSWFhYXl7e1NRUWFhYWFiIB3Ih1D6sWwgRM2PGDHt7+6eXMBiMoUOHkkuEkAbAuoUQSVOnTmWxWIqbDg4Os2bNIpoIIXWHdQshkmbNmuXg4KC4OXLkSFNTU6KJEFJ3WLcQImz27Nl6enp0Z2vatGmk4yCk7rBuIUTY5MmTbW1tAWDUqFFmZmak4yCk7ljr168nnQEhDSOUSXSZrCpxU3xteVpDbZ1EbMPRrxI3xVSVvNjPzWydTF1q0iuznI1MX2Y7VeKmmzWlIqnUUo/bIJWwmaxO/DYIaRg8fguh51DS1Ph/afENMqmPkUVRU0NRc6NYKuWwdCzY3Ca5rKJZSPznSnGTkS67h75Ro1SS21gfbmG/wKmPRC7XZeLgCtISWLcQ6lhyXeXRwgxrjn50RUGjTEo6zvPpxTe21OPacfgLnNxZDKxeSONh3UKoA3lCwccPbpaLm0gHeSkcJmugme2Knl56OHiINBzWLYTalCusO1GUfbmiQKotbxMnroETz+AT9wDSQRB6cVi3EGpdk0y6PPlavkjbLkPMY7IGW9i909OHdBCEXhAOdiPUColc/k16ovYVLQAQymXR5YVxNWWkgyD0grC/hVBLjVLJl2nx8bXlpIOokD5LZ5iFw/Ie/UkHQei5YX8LoZY+fHAzQauLFgA0yqQxVcVnS/BilUjzYN1C6D9EMmm1uLk7jEJUS5orJZo9SRJ1T1i3EPqXnKKOFGaUi0Wkg3SR40WZNyqLSKdA6Plg3ULoX0cLM04WZ5FO0XWa5LKfch9WNneXOo20A9YthP6VXF/VJJeRTtGm5I+/vrv8Y+Vus0EqLW0SKnebCKkU1i2E/mXN4ZGO0J769Cy+i0MnVnwOApmYhacuRBoF2ytCTxQKG2LUeGePRNDYVFLOd3FS+pb35NxvlqlvLxOhFnRIB0BIXRwrzqyTSlS3/dKL1/OOnWnIymNxuVbhA3qveI2po5N76GTJxRt93lmSvvMXQUaOnrmp+ztLzIN96YeUx8Tl/nZCkJmjZ25qN3Y4APBdHZUerLxZ+EhQ7W1sofQtI6QK2N9CqCtk7j2Y+tl3XBurPqvfcJw2tvDU+cLT5wFA1tTckJX7YOMO6xGDe729SCIQPP7+R/ohhafPJ6/5isXV6/Pe6xaDgjL3HgQAfWWPEwKALpNprKun9M0ipCLY30LoiUBTq3NlearYcvW9+zkHjjtOj+z99qsAQMnluQdPNpdXAYBUKNLhcf13fKlnagwAgrSs4rOXAUBUUp62dZ/l4OD+X37AYDAAoCErryE7j21kqPR4IpnMWV/5m0VIRbBuIfRETGWJirZccOJvYDAsBweLa2pFZZX5R/6UNTVbDA4CgMbcAn1XR7poAYBM1KRraAAARWcuyqVStzfm00ULAKQNjXwX5Q8SAkC9pDmmsnigua0qNo6Q0mHdQuiJymZVTQevf5TJ4ujFv/UJUBQA8Bxsvb74wLhvbwBoyMk3D/JVrNlYUMxztAOA2tRHHEtznr0NvZyiqMa8QnoXl9JxWTr1UrEqtoz1JwStAAADeklEQVSQKmDdQuiJQFPr5PoqVWyZkkotBwe5LZsvKqvQMzHmWFswmEwAkDYKm8ur9P/pRVFyeWNugW3EUAAQ19TqmZkotiBIz5YJRaqYlAEAFmxusKm1KraMkCrgvAyEnoiwVv4UcxrHykKQkcM2NTbu25tra8X453iphpx8AOA729M3RcVl8maxvrMDAOgaGYpKy6l/pqfnHjoJAPqqqVs2XH1TNkcVW0ZIFbBuIfREVXOTqWqm1dmMDmvIzk9a81Xx2ejcQyezfv6dXt6YUwAAiv4WXcb0ne0BwHJgoLiq5sFX28qv3b7/xdbya7cAgO+s/MmEAFAkalTFZhFSERwnROgJW46+LpOlii3bTxglrq0vPhddHZ/CtbF0XTCdXt6Qk6/D1+dYmNE36TJGFyeHaeOaqmpKL1wrv37HYlCQZVhI3f00HX2VnM7DnW+sis0ipCJ43UiE/nW6JHtndirpFF3KmWv4rWeooS6bdBCEOgv7Wwj9a4KN6/366mttn+2p/PqdB19te3Y5k60rF7d+ro2A3RuVOL6Xsfu3wlNRzxVg4NE9uob8tja42KUvFi2kWbC/hdB/bM1MOluW19a7QiZqEtfWP7tcLpEwdXVbfYiehSlTR2lfECX1AmljK5cdaScAx8qc0caZc0112CvcfAbgZEKkUbC/hdB/zLTvFVNVUtfG8UwsLofLJTn1TtfQgD4wWSn6GJph0UIaB/tbCLXUKJW8kxqTI2ylX6VN5jj0nufoTjoFQs8N58Ej1JK+ju4sezdDndaH3bSDM48/0lIlR4MhpGpYtxBqRZiF/TgrFz2Gdr5BLNjcb/qGqvlFMhFqC44TItSmW1UlR4oyHgpqSAdRpnmO7oPNbB15SttJhlAXw7qFUHtkFDU/4WKDVCzU/CsCm+ty7Hn8b/qFkg6C0EvBuoVQB6qaRadKchpkkr9Lc0lneUEGOrpLXPqJpNKJtq6ksyD0srBuIdRZlysKDuan6TCZhaIG2X/fOBQwGECpz880Y122oQ6bw9R5r5ePMw+vDIm0BNYthJ5PkajBjsuPKsu7U13mom/oZWR+pbLoXm35UAsHb/X4+VplUYNUEmnj4mloVtEsstDjkn7NEFImrFsIIYQ0iXZO80UIIaStsG4hhBDSJFi3EEIIaRKsWwghhDQJ1i2EEEKaBOsWQgghTfL/F52WllwNdsoAAAAASUVORK5CYII=",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.constants import START, END\n",
        "from langchain_opentutorial.graphs import visualize_graph\n",
        "\n",
        "# Create graph\n",
        "builder = StateGraph(ResearchGraphState)\n",
        "\n",
        "# Define nodes\n",
        "builder.add_node(\"create_analysts\", create_analysts)\n",
        "builder.add_node(\"human_feedback\", human_feedback)\n",
        "builder.add_node(\"conduct_interview\", interview_builder.compile())\n",
        "builder.add_node(\"write_report\", write_report)\n",
        "builder.add_node(\"write_introduction\", write_introduction)\n",
        "builder.add_node(\"write_conclusion\", write_conclusion)\n",
        "builder.add_node(\"finalize_report\", finalize_report)\n",
        "\n",
        "# Define edges\n",
        "builder.add_edge(START, \"create_analysts\")\n",
        "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
        "builder.add_conditional_edges(\n",
        "    \"human_feedback\", initiate_all_interviews, [\"create_analysts\", \"conduct_interview\"]\n",
        ")\n",
        "\n",
        "# Report generation from interviews\n",
        "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
        "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
        "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
        "\n",
        "# Final report assembly\n",
        "builder.add_edge(\n",
        "    [\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\"\n",
        ")\n",
        "builder.add_edge(\"finalize_report\", END)\n",
        "\n",
        "# Compile graph\n",
        "memory = MemorySaver()\n",
        "graph = builder.compile(interrupt_before=[\"human_feedback\"], checkpointer=memory)\n",
        "\n",
        "# Visualize the workflow\n",
        "visualize_graph(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4444392d",
      "metadata": {},
      "source": [
        "The graph structure implements:\n",
        "\n",
        "**Core Workflow Stages**\n",
        "- Analyst Creation\n",
        "- Human Feedback Integration\n",
        "- Parallel Interview Execution\n",
        "- Report Generation\n",
        "- Final Assembly\n",
        "\n",
        "**Key Components**\n",
        "- State Management using ResearchGraphState\n",
        "- Memory persistence with MemorySaver\n",
        "- Conditional routing based on human feedback\n",
        "- Parallel processing of interviews\n",
        "- Synchronized report assembly\n",
        "\n",
        "**Flow Control**\n",
        "- Starts with analyst creation\n",
        "- Allows for human feedback and iteration\n",
        "- Conducts parallel interviews\n",
        "- Generates report components simultaneously\n",
        "- Assembles final report with all components\n",
        "\n",
        "This implementation creates a robust workflow for automated research with human oversight and parallel processing capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbd98ba7",
      "metadata": {},
      "source": [
        "### Executing the Report Writing  Graph\n",
        "\n",
        " Here's how to run the graph with the specified parameters: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "32489294",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "affiliation='Tech University' name='Dr. Emily Chen' role='AI Researcher' description='Dr. Chen focuses on the theoretical underpinnings of Modular and Naive RAGs. She is interested in the scalability and flexibility of Modular RAGs compared to traditional methods, with a focus on how these differences impact real-world applications.'\n",
            "affiliation='Data Solutions Corp' name='Raj Patel' role='Industry Expert' description='Raj Patel is a seasoned industry expert who provides insights into the practical benefits of implementing Modular RAG in production environments. His focus is on cost-efficiency, performance improvements, and integration challenges compared to Naive RAG systems.'\n",
            "affiliation='AI Ethics Council' name='Sofia Martinez' role='Ethicist' description='Sofia Martinez examines the ethical implications and societal impacts of deploying Modular RAG systems. Her work emphasizes the importance of transparency, fairness, and accountability in AI systems, especially when transitioning from traditional models to more modular ones.'\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Set input parameters\n",
        "max_analysts = 3\n",
        "topic = \"Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.\"\n",
        "\n",
        "# Configure execution settings\n",
        "config = RunnableConfig(\n",
        "    recursion_limit=30,\n",
        "    configurable={\"thread_id\": random_uuid()},\n",
        ")\n",
        "\n",
        "# Prepare input data\n",
        "inputs = {\"topic\": topic, \"max_analysts\": max_analysts}\n",
        "\n",
        "# Execute graph until first breakpoint\n",
        "invoke_graph(graph, inputs, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2794ed52",
      "metadata": {},
      "source": [
        "Let's add `human_feedback` to customize the analyst team and continue the graph execution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "43f717a3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '4e5b3dcb-b241-4d1e-b32b-4911b4afde31',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1efd9c7b-89f6-6c47-8002-703349137f1e'}}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Add new analyst with human feedback\n",
        "graph.update_state(\n",
        "    config,\n",
        "    {\"human_analyst_feedback\": \"Add Prof. Jeffrey Hinton as a head of AI analyst\"},\n",
        "    as_node=\"human_feedback\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "af4526f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "affiliation='University of Toronto' name='Prof. Jeffrey Hinton' role='Head of AI Analyst' description='As a pioneer in the field of artificial intelligence, Prof. Hinton focuses on the theoretical underpinnings of AI systems, with a particular interest in how modular RAG can improve the scalability and efficiency of AI models in production environments. His concerns revolve around the sustainability and adaptability of AI systems in real-world applications.'\n",
            "affiliation='Stanford University' name='Dr. Alice Nguyen' role='AI Systems Analyst' description='Dr. Nguyen specializes in the architecture and design of AI systems, focusing on the structural differences between Modular RAG and traditional Naive RAG. She is particularly interested in the modularity aspect and how it enhances system flexibility and maintenance at a production level.'\n",
            "affiliation='Massachusetts Institute of Technology' name='Dr. Rahul Mehta' role='Production AI Specialist' description=\"Dr. Mehta's expertise lies in deploying AI solutions in production. He emphasizes the practical benefits of Modular RAG, including its ability to reduce computational overhead and improve processing speeds, making AI systems more viable for large-scale deployment.\"\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Continue graph execution\n",
        "invoke_graph(graph, None, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f53e4c9",
      "metadata": {},
      "source": [
        "Let's complete the human feedback phase and resume the graph execution: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "05725a6d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '4e5b3dcb-b241-4d1e-b32b-4911b4afde31',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1efd9c7b-b98c-68ab-8004-83aa6162d821'}}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# End human feedback phase\n",
        "graph.update_state(config, {\"human_analyst_feedback\": None}, as_node=\"human_feedback\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "1e2f4ee1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello, my name is Alex Carter, and I am an AI research journalist. Thank you for taking the time to speak with me, Prof. Hinton. To start, could you explain how Modular RAG differs from traditional Naive RAG in the context of AI systems? What are the core distinctions that make Modular RAG more suitable for production environments?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Dr. Mehta, my name is Alex Carter, and I'm an analyst delving into the practical applications of AI. I am particularly interested in understanding how Modular RAG differs from traditional Naive RAG and the benefits it brings when deployed at the production level. Could you explain these differences and benefits, perhaps with some specific examples?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Dr. Nguyen, my name is Jamie Carter, and I'm an analyst eager to delve into the world of AI systems architecture. I'm particularly interested in understanding the structural differences between Modular RAG and traditional Naive RAG, especially from the perspective of modularity and its benefits in a production environment. Could you start by explaining what sets Modular RAG apart from Naive RAG in terms of architecture?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
            "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
            "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
            "Retrieval-augmented generation (RAG) has emerged as a powerful technique that combines the strengths of information retrieval and natural language generation. However, not all RAG implementations are created equal. The traditional or \"Naive\" RAG, while groundbreaking, often struggles with limitations such as inflexibility and inefficiencies in handling diverse and dynamic datasets.\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
            "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
            "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
            "Retrieval-augmented generation (RAG) has emerged as a powerful technique that combines the strengths of information retrieval and natural language generation. However, not all RAG implementations are created equal. The traditional or \"Naive\" RAG, while groundbreaking, often struggles with limitations such as inflexibility and inefficiencies in handling diverse and dynamic datasets.\n",
            "</Document>\n",
            "==================================================\n",
            "MuPDF error: syntax error: expected object number\n",
            "\n",
            "MuPDF error: syntax error: no XObject subtype specified\n",
            "\n",
            "ArXiv search error: [WinError 32] 다른 프로세스가 파일을 사용 중이기 때문에 프로세스가 액세스 할 수 없습니다: './2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks.pdf'\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Error>Failed to retrieve ArXiv search results.</Error>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
            "Learn about the key differences between Modular and Naive RAG, case study, and the significant advantages of Modular RAG. ... Let's start with an overview of Navie and Modular RAG followed by limitations and benefits. Overview of Naive RAG and Modular RAG. ... The rigid architecture of Naive RAG makes it challenging to incorporate custom\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
            "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://www.marktechpost.com/2024/04/01/evolution-of-rags-naive-rag-advanced-rag-and-modular-rag-architectures/\"/>\n",
            "The shift towards a modular RAG approach is becoming prevalent, supporting sequential processing and integrated end-to-end training across its components. Despite its distinctiveness, Modular RAG builds upon the foundational principles of Advanced and Naive RAG, illustrating a progression and refinement within the RAG family.\n",
            "</Document>\n",
            "==================================================\n",
            "ArXiv search error: [WinError 32] 다른 프로세스가 파일을 사용 중이기 때문에 프로세스가 액세스 할 수 없습니다: './2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks.pdf'\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Error>Failed to retrieve ArXiv search results.</Error>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
            "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
            "increasing demands of application scenarios have driven the evolution of RAG,\n",
            "leading to the integration of advanced retrievers, LLMs and other complementary\n",
            "technologies, which in turn has amplified the intricacy of RAG systems.\n",
            "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process of\n",
            "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
            "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
            "decomposing complex RAG systems into independent modules and specialized\n",
            "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a more advanced\n",
            "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
            "extensive research, this paper further identifies prevalent RAG\n",
            "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
            "analysis of their respective implementation nuances. Modular RAG presents\n",
            "innovative opportunities for the conceptualization and deployment of RAG\n",
            "systems. Finally, the paper explores the potential emergence of new operators\n",
            "and paradigms, establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment of RAG\n",
            "technologies.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2406.00944v2\" date=\"2024-10-17\" authors=\"Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng\"/>\n",
            "<Title>\n",
            "A Theory for Token-Level Harmonization in Retrieval-Augmented Generation\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\n",
            "large language models (LLMs). Studies show that while RAG provides valuable\n",
            "external information (benefit), it may also mislead LLMs (detriment) with noisy\n",
            "or incorrect retrieved texts. Although many existing methods attempt to\n",
            "preserve benefit and avoid detriment, they lack a theoretical explanation for\n",
            "RAG. The benefit and detriment in the next token prediction of RAG remain a\n",
            "black box that cannot be quantified or compared in an explainable manner, so\n",
            "existing methods are data-driven, need additional utility evaluators or\n",
            "post-hoc. This paper takes the first step towards providing a theory to explain\n",
            "and trade off the benefit and detriment in RAG. First, we model RAG as the\n",
            "fusion between distribution of LLMs knowledge and distribution of retrieved\n",
            "texts. Then, we formalize the trade-off between the value of external knowledge\n",
            "(benefit) and its potential risk of misleading LLMs (detriment) in next token\n",
            "prediction of RAG by distribution difference in this fusion. Finally, we prove\n",
            "that the actual effect of RAG on the token, which is the comparison between\n",
            "benefit and detriment, can be predicted without any training or accessing the\n",
            "utility of retrieval. Based on our theory, we propose a practical novel method,\n",
            "Tok-RAG, which achieves collaborative generation between the pure LLM and RAG\n",
            "at token level to preserve benefit and avoid detriment. Experiments in\n",
            "real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
            "effectiveness of our method and support our theoretical findings.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "A THEORY FOR TOKEN-LEVEL HARMONIZATION IN\n",
            "RETRIEVAL-AUGMENTED GENERATION\n",
            "Shicheng Xu\n",
            "Liang Pang∗Huawei Shen\n",
            "Xueqi Cheng\n",
            "CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS\n",
            "{xushicheng21s,pangliang,shenhuawei,cxq}@ict.ac.cn\n",
            "ABSTRACT\n",
            "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance large\n",
            "language models (LLMs). Studies show that while RAG provides valuable external\n",
            "information (benefit), it may also mislead LLMs (detriment) with noisy or incorrect\n",
            "retrieved texts. Although many existing methods attempt to preserve benefit and\n",
            "avoid detriment, they lack a theoretical explanation for RAG. The benefit and\n",
            "detriment in the next token prediction of RAG remain a ’black box’ that cannot\n",
            "be quantified or compared in an explainable manner, so existing methods are data-\n",
            "driven, need additional utility evaluators or post-hoc. This paper takes the first step\n",
            "towards providing a theory to explain and trade off the benefit and detriment in\n",
            "RAG. First, we model RAG as the fusion between distribution of LLM’s knowledge\n",
            "and distribution of retrieved texts. Then, we formalize the trade-off between the\n",
            "value of external knowledge (benefit) and its potential risk of misleading LLMs\n",
            "(detriment) in next token prediction of RAG by distribution difference in this\n",
            "fusion. Finally, we prove that the actual effect of RAG on the token, which is the\n",
            "comparison between benefit and detriment, can be predicted without any training or\n",
            "accessing the utility of retrieval. Based on our theory, we propose a practical novel\n",
            "method, Tok-RAG, which achieves collaborative generation between the pure\n",
            "LLM and RAG at token level to preserve benefit and avoid detriment. Experiments\n",
            "in real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
            "effectiveness of our method and support our theoretical findings.\n",
            "1\n",
            "INTRODUCTION\n",
            "Retrieval-augmented generation (RAG) has shown promising performance in enhancing Large\n",
            "Language Models (LLMs) by integrating retrieved texts (Xu et al., 2023; Shi et al., 2023; Asai et al.,\n",
            "2023; Ram et al., 2023). Studies indicate that while RAG provides LLMs with valuable additional\n",
            "knowledge (benefit), it also poses a risk of misleading them (detriment) due to noisy or incorrect\n",
            "retrieved texts (Ram et al., 2023; Xu et al., 2024b;a; Jin et al., 2024a; Xie et al., 2023; Jin et al.,\n",
            "2024b). Existing methods attempt to preserve benefit and avoid detriment by adding utility evaluators\n",
            "for retrieval, prompt engineering, or fine-tuning LLMs (Asai et al., 2023; Ding et al., 2024; Xu et al.,\n",
            "2024b; Yoran et al., 2024; Ren et al., 2023; Feng et al., 2023; Mallen et al., 2022; Jiang et al., 2023).\n",
            "However, existing methods are data-driven, need evaluator for utility of retrieved texts or post-hoc. A\n",
            "theory-based method, focusing on core principles of RAG is urgently needed, which is crucial for\n",
            "consistent and reliable improvements without relying on additional training or utility evaluators and\n",
            "improving our understanding for RAG.\n",
            "This paper takes the first step in providing a theoretical framework to explain and trade off the benefit\n",
            "and detriment at token level in RAG and proposes a novel method to preserve benefit and avoid\n",
            "detriment based on our theoretical findings. Specifically, this paper pioneers in modeling next token\n",
            "prediction in RAG as the fusion between the distribution of LLM’s knowledge and the distribution\n",
            "of retrieved texts as shown in Figure 1. Our theoretical derivation based on this formalizes the core\n",
            "of this fusion as the subtraction between two terms measured by the distribution difference: one is\n",
            "distribution completion and the other is distribution contradiction. Further analysis indicates that\n",
            "the distribution completion measures how much out-of-distribution knowledge that retrieved texts\n",
            "∗Corresponding author\n",
            "1\n",
            "arXiv:2406.00944v2  [cs.CL]  17 Oct 2024\n",
            "Query\n",
            "Wole\n",
            "Query\n",
            "Ernst\n",
            "Soyinka\n",
            "…\n",
            "LLM’s \n",
            "Distribution\n",
            "Retrieved \n",
            "Distribution \n",
            "Fusion\n",
            "Distribution\n",
            "Difference\n",
            "Olanipekun\n",
            "LLM’s \n",
            "Dis\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2409.11598v2\" date=\"2024-12-03\" authors=\"To Eun Kim, Fernando Diaz\"/>\n",
            "<Title>\n",
            "Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Many language models now enhance their responses with retrieval capabilities,\n",
            "leading to the widespread adoption of retrieval-augmented generation (RAG)\n",
            "systems. However, despite retrieval being a core component of RAG, much of the\n",
            "research in this area overlooks the extensive body of work on fair ranking,\n",
            "neglecting the importance of considering all stakeholders involved. This paper\n",
            "presents the first systematic evaluation of RAG systems integrated with fair\n",
            "rankings. We focus specifically on measuring the fair exposure of each relevant\n",
            "item across the rankings utilized by RAG systems (i.e., item-side fairness),\n",
            "aiming to promote equitable growth for relevant item providers. To gain a deep\n",
            "understanding of the relationship between item-fairness, ranking quality, and\n",
            "generation quality in the context of RAG, we analyze nine different RAG systems\n",
            "that incorporate fair rankings across seven distinct datasets. Our findings\n",
            "indicate that RAG systems with fair rankings can maintain a high level of\n",
            "generation quality and, in many cases, even outperform traditional RAG systems,\n",
            "despite the general trend of a tradeoff between ensuring fairness and\n",
            "maintaining system-effectiveness. We believe our insights lay the groundwork\n",
            "for responsible and equitable RAG systems and open new avenues for future\n",
            "research. We publicly release our codebase and dataset at\n",
            "https://github.com/kimdanny/Fair-RAG.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "Towards Fair RAG: On the Impact of Fair Ranking\n",
            "in Retrieval-Augmented Generation\n",
            "To Eun Kim\n",
            "Carnegie Mellon University\n",
            "toeunk@cs.cmu.edu\n",
            "Fernando Diaz\n",
            "Carnegie Mellon University\n",
            "diazf@acm.org\n",
            "Abstract\n",
            "Many language models now enhance their responses with retrieval capabilities,\n",
            "leading to the widespread adoption of retrieval-augmented generation (RAG) systems.\n",
            "However, despite retrieval being a core component of RAG, much of the research\n",
            "in this area overlooks the extensive body of work on fair ranking, neglecting the\n",
            "importance of considering all stakeholders involved. This paper presents the first\n",
            "systematic evaluation of RAG systems integrated with fair rankings. We focus\n",
            "specifically on measuring the fair exposure of each relevant item across the rankings\n",
            "utilized by RAG systems (i.e., item-side fairness), aiming to promote equitable\n",
            "growth for relevant item providers. To gain a deep understanding of the relationship\n",
            "between item-fairness, ranking quality, and generation quality in the context of RAG,\n",
            "we analyze nine different RAG systems that incorporate fair rankings across seven\n",
            "distinct datasets. Our findings indicate that RAG systems with fair rankings can\n",
            "maintain a high level of generation quality and, in many cases, even outperform\n",
            "traditional RAG systems, despite the general trend of a tradeoff between ensuring\n",
            "fairness and maintaining system-effectiveness. We believe our insights lay the\n",
            "groundwork for responsible and equitable RAG systems and open new avenues for\n",
            "future research. We publicly release our codebase and dataset. 1\n",
            "1\n",
            "Introduction\n",
            "In recent years, the concept of fair ranking has emerged as a critical concern in modern information\n",
            "access systems [12]. However, despite its significance, fair ranking has yet to be thoroughly examined\n",
            "in the context of retrieval-augmented generation (RAG) [1, 29], a rapidly advancing trend in natural\n",
            "language processing (NLP) systems [27]. To understand why this is important, consider the RAG\n",
            "system in Figure 1, where a user asks a question about running shoes. A classic retrieval system\n",
            "might return several documents containing information from various running shoe companies. If the\n",
            "RAG system only selects the top two documents, then information from the remaining two relevant\n",
            "companies will not be relayed to the predictive model and will likely be omitted from its answer.\n",
            "The fair ranking literature refers to this situation as unfair because some relevant companies (i.e., in\n",
            "documents at position 3 and 4) receive less or no exposure compared to equally relevant company in\n",
            "the top position [12].\n",
            "Understanding the effect of fair ranking in RAG is fundamental to ensuring responsible and equitable\n",
            "NLP systems. Since retrieval results in RAG often underlie response attribution [15], unfair exposure\n",
            "of content to the RAG system can result in incomplete evidence in responses (thus compromising recall\n",
            "of potentially relevant information for users) or downstream representational harms (thus creating\n",
            "or reinforcing biases across the set of relevant entities). In situations where content providers are\n",
            "compensated for contributions to inference, there can be financial implications for the unfairness\n",
            "[4, 19, 31]. Indeed, the fair ranking literature indicates that these are precisely the harms that emerge\n",
            "1https://github.com/kimdanny/Fair-RAG\n",
            "arXiv:2409.11598v2  [cs.IR]  3 Dec 2024\n",
            "What are the best running shoes \n",
            "to buy for marathons?\n",
            "𝒅𝟏\n",
            "𝒅𝟐\n",
            "top-k \n",
            "truncation\n",
            "Here are some \n",
            "best options from \n",
            "company A and \n",
            "company B\n",
            "LM\n",
            "Corpus\n",
            "𝒅𝟏 (Company A)\n",
            "𝒅𝟐 (Company B)\n",
            "𝒅𝟑 (Company C)\n",
            "𝒅𝟒 (Company D)\n",
            "𝒅𝟓 (Company B)\n",
            "…\n",
            "Rel\n",
            "Non-Rel\n",
            "Rel\n",
            "Rel\n",
            "Company C and D\n",
            "We are also \n",
            "relevant!\n",
            "🤖\n",
            "🙁\n",
            "🧑💻\n",
            "Non-Rel\n",
            "Figure 1: Fairness concerns in RAG. A simplified example of how RAG models can ignore equally\n",
            "relevant items (d3 and d4) and always consume the fixed top-scoring items (d1 and d2) with the same\n",
            "order of ranking over the multiple user requests. This is due to the deterministic nature of the retrieval\n",
            "process and \n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly, Dr. Mehta. Modular RAG, or Retrieval-Augmented Generation, represents an advancement over the traditional Naive RAG by addressing some of its inherent limitations, such as inflexibility and inefficiencies in handling diverse and dynamic datasets [3].\n",
            "\n",
            "**Differences:**\n",
            "\n",
            "1. **Flexibility and Efficiency:** Naive RAG systems are often limited in their ability to efficiently process diverse datasets due to a more rigid architecture. Modular RAG improves upon this by introducing a more flexible framework that can efficiently handle a wider variety of data inputs and adjust to dynamic data changes [3].\n",
            "\n",
            "2. **Embedding Techniques:** Advanced RAG models, including Modular RAG, often employ fine-tuned embeddings. These embeddings capture task-specific semantics or domain knowledge, which improves the quality of both the retrieved information and the generated responses. Modular RAG can adaptively adjust these embeddings during inference based on the context, leading to more relevant outputs [1].\n",
            "\n",
            "**Benefits at the Production Level:**\n",
            "\n",
            "1. **Reduced Computational Overhead:** By optimizing the retrieval and generation processes, Modular RAG can reduce computational overhead, making the system more efficient and cost-effective for large-scale deployment. This is particularly important in production environments where resource efficiency is crucial [1][2].\n",
            "\n",
            "2. **Improved Processing Speeds:** Modular RAG systems can achieve faster processing speeds due to their ability to handle data more efficiently and adapt to changes dynamically. This is advantageous in production settings where quick response times are essential [2].\n",
            "\n",
            "3. **Enhanced Accuracy and Context-Awareness:** By integrating retrieval into the generation process and using sophisticated embedding techniques, Modular RAG can provide more accurate and context-aware outputs. This makes it particularly effective for applications that require current or specialized knowledge, such as real-time customer support or complex data analysis [2].\n",
            "\n",
            "In summary, the transition from Naive to Modular RAG in production environments enhances flexibility, efficiency, and accuracy, thereby making AI systems more viable for large-scale deployment.\n",
            "\n",
            "**Sources:**\n",
            "\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "\n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "\n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Thank you, Dr. Mehta, for the detailed explanation. Could you provide a real-world example where Modular RAG has been successfully deployed, highlighting the specific benefits it brought to that deployment?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly, Prof. Hinton. Modular RAG differs from traditional Naive RAG in several key ways that enhance its effectiveness in production environments.\n",
            "\n",
            "1. **Flexibility and Adaptability**: Naive RAG, while foundational, often struggles with inflexibility and inefficiencies when dealing with diverse and dynamic datasets. Modular RAG, on the other hand, improves upon this by allowing for more adaptable and flexible integration of retrieval and generation components. This modularity facilitates the customization of each component to better handle various data types and query contexts [3].\n",
            "\n",
            "2. **Dynamic Embedding Techniques**: Modular RAG employs dynamic embedding techniques that enable the system to adjust embeddings during inference based on the specific context of the query or the retrieved information. This results in more contextually relevant and coherent responses, which is a significant improvement over the static nature of embeddings in Naive RAG [1].\n",
            "\n",
            "3. **Integration with Vector Databases**: Another benefit of Modular RAG is the integration with vector databases, which provides the necessary infrastructure for storing and retrieving high-dimensional embeddings of contextual information. This integration allows for precise similarity searches, leading to more accurate and context-aware outputs. This capability is crucial for applications that require current or specialized knowledge [2].\n",
            "\n",
            "Overall, the modular nature of this approach allows for greater scalability and efficiency in AI models, making it particularly suited for production environments where adaptability and performance are essential.\n",
            "\n",
            "Sources:\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Thank you for that detailed explanation, Prof. Hinton. Could you provide a specific example of how Modular RAG has been successfully implemented in a real-world application, highlighting its scalability and efficiency benefits?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372\"/>\n",
            "These sub-modules allow for more granular control over the RAG process, enabling the system to fine-tune its operations based on the specific requirements of the task​. This pattern closely resembles the traditional RAG process but benefits from the modular approach by allowing individual modules to be optimized or replaced without altering the overall flow. For example, a linear RAG flow might begin with a query expansion module to refine the user’s input, followed by the retrieval module, which fetches the most relevant data chunks. The Modular RAG framework embraces this need through various tuning patterns that enhance the system’s ability to adapt to specific tasks and datasets. Managing these different data types and ensuring seamless integration within the RAG system can be difficult, requiring sophisticated data processing and retrieval strategies​(modular rag paper).\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://medium.com/@xanoweg441/building-a-production-ready-rag-application-a-practical-guide-d43a29908fe4\"/>\n",
            "With advancements in retrieval techniques, indexing methods, and fine-tuning models like RAFT (Retrieval-Augmented Fine-Tuning), building modular, configurable RAG applications is key to handling real-world, large-scale workloads. We’ll break down the architecture into three main pipelines: Indexing, Retrieval, and Generation, and explore some AWS tools that make RAG applications easier to implement. AWS offers vector databases like Amazon OpenSearch, Pinecone, and PostgreSQL with vector support for building scalable knowledge bases. Amazon Bedrock: A fully managed service for generative AI, Amazon Bedrock supports integration with vector databases like OpenSearch and offers pre-built models for text generation and embeddings. These libraries allow you to easily build modular RAG systems and automate key tasks like data loading, vectorization, and retrieval.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://medium.com/aingineer/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e\"/>\n",
            "A Comprehensive Guide to Implementing Modular RAG for Scalable AI Systems | by Gaurav Nigam | aingineer | Dec, 2024 | Medium A Comprehensive Guide to Implementing Modular RAG for Scalable AI Systems In the rapidly evolving landscape of AI, Modular RAG (Retrieval-Augmented Generation) has emerged as a transformative approach to building robust, scalable, and adaptable AI systems. By decoupling retrieval, reasoning, and generation into independent modules, Modular RAG empowers engineering leaders, architects, and senior engineers to design systems that are not only efficient but also flexible enough to meet the dynamic demands of modern enterprises. This guide aims to provide an in-depth exploration of Modular RAG, from its foundational principles to practical implementation strategies, tailored for professionals with a keen interest in scaling enterprise AI systems.\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
            "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
            "increasing demands of application scenarios have driven the evolution of RAG,\n",
            "leading to the integration of advanced retrievers, LLMs and other complementary\n",
            "technologies, which in turn has amplified the intricacy of RAG systems.\n",
            "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process of\n",
            "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
            "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
            "decomposing complex RAG systems into independent modules and specialized\n",
            "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a more advanced\n",
            "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
            "extensive research, this paper further identifies prevalent RAG\n",
            "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
            "analysis of their respective implementation nuances. Modular RAG presents\n",
            "innovative opportunities for the conceptualization and deployment of RAG\n",
            "systems. Finally, the paper explores the potential emergence of new operators\n",
            "and paradigms, establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment of RAG\n",
            "technologies.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2405.13576v1\" date=\"2024-05-22\" authors=\"Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou\"/>\n",
            "<Title>\n",
            "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "With the advent of Large Language Models (LLMs), the potential of Retrieval\n",
            "Augmented Generation (RAG) techniques have garnered considerable research\n",
            "attention. Numerous novel algorithms and models have been introduced to enhance\n",
            "various aspects of RAG systems. However, the absence of a standardized\n",
            "framework for implementation, coupled with the inherently intricate RAG\n",
            "process, makes it challenging and time-consuming for researchers to compare and\n",
            "evaluate these approaches in a consistent environment. Existing RAG toolkits\n",
            "like LangChain and LlamaIndex, while available, are often heavy and unwieldy,\n",
            "failing to meet the personalized needs of researchers. In response to this\n",
            "challenge, we propose FlashRAG, an efficient and modular open-source toolkit\n",
            "designed to assist researchers in reproducing existing RAG methods and in\n",
            "developing their own RAG algorithms within a unified framework. Our toolkit\n",
            "implements 12 advanced RAG methods and has gathered and organized 32 benchmark\n",
            "datasets. Our toolkit has various features, including customizable modular\n",
            "framework, rich collection of pre-implemented RAG works, comprehensive\n",
            "datasets, efficient auxiliary pre-processing scripts, and extensive and\n",
            "standard evaluation metrics. Our toolkit and resources are available at\n",
            "https://github.com/RUC-NLPIR/FlashRAG.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "FlashRAG: A Modular Toolkit for Efficient\n",
            "Retrieval-Augmented Generation Research\n",
            "Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou∗\n",
            "Gaoling School of Artificial Intelligence\n",
            "Renmin University of China\n",
            "{jinjiajie, dou}@ruc.edu.cn, yutaozhu94@gmail.com\n",
            "Abstract\n",
            "With the advent of Large Language Models (LLMs), the potential of Retrieval\n",
            "Augmented Generation (RAG) techniques have garnered considerable research\n",
            "attention. Numerous novel algorithms and models have been introduced to enhance\n",
            "various aspects of RAG systems. However, the absence of a standardized framework\n",
            "for implementation, coupled with the inherently intricate RAG process, makes it\n",
            "challenging and time-consuming for researchers to compare and evaluate these\n",
            "approaches in a consistent environment. Existing RAG toolkits like LangChain\n",
            "and LlamaIndex, while available, are often heavy and unwieldy, failing to\n",
            "meet the personalized needs of researchers. In response to this challenge, we\n",
            "propose FlashRAG, an efficient and modular open-source toolkit designed to assist\n",
            "researchers in reproducing existing RAG methods and in developing their own\n",
            "RAG algorithms within a unified framework. Our toolkit implements 12 advanced\n",
            "RAG methods and has gathered and organized 32 benchmark datasets. Our toolkit\n",
            "has various features, including customizable modular framework, rich collection\n",
            "of pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-\n",
            "processing scripts, and extensive and standard evaluation metrics. Our toolkit and\n",
            "resources are available at https://github.com/RUC-NLPIR/FlashRAG.\n",
            "1\n",
            "Introduction\n",
            "In the era of large language models (LLMs), retrieval-augmented generation (RAG) [1, 2] has\n",
            "emerged as a robust solution to mitigate hallucination issues in LLMs by leveraging external\n",
            "knowledge bases [3]. The substantial applications and the potential of RAG technology have\n",
            "attracted considerable research attention. With the introduction of a large number of new algorithms\n",
            "and models to improve various facets of RAG systems in recent years, comparing and evaluating\n",
            "these methods under a consistent setting has become increasingly challenging.\n",
            "Many works are not open-source or have fixed settings in their open-source code, making it difficult\n",
            "to adapt to new data or innovative components. Besides, the datasets and retrieval corpus used often\n",
            "vary, with resources being scattered, which can lead researchers to spend excessive time on pre-\n",
            "processing steps instead of focusing on optimizing their methods. Furthermore, due to the complexity\n",
            "of RAG systems, involving multiple steps such as indexing, retrieval, and generation, researchers\n",
            "often need to implement many parts of the system themselves. Although there are some existing RAG\n",
            "toolkits like LangChain [4] and LlamaIndex [5], they are typically large and cumbersome, hindering\n",
            "researchers from implementing customized processes and failing to address the aforementioned issues.\n",
            "∗Corresponding author\n",
            "Preprint. Under review.\n",
            "arXiv:2405.13576v1  [cs.CL]  22 May 2024\n",
            "Thus, a unified, researcher-oriented RAG toolkit is urgently needed to streamline methodological\n",
            "development and comparative studies.\n",
            "To address the issue mentioned above, we introduce FlashRAG, an open-source library designed to\n",
            "enable researchers to easily reproduce existing RAG methods and develop their own RAG algorithms.\n",
            "This library allows researchers to utilize built pipelines to replicate existing work, employ provided\n",
            "RAG components to construct their own RAG processes, or simply use organized datasets and corpora\n",
            "to accelerate their own RAG workflow. Compared to existing RAG toolkits, FlashRAG is more suited\n",
            "for researchers. To summarize, the key features and capabilities of our FlashRAG library can be\n",
            "outlined in the following four aspects:\n",
            "Extensive and Customizable Modular RAG Framework.\n",
            "To facilitate an easily expandable\n",
            "RAG process, we implemented modular RAG at two levels. At the component level, we offer\n",
            "comprehensive RAG compon\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2406.00944v2\" date=\"2024-10-17\" authors=\"Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng\"/>\n",
            "<Title>\n",
            "A Theory for Token-Level Harmonization in Retrieval-Augmented Generation\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\n",
            "large language models (LLMs). Studies show that while RAG provides valuable\n",
            "external information (benefit), it may also mislead LLMs (detriment) with noisy\n",
            "or incorrect retrieved texts. Although many existing methods attempt to\n",
            "preserve benefit and avoid detriment, they lack a theoretical explanation for\n",
            "RAG. The benefit and detriment in the next token prediction of RAG remain a\n",
            "black box that cannot be quantified or compared in an explainable manner, so\n",
            "existing methods are data-driven, need additional utility evaluators or\n",
            "post-hoc. This paper takes the first step towards providing a theory to explain\n",
            "and trade off the benefit and detriment in RAG. First, we model RAG as the\n",
            "fusion between distribution of LLMs knowledge and distribution of retrieved\n",
            "texts. Then, we formalize the trade-off between the value of external knowledge\n",
            "(benefit) and its potential risk of misleading LLMs (detriment) in next token\n",
            "prediction of RAG by distribution difference in this fusion. Finally, we prove\n",
            "that the actual effect of RAG on the token, which is the comparison between\n",
            "benefit and detriment, can be predicted without any training or accessing the\n",
            "utility of retrieval. Based on our theory, we propose a practical novel method,\n",
            "Tok-RAG, which achieves collaborative generation between the pure LLM and RAG\n",
            "at token level to preserve benefit and avoid detriment. Experiments in\n",
            "real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
            "effectiveness of our method and support our theoretical findings.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "A THEORY FOR TOKEN-LEVEL HARMONIZATION IN\n",
            "RETRIEVAL-AUGMENTED GENERATION\n",
            "Shicheng Xu\n",
            "Liang Pang∗Huawei Shen\n",
            "Xueqi Cheng\n",
            "CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS\n",
            "{xushicheng21s,pangliang,shenhuawei,cxq}@ict.ac.cn\n",
            "ABSTRACT\n",
            "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance large\n",
            "language models (LLMs). Studies show that while RAG provides valuable external\n",
            "information (benefit), it may also mislead LLMs (detriment) with noisy or incorrect\n",
            "retrieved texts. Although many existing methods attempt to preserve benefit and\n",
            "avoid detriment, they lack a theoretical explanation for RAG. The benefit and\n",
            "detriment in the next token prediction of RAG remain a ’black box’ that cannot\n",
            "be quantified or compared in an explainable manner, so existing methods are data-\n",
            "driven, need additional utility evaluators or post-hoc. This paper takes the first step\n",
            "towards providing a theory to explain and trade off the benefit and detriment in\n",
            "RAG. First, we model RAG as the fusion between distribution of LLM’s knowledge\n",
            "and distribution of retrieved texts. Then, we formalize the trade-off between the\n",
            "value of external knowledge (benefit) and its potential risk of misleading LLMs\n",
            "(detriment) in next token prediction of RAG by distribution difference in this\n",
            "fusion. Finally, we prove that the actual effect of RAG on the token, which is the\n",
            "comparison between benefit and detriment, can be predicted without any training or\n",
            "accessing the utility of retrieval. Based on our theory, we propose a practical novel\n",
            "method, Tok-RAG, which achieves collaborative generation between the pure\n",
            "LLM and RAG at token level to preserve benefit and avoid detriment. Experiments\n",
            "in real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
            "effectiveness of our method and support our theoretical findings.\n",
            "1\n",
            "INTRODUCTION\n",
            "Retrieval-augmented generation (RAG) has shown promising performance in enhancing Large\n",
            "Language Models (LLMs) by integrating retrieved texts (Xu et al., 2023; Shi et al., 2023; Asai et al.,\n",
            "2023; Ram et al., 2023). Studies indicate that while RAG provides LLMs with valuable additional\n",
            "knowledge (benefit), it also poses a risk of misleading them (detriment) due to noisy or incorrect\n",
            "retrieved texts (Ram et al., 2023; Xu et al., 2024b;a; Jin et al., 2024a; Xie et al., 2023; Jin et al.,\n",
            "2024b). Existing methods attempt to preserve benefit and avoid detriment by adding utility evaluators\n",
            "for retrieval, prompt engineering, or fine-tuning LLMs (Asai et al., 2023; Ding et al., 2024; Xu et al.,\n",
            "2024b; Yoran et al., 2024; Ren et al., 2023; Feng et al., 2023; Mallen et al., 2022; Jiang et al., 2023).\n",
            "However, existing methods are data-driven, need evaluator for utility of retrieved texts or post-hoc. A\n",
            "theory-based method, focusing on core principles of RAG is urgently needed, which is crucial for\n",
            "consistent and reliable improvements without relying on additional training or utility evaluators and\n",
            "improving our understanding for RAG.\n",
            "This paper takes the first step in providing a theoretical framework to explain and trade off the benefit\n",
            "and detriment at token level in RAG and proposes a novel method to preserve benefit and avoid\n",
            "detriment based on our theoretical findings. Specifically, this paper pioneers in modeling next token\n",
            "prediction in RAG as the fusion between the distribution of LLM’s knowledge and the distribution\n",
            "of retrieved texts as shown in Figure 1. Our theoretical derivation based on this formalizes the core\n",
            "of this fusion as the subtraction between two terms measured by the distribution difference: one is\n",
            "distribution completion and the other is distribution contradiction. Further analysis indicates that\n",
            "the distribution completion measures how much out-of-distribution knowledge that retrieved texts\n",
            "∗Corresponding author\n",
            "1\n",
            "arXiv:2406.00944v2  [cs.CL]  17 Oct 2024\n",
            "Query\n",
            "Wole\n",
            "Query\n",
            "Ernst\n",
            "Soyinka\n",
            "…\n",
            "LLM’s \n",
            "Distribution\n",
            "Retrieved \n",
            "Distribution \n",
            "Fusion\n",
            "Distribution\n",
            "Difference\n",
            "Olanipekun\n",
            "LLM’s \n",
            "Dis\n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2405.13576v1\" date=\"2024-05-22\" authors=\"Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou\"/>\n",
            "<Title>\n",
            "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "With the advent of Large Language Models (LLMs), the potential of Retrieval\n",
            "Augmented Generation (RAG) techniques have garnered considerable research\n",
            "attention. Numerous novel algorithms and models have been introduced to enhance\n",
            "various aspects of RAG systems. However, the absence of a standardized\n",
            "framework for implementation, coupled with the inherently intricate RAG\n",
            "process, makes it challenging and time-consuming for researchers to compare and\n",
            "evaluate these approaches in a consistent environment. Existing RAG toolkits\n",
            "like LangChain and LlamaIndex, while available, are often heavy and unwieldy,\n",
            "failing to meet the personalized needs of researchers. In response to this\n",
            "challenge, we propose FlashRAG, an efficient and modular open-source toolkit\n",
            "designed to assist researchers in reproducing existing RAG methods and in\n",
            "developing their own RAG algorithms within a unified framework. Our toolkit\n",
            "implements 12 advanced RAG methods and has gathered and organized 32 benchmark\n",
            "datasets. Our toolkit has various features, including customizable modular\n",
            "framework, rich collection of pre-implemented RAG works, comprehensive\n",
            "datasets, efficient auxiliary pre-processing scripts, and extensive and\n",
            "standard evaluation metrics. Our toolkit and resources are available at\n",
            "https://github.com/RUC-NLPIR/FlashRAG.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "FlashRAG: A Modular Toolkit for Efficient\n",
            "Retrieval-Augmented Generation Research\n",
            "Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou∗\n",
            "Gaoling School of Artificial Intelligence\n",
            "Renmin University of China\n",
            "{jinjiajie, dou}@ruc.edu.cn, yutaozhu94@gmail.com\n",
            "Abstract\n",
            "With the advent of Large Language Models (LLMs), the potential of Retrieval\n",
            "Augmented Generation (RAG) techniques have garnered considerable research\n",
            "attention. Numerous novel algorithms and models have been introduced to enhance\n",
            "various aspects of RAG systems. However, the absence of a standardized framework\n",
            "for implementation, coupled with the inherently intricate RAG process, makes it\n",
            "challenging and time-consuming for researchers to compare and evaluate these\n",
            "approaches in a consistent environment. Existing RAG toolkits like LangChain\n",
            "and LlamaIndex, while available, are often heavy and unwieldy, failing to\n",
            "meet the personalized needs of researchers. In response to this challenge, we\n",
            "propose FlashRAG, an efficient and modular open-source toolkit designed to assist\n",
            "researchers in reproducing existing RAG methods and in developing their own\n",
            "RAG algorithms within a unified framework. Our toolkit implements 12 advanced\n",
            "RAG methods and has gathered and organized 32 benchmark datasets. Our toolkit\n",
            "has various features, including customizable modular framework, rich collection\n",
            "of pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-\n",
            "processing scripts, and extensive and standard evaluation metrics. Our toolkit and\n",
            "resources are available at https://github.com/RUC-NLPIR/FlashRAG.\n",
            "1\n",
            "Introduction\n",
            "In the era of large language models (LLMs), retrieval-augmented generation (RAG) [1, 2] has\n",
            "emerged as a robust solution to mitigate hallucination issues in LLMs by leveraging external\n",
            "knowledge bases [3]. The substantial applications and the potential of RAG technology have\n",
            "attracted considerable research attention. With the introduction of a large number of new algorithms\n",
            "and models to improve various facets of RAG systems in recent years, comparing and evaluating\n",
            "these methods under a consistent setting has become increasingly challenging.\n",
            "Many works are not open-source or have fixed settings in their open-source code, making it difficult\n",
            "to adapt to new data or innovative components. Besides, the datasets and retrieval corpus used often\n",
            "vary, with resources being scattered, which can lead researchers to spend excessive time on pre-\n",
            "processing steps instead of focusing on optimizing their methods. Furthermore, due to the complexity\n",
            "of RAG systems, involving multiple steps such as indexing, retrieval, and generation, researchers\n",
            "often need to implement many parts of the system themselves. Although there are some existing RAG\n",
            "toolkits like LangChain [4] and LlamaIndex [5], they are typically large and cumbersome, hindering\n",
            "researchers from implementing customized processes and failing to address the aforementioned issues.\n",
            "∗Corresponding author\n",
            "Preprint. Under review.\n",
            "arXiv:2405.13576v1  [cs.CL]  22 May 2024\n",
            "Thus, a unified, researcher-oriented RAG toolkit is urgently needed to streamline methodological\n",
            "development and comparative studies.\n",
            "To address the issue mentioned above, we introduce FlashRAG, an open-source library designed to\n",
            "enable researchers to easily reproduce existing RAG methods and develop their own RAG algorithms.\n",
            "This library allows researchers to utilize built pipelines to replicate existing work, employ provided\n",
            "RAG components to construct their own RAG processes, or simply use organized datasets and corpora\n",
            "to accelerate their own RAG workflow. Compared to existing RAG toolkits, FlashRAG is more suited\n",
            "for researchers. To summarize, the key features and capabilities of our FlashRAG library can be\n",
            "outlined in the following four aspects:\n",
            "Extensive and Customizable Modular RAG Framework.\n",
            "To facilitate an easily expandable\n",
            "RAG process, we implemented modular RAG at two levels. At the component level, we offer\n",
            "comprehensive RAG compon\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
            "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
            "increasing demands of application scenarios have driven the evolution of RAG,\n",
            "leading to the integration of advanced retrievers, LLMs and other complementary\n",
            "technologies, which in turn has amplified the intricacy of RAG systems.\n",
            "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process of\n",
            "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
            "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
            "decomposing complex RAG systems into independent modules and specialized\n",
            "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a more advanced\n",
            "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
            "extensive research, this paper further identifies prevalent RAG\n",
            "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
            "analysis of their respective implementation nuances. Modular RAG presents\n",
            "innovative opportunities for the conceptualization and deployment of RAG\n",
            "systems. Finally, the paper explores the potential emergence of new operators\n",
            "and paradigms, establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment of RAG\n",
            "technologies.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2407.19994v3\" date=\"2024-09-13\" authors=\"Cheonsu Jeong\"/>\n",
            "<Title>\n",
            "A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "This study aims to improve knowledge-based question-answering (QA) systems by\n",
            "overcoming the limitations of existing Retrieval-Augmented Generation (RAG)\n",
            "models and implementing an advanced RAG system based on Graph technology to\n",
            "develop high-quality generative AI services. While existing RAG models\n",
            "demonstrate high accuracy and fluency by utilizing retrieved information, they\n",
            "may suffer from accuracy degradation as they generate responses using\n",
            "pre-loaded knowledge without reprocessing. Additionally, they cannot\n",
            "incorporate real-time data after the RAG configuration stage, leading to issues\n",
            "with contextual understanding and biased information. To address these\n",
            "limitations, this study implemented an enhanced RAG system utilizing Graph\n",
            "technology. This system is designed to efficiently search and utilize\n",
            "information. Specifically, it employs LangGraph to evaluate the reliability of\n",
            "retrieved information and synthesizes diverse data to generate more accurate\n",
            "and enhanced responses. Furthermore, the study provides a detailed explanation\n",
            "of the system's operation, key implementation steps, and examples through\n",
            "implementation code and validation results, thereby enhancing the understanding\n",
            "of advanced RAG technology. This approach offers practical guidelines for\n",
            "implementing advanced RAG systems in corporate services, making it a valuable\n",
            "resource for practical application.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            " \n",
            "* Corresponding Author: Cheonsu Jeong; paripal@korea.ac.kr \n",
            " \n",
            " \n",
            "1  \n",
            " \n",
            "A Study on the Implementation Method \n",
            "of an Agent-Based Advanced RAG  \n",
            "System Using Graph \n",
            "Cheonsu Jeong1 \n",
            " \n",
            " \n",
            " \n",
            "1 Dr. Jeong is Principal Consultant & the Technical Leader for AI Automation at SAMSUNG SDS; \n",
            " \n",
            "Abstract  \n",
            "This study aims to improve knowledge-based question-answering (QA) systems by overcoming the limitations of \n",
            "existing Retrieval-Augmented Generation (RAG) models and implementing an advanced RAG system based on \n",
            "Graph technology to develop high-quality generative AI services. While existing RAG models demonstrate high ac-\n",
            "curacy and fluency by utilizing retrieved information, they may suffer from accuracy degradation as they generate \n",
            "responses using pre-loaded knowledge without reprocessing. Additionally, they cannot incorporate real-time data \n",
            "after the RAG configuration stage, leading to issues with contextual understanding and biased information. \n",
            "To address these limitations, this study implemented an enhanced RAG system utilizing Graph technology. This system \n",
            "is designed to efficiently search and utilize information. Specifically, it employs LangGraph to evaluate the reliability \n",
            "of retrieved information and synthesizes diverse data to generate more accurate and enhanced responses. Furthermore, \n",
            "the study provides a detailed explanation of the system's operation, key implementation steps, and examples through \n",
            "implementation code and validation results, thereby enhancing the understanding of advanced RAG technology. This \n",
            "approach offers practical guidelines for implementing advanced RAG systems in corporate services, making it a valu-\n",
            "able resource for practical application. \n",
            " \n",
            "Keywords \n",
            "Advance RAG; Agent RAG; LLM; Generative AI; LangGraph \n",
            " \n",
            " \n",
            "I. Introduction \n",
            "Recent advancements in AI technology have brought sig-\n",
            "nificant attention to Generative AI. Generative AI, a form \n",
            "of artificial intelligence that can create new content such \n",
            "as text, images, audio, and video based on vast amounts \n",
            "of trained data models (Jeong, 2023d), is being applied in \n",
            "various fields, including daily conversations, finance, \n",
            "healthcare, education, and entertainment (Ahn & Park, \n",
            "2023). As generative AI services become more accessible \n",
            "to the general public, the role of generative AI-based chat-\n",
            "bots is becoming increasingly important (Adam et al., \n",
            "2021; Przegalinska et al., 2019; Park, 2024). A chatbot is \n",
            "an intelligent agent that allows users to have conversa-\n",
            "tions typically through text or voice (Sánchez-Díaz et al., \n",
            "2018; Jeong & Jeong, 2020). Recently, generative AI \n",
            "chatbots have advanced to the level of analyzing human \n",
            "emotions and intentions to provide responses (Jeong, \n",
            "2023a). With the advent of large language models \n",
            "(LLMs), these chatbots can now be utilized for automatic \n",
            " \n",
            "Cheonsu Jeong \n",
            " \n",
            "2  \n",
            " \n",
            "dialogue generation and translation (Jeong, 2023b). How-\n",
            "ever, they may generate responses that conflict with the \n",
            "latest information and have a low understanding of new \n",
            "problems or domains as they rely on previously trained \n",
            "data (Jeong, 2023c). While 2023 was marked by the re-\n",
            "lease of foundational large language models (LLMs) like \n",
            "ChatGPT and Llama-2, experts predict that 2024 will be \n",
            "the year of Retrieval Augmented Generation (RAG) and \n",
            "AI Agents (Skelter Labs, 2024). \n",
            "However, there are several considerations for companies \n",
            "looking to adopt generative AI services. Companies must \n",
            "address concerns such as whether the AI can provide ac-\n",
            "curate responses based on internal data, the potential risk \n",
            "of internal data leakage, and how to integrate generative \n",
            "AI with corporate systems. Solutions include using do-\n",
            "main-specific fine-tuned LLMs and enhancing reliability \n",
            "with RAG that utilizes internal information (Jung, 2024). \n",
            "When domain-specific information is fine-tuned on GPT-\n",
            "4 LLM, accuracy improves from 75% to 81%, and adding \n",
            "RAG can further increase accuracy to 86% (Angels et al., \n",
            "2024). RAG models are known for effectively combinin\n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly, let's explore the differences between Modular RAG and traditional Naive RAG in terms of architecture and their implications for production environments.\n",
            "\n",
            "**Architectural Differences:**\n",
            "\n",
            "1. **Structure and Modularity:**\n",
            "   - **Naive RAG:** This approach is characterized by a rigid and linear architecture, typically following a straightforward \"retrieve-then-generate\" process. This limits flexibility and adaptability, as each component of the system is tightly coupled [1].\n",
            "   - **Modular RAG:** In contrast, Modular RAG decomposes the RAG system into independent modules and specialized operators. This creates a reconfigurable framework that is more like a LEGO set, allowing for easy modification, addition, or removal of components as needed [1].\n",
            "\n",
            "2. **Design Innovations:**\n",
            "   - **Naive RAG:** Relies heavily on the similarity of document chunks for retrieval, which can lead to challenges like shallow query understanding and retrieval noise [1].\n",
            "   - **Modular RAG:** Incorporates advanced design features such as routing, scheduling, and fusion mechanisms, supporting more complex RAG patterns like conditional, branching, and looping. This advanced design facilitates better handling of complex queries and variability in data [1].\n",
            "\n",
            "**Benefits at the Production Level:**\n",
            "\n",
            "1. **Flexibility and Scalability:**\n",
            "   - Modular RAG's reconfigurable nature allows for rapid adaptation to changing requirements or integration of new technologies without overhauling the entire system. This is particularly advantageous in dynamic production environments where scalability and flexibility are paramount [1].\n",
            "\n",
            "2. **Maintenance and Upgrades:**\n",
            "   - By having independent modules, updates or maintenance can be performed on specific parts of the system without affecting the overall functionality. This modular approach reduces downtime and simplifies the process of implementing new features or improvements [1].\n",
            "\n",
            "3. **Enhanced Performance:**\n",
            "   - The ability to incorporate advanced mechanisms and patterns leads to more efficient and accurate retrieval and generation processes, resulting in improved performance of AI systems in production [1].\n",
            "\n",
            "These distinctions and benefits make Modular RAG a compelling choice for developers and organizations looking to deploy robust, adaptable, and high-performing AI systems in real-world applications.\n",
            "\n",
            "Sources:\n",
            "[1] http://arxiv.org/abs/2407.21059v1\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "While the context provided does not include a specific real-world case study of Modular RAG deployment, it does highlight the general benefits that such a system would offer in practice. For example, in a production environment like customer service or a recommendation system, Modular RAG could significantly enhance performance by reducing computational overhead and improving processing speeds. This allows the system to handle large volumes of queries more efficiently, providing timely and accurate responses to users.\n",
            "\n",
            "The modular nature of the system also means that individual components can be optimized or replaced without disrupting the entire workflow, enabling continuous improvement and adaptation to changing requirements or data types [1][2]. This flexibility and efficiency make Modular RAG particularly suited for large-scale applications where scalability and adaptability are crucial.\n",
            "\n",
            "For a practical implementation, you might consider a company that deals with extensive customer interactions. By utilizing Modular RAG, it could systematically refine its retrieval and generation processes to ensure that customer queries are addressed with the most relevant and up-to-date information, thereby improving customer satisfaction and operational efficiency.\n",
            "\n",
            "**Sources:**\n",
            "\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "\n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msave_interview\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32minterview\u001b[0m:\n",
            "Human: So you said you were writing an article on Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.?\n",
            "AI: Hello Dr. Mehta, my name is Alex Carter, and I'm an analyst delving into the practical applications of AI. I am particularly interested in understanding how Modular RAG differs from traditional Naive RAG and the benefits it brings when deployed at the production level. Could you explain these differences and benefits, perhaps with some specific examples?\n",
            "AI: Certainly, Dr. Mehta. Modular RAG, or Retrieval-Augmented Generation, represents an advancement over the traditional Naive RAG by addressing some of its inherent limitations, such as inflexibility and inefficiencies in handling diverse and dynamic datasets [3].\n",
            "\n",
            "**Differences:**\n",
            "\n",
            "1. **Flexibility and Efficiency:** Naive RAG systems are often limited in their ability to efficiently process diverse datasets due to a more rigid architecture. Modular RAG improves upon this by introducing a more flexible framework that can efficiently handle a wider variety of data inputs and adjust to dynamic data changes [3].\n",
            "\n",
            "2. **Embedding Techniques:** Advanced RAG models, including Modular RAG, often employ fine-tuned embeddings. These embeddings capture task-specific semantics or domain knowledge, which improves the quality of both the retrieved information and the generated responses. Modular RAG can adaptively adjust these embeddings during inference based on the context, leading to more relevant outputs [1].\n",
            "\n",
            "**Benefits at the Production Level:**\n",
            "\n",
            "1. **Reduced Computational Overhead:** By optimizing the retrieval and generation processes, Modular RAG can reduce computational overhead, making the system more efficient and cost-effective for large-scale deployment. This is particularly important in production environments where resource efficiency is crucial [1][2].\n",
            "\n",
            "2. **Improved Processing Speeds:** Modular RAG systems can achieve faster processing speeds due to their ability to handle data more efficiently and adapt to changes dynamically. This is advantageous in production settings where quick response times are essential [2].\n",
            "\n",
            "3. **Enhanced Accuracy and Context-Awareness:** By integrating retrieval into the generation process and using sophisticated embedding techniques, Modular RAG can provide more accurate and context-aware outputs. This makes it particularly effective for applications that require current or specialized knowledge, such as real-time customer support or complex data analysis [2].\n",
            "\n",
            "In summary, the transition from Naive to Modular RAG in production environments enhances flexibility, efficiency, and accuracy, thereby making AI systems more viable for large-scale deployment.\n",
            "\n",
            "**Sources:**\n",
            "\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "\n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "\n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "AI: Thank you, Dr. Mehta, for the detailed explanation. Could you provide a real-world example where Modular RAG has been successfully deployed, highlighting the specific benefits it brought to that deployment?\n",
            "AI: While the context provided does not include a specific real-world case study of Modular RAG deployment, it does highlight the general benefits that such a system would offer in practice. For example, in a production environment like customer service or a recommendation system, Modular RAG could significantly enhance performance by reducing computational overhead and improving processing speeds. This allows the system to handle large volumes of queries more efficiently, providing timely and accurate responses to users.\n",
            "\n",
            "The modular nature of the system also means that individual components can be optimized or replaced without disrupting the entire workflow, enabling continuous improvement and adaptation to changing requirements or data types [1][2]. This flexibility and efficiency make Modular RAG particularly suited for large-scale applications where scalability and adaptability are crucial.\n",
            "\n",
            "For a practical implementation, you might consider a company that deals with extensive customer interactions. By utilizing Modular RAG, it could systematically refine its retrieval and generation processes to ensure that customer queries are addressed with the most relevant and up-to-date information, thereby improving customer satisfaction and operational efficiency.\n",
            "\n",
            "**Sources:**\n",
            "\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "\n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Thank you, Dr. Nguyen, for that detailed explanation. I'm curious about specific examples or cases where Modular RAG's modularity has significantly enhanced system flexibility and maintenance. Could you provide a real-world application or scenario where this has been particularly beneficial?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://medium.com/aingineer/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e\"/>\n",
            "A Comprehensive Guide to Implementing Modular RAG for Scalable AI Systems | by Gaurav Nigam | aingineer | Dec, 2024 | Medium A Comprehensive Guide to Implementing Modular RAG for Scalable AI Systems In the rapidly evolving landscape of AI, Modular RAG (Retrieval-Augmented Generation) has emerged as a transformative approach to building robust, scalable, and adaptable AI systems. By decoupling retrieval, reasoning, and generation into independent modules, Modular RAG empowers engineering leaders, architects, and senior engineers to design systems that are not only efficient but also flexible enough to meet the dynamic demands of modern enterprises. This guide aims to provide an in-depth exploration of Modular RAG, from its foundational principles to practical implementation strategies, tailored for professionals with a keen interest in scaling enterprise AI systems.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://medium.com/@yusufsevinir/12-️-modular-rag-crafting-customizable-knowledge-retrieval-systems-1298686d9358\"/>\n",
            "6. Benefits and Challenges of Modular RAG ⚖️. Benefits: Flexibility: Modular RAG enables AI systems to handle diverse queries across different data sources. Scalability: New modules can be\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372\"/>\n",
            "These sub-modules allow for more granular control over the RAG process, enabling the system to fine-tune its operations based on the specific requirements of the task​. This pattern closely resembles the traditional RAG process but benefits from the modular approach by allowing individual modules to be optimized or replaced without altering the overall flow. For example, a linear RAG flow might begin with a query expansion module to refine the user’s input, followed by the retrieval module, which fetches the most relevant data chunks. The Modular RAG framework embraces this need through various tuning patterns that enhance the system’s ability to adapt to specific tasks and datasets. Managing these different data types and ensuring seamless integration within the RAG system can be difficult, requiring sophisticated data processing and retrieval strategies​(modular rag paper).\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
            "The traditional or \"Naive\" RAG, while groundbreaking, often struggles with limitations such as inflexibility and inefficiencies in handling diverse and dynamic datasets. Enter Modular RAG—a sophisticated, next-generation approach that significantly enhances the capabilities of Naive RAG by introducing modularity and flexibility into the\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
            "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
            "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
            "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
            "increasing demands of application scenarios have driven the evolution of RAG,\n",
            "leading to the integration of advanced retrievers, LLMs and other complementary\n",
            "technologies, which in turn has amplified the intricacy of RAG systems.\n",
            "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process of\n",
            "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
            "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
            "decomposing complex RAG systems into independent modules and specialized\n",
            "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a more advanced\n",
            "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
            "extensive research, this paper further identifies prevalent RAG\n",
            "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
            "analysis of their respective implementation nuances. Modular RAG presents\n",
            "innovative opportunities for the conceptualization and deployment of RAG\n",
            "systems. Finally, the paper explores the potential emergence of new operators\n",
            "and paradigms, establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment of RAG\n",
            "technologies.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2406.00944v2\" date=\"2024-10-17\" authors=\"Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng\"/>\n",
            "<Title>\n",
            "A Theory for Token-Level Harmonization in Retrieval-Augmented Generation\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\n",
            "large language models (LLMs). Studies show that while RAG provides valuable\n",
            "external information (benefit), it may also mislead LLMs (detriment) with noisy\n",
            "or incorrect retrieved texts. Although many existing methods attempt to\n",
            "preserve benefit and avoid detriment, they lack a theoretical explanation for\n",
            "RAG. The benefit and detriment in the next token prediction of RAG remain a\n",
            "black box that cannot be quantified or compared in an explainable manner, so\n",
            "existing methods are data-driven, need additional utility evaluators or\n",
            "post-hoc. This paper takes the first step towards providing a theory to explain\n",
            "and trade off the benefit and detriment in RAG. First, we model RAG as the\n",
            "fusion between distribution of LLMs knowledge and distribution of retrieved\n",
            "texts. Then, we formalize the trade-off between the value of external knowledge\n",
            "(benefit) and its potential risk of misleading LLMs (detriment) in next token\n",
            "prediction of RAG by distribution difference in this fusion. Finally, we prove\n",
            "that the actual effect of RAG on the token, which is the comparison between\n",
            "benefit and detriment, can be predicted without any training or accessing the\n",
            "utility of retrieval. Based on our theory, we propose a practical novel method,\n",
            "Tok-RAG, which achieves collaborative generation between the pure LLM and RAG\n",
            "at token level to preserve benefit and avoid detriment. Experiments in\n",
            "real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
            "effectiveness of our method and support our theoretical findings.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "A THEORY FOR TOKEN-LEVEL HARMONIZATION IN\n",
            "RETRIEVAL-AUGMENTED GENERATION\n",
            "Shicheng Xu\n",
            "Liang Pang∗Huawei Shen\n",
            "Xueqi Cheng\n",
            "CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS\n",
            "{xushicheng21s,pangliang,shenhuawei,cxq}@ict.ac.cn\n",
            "ABSTRACT\n",
            "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance large\n",
            "language models (LLMs). Studies show that while RAG provides valuable external\n",
            "information (benefit), it may also mislead LLMs (detriment) with noisy or incorrect\n",
            "retrieved texts. Although many existing methods attempt to preserve benefit and\n",
            "avoid detriment, they lack a theoretical explanation for RAG. The benefit and\n",
            "detriment in the next token prediction of RAG remain a ’black box’ that cannot\n",
            "be quantified or compared in an explainable manner, so existing methods are data-\n",
            "driven, need additional utility evaluators or post-hoc. This paper takes the first step\n",
            "towards providing a theory to explain and trade off the benefit and detriment in\n",
            "RAG. First, we model RAG as the fusion between distribution of LLM’s knowledge\n",
            "and distribution of retrieved texts. Then, we formalize the trade-off between the\n",
            "value of external knowledge (benefit) and its potential risk of misleading LLMs\n",
            "(detriment) in next token prediction of RAG by distribution difference in this\n",
            "fusion. Finally, we prove that the actual effect of RAG on the token, which is the\n",
            "comparison between benefit and detriment, can be predicted without any training or\n",
            "accessing the utility of retrieval. Based on our theory, we propose a practical novel\n",
            "method, Tok-RAG, which achieves collaborative generation between the pure\n",
            "LLM and RAG at token level to preserve benefit and avoid detriment. Experiments\n",
            "in real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
            "effectiveness of our method and support our theoretical findings.\n",
            "1\n",
            "INTRODUCTION\n",
            "Retrieval-augmented generation (RAG) has shown promising performance in enhancing Large\n",
            "Language Models (LLMs) by integrating retrieved texts (Xu et al., 2023; Shi et al., 2023; Asai et al.,\n",
            "2023; Ram et al., 2023). Studies indicate that while RAG provides LLMs with valuable additional\n",
            "knowledge (benefit), it also poses a risk of misleading them (detriment) due to noisy or incorrect\n",
            "retrieved texts (Ram et al., 2023; Xu et al., 2024b;a; Jin et al., 2024a; Xie et al., 2023; Jin et al.,\n",
            "2024b). Existing methods attempt to preserve benefit and avoid detriment by adding utility evaluators\n",
            "for retrieval, prompt engineering, or fine-tuning LLMs (Asai et al., 2023; Ding et al., 2024; Xu et al.,\n",
            "2024b; Yoran et al., 2024; Ren et al., 2023; Feng et al., 2023; Mallen et al., 2022; Jiang et al., 2023).\n",
            "However, existing methods are data-driven, need evaluator for utility of retrieved texts or post-hoc. A\n",
            "theory-based method, focusing on core principles of RAG is urgently needed, which is crucial for\n",
            "consistent and reliable improvements without relying on additional training or utility evaluators and\n",
            "improving our understanding for RAG.\n",
            "This paper takes the first step in providing a theoretical framework to explain and trade off the benefit\n",
            "and detriment at token level in RAG and proposes a novel method to preserve benefit and avoid\n",
            "detriment based on our theoretical findings. Specifically, this paper pioneers in modeling next token\n",
            "prediction in RAG as the fusion between the distribution of LLM’s knowledge and the distribution\n",
            "of retrieved texts as shown in Figure 1. Our theoretical derivation based on this formalizes the core\n",
            "of this fusion as the subtraction between two terms measured by the distribution difference: one is\n",
            "distribution completion and the other is distribution contradiction. Further analysis indicates that\n",
            "the distribution completion measures how much out-of-distribution knowledge that retrieved texts\n",
            "∗Corresponding author\n",
            "1\n",
            "arXiv:2406.00944v2  [cs.CL]  17 Oct 2024\n",
            "Query\n",
            "Wole\n",
            "Query\n",
            "Ernst\n",
            "Soyinka\n",
            "…\n",
            "LLM’s \n",
            "Distribution\n",
            "Retrieved \n",
            "Distribution \n",
            "Fusion\n",
            "Distribution\n",
            "Difference\n",
            "Olanipekun\n",
            "LLM’s \n",
            "Dis\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2409.12682v1\" date=\"2024-09-19\" authors=\"Jiho Shin, Reem Aleithan, Hadi Hemmati, Song Wang\"/>\n",
            "<Title>\n",
            "Retrieval-Augmented Test Generation: How Far Are We?\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval Augmented Generation (RAG) has shown notable advancements in\n",
            "software engineering tasks. Despite its potential, RAG's application in unit\n",
            "test generation remains under-explored. To bridge this gap, we take the\n",
            "initiative to investigate the efficacy of RAG-based LLMs in test generation. As\n",
            "RAGs can leverage various knowledge sources to enhance their performance, we\n",
            "also explore the impact of different sources of RAGs' knowledge bases on unit\n",
            "test generation to provide insights into their practical benefits and\n",
            "limitations. Specifically, we examine RAG built upon three types of domain\n",
            "knowledge: 1) API documentation, 2) GitHub issues, and 3) StackOverflow Q&As.\n",
            "Each source offers essential knowledge for creating tests from different\n",
            "perspectives, i.e., API documentations provide official API usage guidelines,\n",
            "GitHub issues offer resolutions of issues related to the APIs from the library\n",
            "developers, and StackOverflow Q&As present community-driven solutions and best\n",
            "practices. For our experiment, we focus on five widely used and typical\n",
            "Python-based machine learning (ML) projects, i.e., TensorFlow, PyTorch,\n",
            "Scikit-learn, Google JAX, and XGBoost to build, train, and deploy complex\n",
            "neural networks efficiently. We conducted experiments using the top 10% most\n",
            "widely used APIs across these projects, involving a total of 188 APIs. We\n",
            "investigate the effectiveness of four state-of-the-art LLMs (open and\n",
            "closed-sourced), i.e., GPT-3.5-Turbo, GPT-4o, Mistral MoE 8x22B, and Llamma 3.1\n",
            "405B. Additionally, we compare three prompting strategies in generating unit\n",
            "test cases for the experimental APIs, i.e., zero-shot, a Basic RAG, and an\n",
            "API-level RAG on the three external sources. Finally, we compare the cost of\n",
            "different sources of knowledge used for the RAG.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "Retrieval-Augmented Test Generation: How Far Are We?\n",
            "JIHO SHIN, York University, Canada\n",
            "REEM ALEITHAN, York University, Canada\n",
            "HADI HEMMATI, York University, Canada\n",
            "SONG WANG, York University, Canada\n",
            "Retrieval Augmented Generation (RAG) has shown notable advancements in software engineering tasks.\n",
            "Despite its potential, RAG’s application in unit test generation remains under-explored. To bridge this gap, we\n",
            "take the initiative to investigate the efficacy of RAG-based LLMs in test generation. As RAGs can leverage\n",
            "various knowledge sources to enhance their performance, we also explore the impact of different sources of\n",
            "RAGs’ knowledge bases on unit test generation to provide insights into their practical benefits and limitations.\n",
            "Specifically, we examine RAG built upon three types of domain knowledge: 1) API documentation, 2) GitHub\n",
            "issues, and 3) StackOverflow Q&As. Each source offers essential knowledge for creating tests from different\n",
            "perspectives, i.e., API documentations provide official API usage guidelines, GitHub issues offer resolutions of\n",
            "issues related to the APIs from the library developers, and StackOverflow Q&As present community-driven\n",
            "solutions and best practices. For our experiment, we focus on five widely used and typical Python-based\n",
            "machine learning (ML) projects, i.e., TensorFlow, PyTorch, Scikit-learn, Google JAX, and XGBoost to build,\n",
            "train, and deploy complex neural networks efficiently. We conducted experiments using the top 10% most\n",
            "widely used APIs across these projects, involving a total of 188 APIs.\n",
            "We investigate the effectiveness of four state-of-the-art LLMs (open and closed-sourced), i.e., GPT-3.5-Turbo,\n",
            "GPT-4o, Mistral MoE 8x22B, and Llamma 3.1 405B. Additionally, we compare three prompting strategies in\n",
            "generating unit test cases for the experimental APIs, i.e., zero-shot, a Basic RAG, and an API-level RAG on the\n",
            "three external sources. Finally, we compare the cost of different sources of knowledge used for the RAG.\n",
            "We conduct both qualitative and quantitative evaluations to investigate the generated test cases. For the\n",
            "quantitative analysis, we assess the syntactical and dynamic correctness of the generated tests. We observe\n",
            "that RAG does not improve the syntactical or dynamic correctness of unit test cases. However, using Basic\n",
            "RAG could improve the line coverage by an average of 8.94% and API-level RAG by 9.68%. We investigate\n",
            "the token cost of different RAGs with different prompting strategies. We find that RAGs using GitHub issue\n",
            "documents have the highest token cost. We also find that using limiting the number of test cases for cost\n",
            "efficiency significantly reduces the cost. Finally, we perform a manual analysis over a subset of the generated\n",
            "tests to evaluate how different strategies impact the software under test in more depth. We find that RAG helps\n",
            "cover unique lines by providing unique states of programs that cannot be generated directly by LLMs. Our\n",
            "study suggests that RAG has great potential in improving unit tests’ line coverage when the right documents\n",
            "with unique examples of program states are given. Proposing new retrieval techniques that can search these\n",
            "documents will further improve RAG-based unit test generation for future work.\n",
            "CCS Concepts: • Software and its engineering →Software maintenance tools; Automatic programming; •\n",
            "Computing methodologies →Natural language generation; Machine translation; • General and reference\n",
            "→Surveys and overviews.\n",
            "Authors’ Contact Information: Jiho Shin, jihoshin@yorku.ca, York University, Toronto, ON, Canada; Reem Aleithan,\n",
            "reem1100@yorku.ca, York University, Toronto, ON, Canada; Hadi Hemmati, hemmati@yorku.ca, York University, Toronto,\n",
            "ON, Canada; Song Wang, wangsong@yorku.ca, York University, Toronto, ON, Canada.\n",
            "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\n",
            "provided that copies are not made or distributed for profit or commercial advantage and that copies bear \n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly. One compelling example of Modular RAG's successful implementation is its use in enhancing knowledge-based question-answering (QA) systems with high-quality generative AI services. In this context, Modular RAG overcomes limitations found in traditional RAG models, such as the inability to incorporate real-time data and issues with contextual understanding. By utilizing graph technology, Modular RAG enables efficient information retrieval and synthesis, leading to more accurate and contextually aware responses [9].\n",
            "\n",
            "Specifically, a system using Modular RAG can employ a framework like LangGraph to assess the reliability of retrieved information and synthesize diverse data. This capability allows the system to adapt quickly to new information and queries, which is crucial in dynamic production environments. The modular framework's flexibility facilitates the integration of new modules and technologies without disrupting the existing setup, thereby enhancing scalability and efficiency [9].\n",
            "\n",
            "This adaptability and efficiency make Modular RAG particularly valuable for enterprise-level applications where the ability to handle a wide array of queries and datasets efficiently is a significant advantage.\n",
            "\n",
            "Source:\n",
            "[9] http://arxiv.org/abs/2407.19994v3\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msave_interview\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32minterview\u001b[0m:\n",
            "Human: So you said you were writing an article on Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.?\n",
            "AI: Hello, my name is Alex Carter, and I am an AI research journalist. Thank you for taking the time to speak with me, Prof. Hinton. To start, could you explain how Modular RAG differs from traditional Naive RAG in the context of AI systems? What are the core distinctions that make Modular RAG more suitable for production environments?\n",
            "AI: Certainly, Prof. Hinton. Modular RAG differs from traditional Naive RAG in several key ways that enhance its effectiveness in production environments.\n",
            "\n",
            "1. **Flexibility and Adaptability**: Naive RAG, while foundational, often struggles with inflexibility and inefficiencies when dealing with diverse and dynamic datasets. Modular RAG, on the other hand, improves upon this by allowing for more adaptable and flexible integration of retrieval and generation components. This modularity facilitates the customization of each component to better handle various data types and query contexts [3].\n",
            "\n",
            "2. **Dynamic Embedding Techniques**: Modular RAG employs dynamic embedding techniques that enable the system to adjust embeddings during inference based on the specific context of the query or the retrieved information. This results in more contextually relevant and coherent responses, which is a significant improvement over the static nature of embeddings in Naive RAG [1].\n",
            "\n",
            "3. **Integration with Vector Databases**: Another benefit of Modular RAG is the integration with vector databases, which provides the necessary infrastructure for storing and retrieving high-dimensional embeddings of contextual information. This integration allows for precise similarity searches, leading to more accurate and context-aware outputs. This capability is crucial for applications that require current or specialized knowledge [2].\n",
            "\n",
            "Overall, the modular nature of this approach allows for greater scalability and efficiency in AI models, making it particularly suited for production environments where adaptability and performance are essential.\n",
            "\n",
            "Sources:\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "AI: Thank you for that detailed explanation, Prof. Hinton. Could you provide a specific example of how Modular RAG has been successfully implemented in a real-world application, highlighting its scalability and efficiency benefits?\n",
            "AI: Certainly. One compelling example of Modular RAG's successful implementation is its use in enhancing knowledge-based question-answering (QA) systems with high-quality generative AI services. In this context, Modular RAG overcomes limitations found in traditional RAG models, such as the inability to incorporate real-time data and issues with contextual understanding. By utilizing graph technology, Modular RAG enables efficient information retrieval and synthesis, leading to more accurate and contextually aware responses [9].\n",
            "\n",
            "Specifically, a system using Modular RAG can employ a framework like LangGraph to assess the reliability of retrieved information and synthesize diverse data. This capability allows the system to adapt quickly to new information and queries, which is crucial in dynamic production environments. The modular framework's flexibility facilitates the integration of new modules and technologies without disrupting the existing setup, thereby enhancing scalability and efficiency [9].\n",
            "\n",
            "This adaptability and efficiency make Modular RAG particularly valuable for enterprise-level applications where the ability to handle a wide array of queries and datasets efficiently is a significant advantage.\n",
            "\n",
            "Source:\n",
            "[9] http://arxiv.org/abs/2407.19994v3\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly! Let's consider a real-world application in the realm of customer service systems, where Modular RAG's flexibility and modularity have shown significant benefits.\n",
            "\n",
            "**Scenario: Dynamic Customer Support Systems**\n",
            "\n",
            "In a customer service setting, AI systems often need to handle a variety of queries that can range from simple information requests to complex problem-solving tasks. Traditional Naive RAG systems might struggle with the diversity and complexity of queries due to their rigid architecture. However, Modular RAG can be particularly beneficial in such a scenario for the following reasons:\n",
            "\n",
            "1. **Dynamic Integration of Knowledge Sources:**\n",
            "   - Modular RAG allows for the easy integration of various knowledge bases, such as product manuals, FAQ databases, and recent customer interaction logs. This modular integration means that as new information becomes available, like updated product features or resolved issues, it can be seamlessly added to the system without disrupting current operations [1].\n",
            "\n",
            "2. **Adaptive Query Processing:**\n",
            "   - With its ability to incorporate advanced routing and scheduling mechanisms, Modular RAG can dynamically adjust how queries are processed. For instance, simple queries can be routed through a basic retrieval module, while more complex queries might engage a series of specialized modules that can interact with multiple data sources to generate a comprehensive response [1].\n",
            "\n",
            "3. **Continuous Improvement and A/B Testing:**\n",
            "   - The modular nature of Modular RAG systems allows for iterative improvements and experimentation. Different configurations or modules can be tested in parallel, such as trying out new retrieval strategies or integrating third-party APIs, to determine the most effective setup without overhauling the entire system [1].\n",
            "\n",
            "4. **Scalability and Customization:**\n",
            "   - As the volume of customer interactions grows, the system can scale by adding more modules or enhancing existing ones. Additionally, customization for specific client needs or industry requirements can be achieved by swapping or upgrading individual modules without affecting the entire architecture [1].\n",
            "\n",
            "This kind of adaptability and maintainability not only enhances the performance and reliability of customer service systems but also allows businesses to provide timely and accurate assistance to their customers, ultimately improving customer satisfaction and operational efficiency.\n",
            "\n",
            "Sources:\n",
            "[1] http://arxiv.org/abs/2407.21059v1\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msave_interview\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32minterview\u001b[0m:\n",
            "Human: So you said you were writing an article on Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.?\n",
            "AI: Hi Dr. Nguyen, my name is Jamie Carter, and I'm an analyst eager to delve into the world of AI systems architecture. I'm particularly interested in understanding the structural differences between Modular RAG and traditional Naive RAG, especially from the perspective of modularity and its benefits in a production environment. Could you start by explaining what sets Modular RAG apart from Naive RAG in terms of architecture?\n",
            "AI: Certainly, let's explore the differences between Modular RAG and traditional Naive RAG in terms of architecture and their implications for production environments.\n",
            "\n",
            "**Architectural Differences:**\n",
            "\n",
            "1. **Structure and Modularity:**\n",
            "   - **Naive RAG:** This approach is characterized by a rigid and linear architecture, typically following a straightforward \"retrieve-then-generate\" process. This limits flexibility and adaptability, as each component of the system is tightly coupled [1].\n",
            "   - **Modular RAG:** In contrast, Modular RAG decomposes the RAG system into independent modules and specialized operators. This creates a reconfigurable framework that is more like a LEGO set, allowing for easy modification, addition, or removal of components as needed [1].\n",
            "\n",
            "2. **Design Innovations:**\n",
            "   - **Naive RAG:** Relies heavily on the similarity of document chunks for retrieval, which can lead to challenges like shallow query understanding and retrieval noise [1].\n",
            "   - **Modular RAG:** Incorporates advanced design features such as routing, scheduling, and fusion mechanisms, supporting more complex RAG patterns like conditional, branching, and looping. This advanced design facilitates better handling of complex queries and variability in data [1].\n",
            "\n",
            "**Benefits at the Production Level:**\n",
            "\n",
            "1. **Flexibility and Scalability:**\n",
            "   - Modular RAG's reconfigurable nature allows for rapid adaptation to changing requirements or integration of new technologies without overhauling the entire system. This is particularly advantageous in dynamic production environments where scalability and flexibility are paramount [1].\n",
            "\n",
            "2. **Maintenance and Upgrades:**\n",
            "   - By having independent modules, updates or maintenance can be performed on specific parts of the system without affecting the overall functionality. This modular approach reduces downtime and simplifies the process of implementing new features or improvements [1].\n",
            "\n",
            "3. **Enhanced Performance:**\n",
            "   - The ability to incorporate advanced mechanisms and patterns leads to more efficient and accurate retrieval and generation processes, resulting in improved performance of AI systems in production [1].\n",
            "\n",
            "These distinctions and benefits make Modular RAG a compelling choice for developers and organizations looking to deploy robust, adaptable, and high-performing AI systems in real-world applications.\n",
            "\n",
            "Sources:\n",
            "[1] http://arxiv.org/abs/2407.21059v1\n",
            "AI: Thank you, Dr. Nguyen, for that detailed explanation. I'm curious about specific examples or cases where Modular RAG's modularity has significantly enhanced system flexibility and maintenance. Could you provide a real-world application or scenario where this has been particularly beneficial?\n",
            "AI: Certainly! Let's consider a real-world application in the realm of customer service systems, where Modular RAG's flexibility and modularity have shown significant benefits.\n",
            "\n",
            "**Scenario: Dynamic Customer Support Systems**\n",
            "\n",
            "In a customer service setting, AI systems often need to handle a variety of queries that can range from simple information requests to complex problem-solving tasks. Traditional Naive RAG systems might struggle with the diversity and complexity of queries due to their rigid architecture. However, Modular RAG can be particularly beneficial in such a scenario for the following reasons:\n",
            "\n",
            "1. **Dynamic Integration of Knowledge Sources:**\n",
            "   - Modular RAG allows for the easy integration of various knowledge bases, such as product manuals, FAQ databases, and recent customer interaction logs. This modular integration means that as new information becomes available, like updated product features or resolved issues, it can be seamlessly added to the system without disrupting current operations [1].\n",
            "\n",
            "2. **Adaptive Query Processing:**\n",
            "   - With its ability to incorporate advanced routing and scheduling mechanisms, Modular RAG can dynamically adjust how queries are processed. For instance, simple queries can be routed through a basic retrieval module, while more complex queries might engage a series of specialized modules that can interact with multiple data sources to generate a comprehensive response [1].\n",
            "\n",
            "3. **Continuous Improvement and A/B Testing:**\n",
            "   - The modular nature of Modular RAG systems allows for iterative improvements and experimentation. Different configurations or modules can be tested in parallel, such as trying out new retrieval strategies or integrating third-party APIs, to determine the most effective setup without overhauling the entire system [1].\n",
            "\n",
            "4. **Scalability and Customization:**\n",
            "   - As the volume of customer interactions grows, the system can scale by adding more modules or enhancing existing ones. Additionally, customization for specific client needs or industry requirements can be achieved by swapping or upgrading individual modules without affecting the entire architecture [1].\n",
            "\n",
            "This kind of adaptability and maintainability not only enhances the performance and reliability of customer service systems but also allows businesses to provide timely and accurate assistance to their customers, ultimately improving customer satisfaction and operational efficiency.\n",
            "\n",
            "Sources:\n",
            "[1] http://arxiv.org/abs/2407.21059v1\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mwrite_section\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Unlocking the Potential of Modular RAG for Scalable AI Systems\n",
            "\n",
            "### Summary\n",
            "\n",
            "In the rapidly advancing field of AI, the integration of retrieval-augmented generation (RAG) with large language models (LLMs) has revolutionized the landscape, offering significant enhancements in handling knowledge-intensive tasks. Dr. Mehta's focus on deploying AI solutions in production highlights the pivotal role that Modular RAG can play in creating scalable, efficient, and adaptable AI systems. The insights gathered from various sources underscore the novel approach of Modular RAG, which stands out due to its flexibility and efficiency compared to traditional RAG models.\n",
            "\n",
            "The key insights from the analysis of the documents are as follows:\n",
            "\n",
            "1. **Naive RAG Foundations and Limitations**: Naive RAG models combine information retrieval with natural language generation but are often limited by their inflexibility and inefficiencies, particularly in handling diverse and dynamic datasets [1].\n",
            "\n",
            "2. **Advanced RAG Techniques**: Advanced RAG models improve upon Naive RAG by utilizing dynamic embeddings and vector databases, which enhance the retrieval and generation processes, making them more context-aware and accurate [2].\n",
            "\n",
            "3. **Introduction of Modular RAG**: The Modular RAG framework decomposes complex RAG systems into independent modules, allowing for reconfigurable designs that improve computational efficiency and scalability [3].\n",
            "\n",
            "4. **Theoretical Advancements in RAG**: Research introduces a theoretical framework for understanding the trade-offs between the benefits and detriments of RAG, offering a more structured approach to optimizing its performance [4].\n",
            "\n",
            "5. **Practical Implementations and Toolkits**: Tools like FlashRAG provide a modular and efficient open-source framework for researchers to develop and test RAG algorithms, ensuring consistency and ease of adaptation to new data [5].\n",
            "\n",
            "Modular RAG presents a compelling evolution in AI system design, addressing the limitations of traditional RAG by offering a more adaptable and efficient framework. This transformation is crucial for deploying AI solutions at scale, where computational efficiency and flexibility are paramount.\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### Naive RAG: Foundations and Challenges\n",
            "\n",
            "Naive RAG models laid the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. The primary structure involves indexing, retrieval, and generation, yet these models face several limitations:\n",
            "\n",
            "- **Inflexibility**: Naive RAG systems often struggle with diverse datasets, resulting in inefficiencies [1].\n",
            "- **Retrieval Redundancy and Noise**: Directly feeding retrieved chunks into LLMs can lead to noise, increasing the risk of generating erroneous responses [3].\n",
            "- **Shallow Query Understanding**: The reliance on similarity calculations without deeper semantic analysis hinders performance in complex queries [4].\n",
            "\n",
            "#### Advanced RAG Techniques\n",
            "\n",
            "Advanced RAG models expand upon the Naive RAG framework by incorporating dynamic embedding techniques and vector databases:\n",
            "\n",
            "- **Dynamic Embeddings**: These techniques allow models to fine-tune embeddings, capturing domain-specific semantics and improving retrieval accuracy [1].\n",
            "- **Vector Databases**: They store high-dimensional embeddings, enabling precise similarity searches that enhance the RAG system's effectiveness in specialized applications [2].\n",
            "\n",
            "#### Modular RAG: A Reconfigurable Approach\n",
            "\n",
            "Modular RAG introduces a paradigm shift by decomposing RAG systems into independent modules, facilitating a reconfigurable framework:\n",
            "\n",
            "- **Independent Modules and Specialized Operators**: This decomposition allows for greater flexibility in system design, enabling modules to be optimized or replaced without disrupting the overall system [3].\n",
            "- **Routing, Scheduling, and Fusion Mechanisms**: Modular RAG integrates advanced mechanisms that transcend the traditional linear architecture, offering improved efficiency and scalability [3].\n",
            "- **Innovative Opportunities**: The modular approach provides a solid foundation for new conceptualizations in RAG system deployment, with opportunities to introduce novel operators and paradigms [3].\n",
            "\n",
            "#### Theoretical Insights into RAG\n",
            "\n",
            "Recent studies offer theoretical frameworks to understand the benefits and detriments of RAG:\n",
            "\n",
            "- **Benefit and Detriment Trade-offs**: Researchers model RAG as a fusion of LLM knowledge and retrieved texts, formalizing the trade-off between external knowledge benefits and the risk of misleading LLMs [4].\n",
            "- **Token-Level Harmonization**: The Tok-RAG method collaborates generation between pure LLMs and RAG at the token level, preserving benefits while avoiding detriments [4].\n",
            "\n",
            "#### Practical Implementations and Toolkits\n",
            "\n",
            "The development of toolkits like FlashRAG addresses the challenges of implementing and testing RAG systems:\n",
            "\n",
            "- **Unified Framework**: FlashRAG offers a customizable modular framework, enabling researchers to reproduce existing RAG methods and develop new algorithms consistently [5].\n",
            "- **Comprehensive Resources**: The toolkit includes pre-implemented RAG works, extensive datasets, and standard evaluation metrics, streamlining the research process [5].\n",
            "\n",
            "### Sources\n",
            "\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[3] http://arxiv.org/abs/2407.21059v1  \n",
            "[4] http://arxiv.org/abs/2406.00944v2  \n",
            "[5] http://arxiv.org/abs/2405.13576v1\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mconduct_interview\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Unlocking the Potential of Modular RAG for Scalable AI Systems\n",
            "\n",
            "### Summary\n",
            "\n",
            "In the rapidly advancing field of AI, the integration of retrieval-augmented generation (RAG) with large language models (LLMs) has revolutionized the landscape, offering significant enhancements in handling knowledge-intensive tasks. Dr. Mehta's focus on deploying AI solutions in production highlights the pivotal role that Modular RAG can play in creating scalable, efficient, and adaptable AI systems. The insights gathered from various sources underscore the novel approach of Modular RAG, which stands out due to its flexibility and efficiency compared to traditional RAG models.\n",
            "\n",
            "The key insights from the analysis of the documents are as follows:\n",
            "\n",
            "1. **Naive RAG Foundations and Limitations**: Naive RAG models combine information retrieval with natural language generation but are often limited by their inflexibility and inefficiencies, particularly in handling diverse and dynamic datasets [1].\n",
            "\n",
            "2. **Advanced RAG Techniques**: Advanced RAG models improve upon Naive RAG by utilizing dynamic embeddings and vector databases, which enhance the retrieval and generation processes, making them more context-aware and accurate [2].\n",
            "\n",
            "3. **Introduction of Modular RAG**: The Modular RAG framework decomposes complex RAG systems into independent modules, allowing for reconfigurable designs that improve computational efficiency and scalability [3].\n",
            "\n",
            "4. **Theoretical Advancements in RAG**: Research introduces a theoretical framework for understanding the trade-offs between the benefits and detriments of RAG, offering a more structured approach to optimizing its performance [4].\n",
            "\n",
            "5. **Practical Implementations and Toolkits**: Tools like FlashRAG provide a modular and efficient open-source framework for researchers to develop and test RAG algorithms, ensuring consistency and ease of adaptation to new data [5].\n",
            "\n",
            "Modular RAG presents a compelling evolution in AI system design, addressing the limitations of traditional RAG by offering a more adaptable and efficient framework. This transformation is crucial for deploying AI solutions at scale, where computational efficiency and flexibility are paramount.\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### Naive RAG: Foundations and Challenges\n",
            "\n",
            "Naive RAG models laid the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. The primary structure involves indexing, retrieval, and generation, yet these models face several limitations:\n",
            "\n",
            "- **Inflexibility**: Naive RAG systems often struggle with diverse datasets, resulting in inefficiencies [1].\n",
            "- **Retrieval Redundancy and Noise**: Directly feeding retrieved chunks into LLMs can lead to noise, increasing the risk of generating erroneous responses [3].\n",
            "- **Shallow Query Understanding**: The reliance on similarity calculations without deeper semantic analysis hinders performance in complex queries [4].\n",
            "\n",
            "#### Advanced RAG Techniques\n",
            "\n",
            "Advanced RAG models expand upon the Naive RAG framework by incorporating dynamic embedding techniques and vector databases:\n",
            "\n",
            "- **Dynamic Embeddings**: These techniques allow models to fine-tune embeddings, capturing domain-specific semantics and improving retrieval accuracy [1].\n",
            "- **Vector Databases**: They store high-dimensional embeddings, enabling precise similarity searches that enhance the RAG system's effectiveness in specialized applications [2].\n",
            "\n",
            "#### Modular RAG: A Reconfigurable Approach\n",
            "\n",
            "Modular RAG introduces a paradigm shift by decomposing RAG systems into independent modules, facilitating a reconfigurable framework:\n",
            "\n",
            "- **Independent Modules and Specialized Operators**: This decomposition allows for greater flexibility in system design, enabling modules to be optimized or replaced without disrupting the overall system [3].\n",
            "- **Routing, Scheduling, and Fusion Mechanisms**: Modular RAG integrates advanced mechanisms that transcend the traditional linear architecture, offering improved efficiency and scalability [3].\n",
            "- **Innovative Opportunities**: The modular approach provides a solid foundation for new conceptualizations in RAG system deployment, with opportunities to introduce novel operators and paradigms [3].\n",
            "\n",
            "#### Theoretical Insights into RAG\n",
            "\n",
            "Recent studies offer theoretical frameworks to understand the benefits and detriments of RAG:\n",
            "\n",
            "- **Benefit and Detriment Trade-offs**: Researchers model RAG as a fusion of LLM knowledge and retrieved texts, formalizing the trade-off between external knowledge benefits and the risk of misleading LLMs [4].\n",
            "- **Token-Level Harmonization**: The Tok-RAG method collaborates generation between pure LLMs and RAG at the token level, preserving benefits while avoiding detriments [4].\n",
            "\n",
            "#### Practical Implementations and Toolkits\n",
            "\n",
            "The development of toolkits like FlashRAG addresses the challenges of implementing and testing RAG systems:\n",
            "\n",
            "- **Unified Framework**: FlashRAG offers a customizable modular framework, enabling researchers to reproduce existing RAG methods and develop new algorithms consistently [5].\n",
            "- **Comprehensive Resources**: The toolkit includes pre-implemented RAG works, extensive datasets, and standard evaluation metrics, streamlining the research process [5].\n",
            "\n",
            "### Sources\n",
            "\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[3] http://arxiv.org/abs/2407.21059v1  \n",
            "[4] http://arxiv.org/abs/2406.00944v2  \n",
            "[5] http://arxiv.org/abs/2405.13576v1\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mwrite_section\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Modular RAG: Enhancing Flexibility and Maintenance in AI Systems\n",
            "\n",
            "### Summary\n",
            "\n",
            "In the evolving landscape of artificial intelligence, Retrieval-Augmented Generation (RAG) has emerged as a potent tool for empowering Large Language Models (LLMs) in handling complex, knowledge-intensive tasks. However, as the demands on these systems increase, the traditional RAG framework, known as Naive RAG, struggles to keep pace due to its rigid architecture and inefficiencies. In response, the concept of Modular RAG has been introduced, offering a transformative approach to RAG system design.\n",
            "\n",
            "Modular RAG distinguishes itself by decomposing the traditional RAG process into independent modules and specialized operators. This modularity allows for greater flexibility and adaptability, akin to LEGO-like reconfigurable frameworks, which significantly enhances system flexibility and maintenance at a production level [1]. This novel approach moves beyond the linear architecture of Naive RAG, integrating advanced design features such as routing, scheduling, and fusion mechanisms to address the increasing complexity of application scenarios.\n",
            "\n",
            "The innovative aspect of Modular RAG lies in its ability to identify and implement various RAG patterns—linear, conditional, branching, and looping—each with its specific nuances. This modularity not only facilitates the conceptualization and deployment of more sophisticated RAG systems but also opens up possibilities for new operators and paradigms that could further the evolution of RAG technologies [1]. Moreover, Modular RAG's adaptability supports end-to-end training across its components, marking a significant advancement over its predecessors [2].\n",
            "\n",
            "Despite the potential benefits, the transition to Modular RAG is not without challenges. Ensuring the integration of fair ranking systems and maintaining high generation quality are critical concerns that need to be addressed to achieve responsible and equitable RAG applications [3]. Furthermore, the theoretical underpinnings of RAG systems, particularly the balance between the benefit and detriment of retrieved texts, remain areas for ongoing research [4].\n",
            "\n",
            "In summary, the transformation from Naive to Modular RAG represents a significant leap in AI system architecture, offering enhanced flexibility and maintenance capabilities. This shift holds promise for more efficient and effective AI applications, although it necessitates further research to fully realize its potential.\n",
            "\n",
            "Sources:\n",
            "1. [1] http://arxiv.org/abs/2407.21059v1\n",
            "2. [2] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "3. [3] https://github.com/kimdanny/Fair-RAG\n",
            "4. [4] http://arxiv.org/abs/2406.00944v2\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### Evolution from Naive to Modular RAG\n",
            "\n",
            "Naive RAG systems, while foundational, have been limited by their rigid structure, which often results in inefficiencies and difficulties in adapting to dynamic datasets. Modular RAG emerges as a sophisticated evolution, designed to overcome these limitations through modularity and reconfigurability [1]. This approach allows for independent modules to be developed and integrated, providing the flexibility needed to handle complex queries and diverse datasets [2].\n",
            "\n",
            "- **Modular Architecture**: By breaking down RAG systems into discrete modules, Modular RAG enables the seamless integration of advanced technologies such as routing and scheduling mechanisms. This modularity not only enhances flexibility but also simplifies the maintenance and upgrading of RAG systems, making them more adaptable to evolving demands [1].\n",
            "  \n",
            "- **Implementation Patterns**: The identification of prevalent RAG patterns—linear, conditional, branching, and looping—allows for targeted implementation strategies. Each pattern presents unique implementation nuances, which Modular RAG addresses through specialized operators, thereby optimizing the performance of RAG systems across various scenarios [1].\n",
            "\n",
            "#### Addressing Challenges in RAG Systems\n",
            "\n",
            "The transition to Modular RAG is not without its challenges. Ensuring fair ranking and maintaining high generation quality are critical areas that require careful consideration and development [3].\n",
            "\n",
            "- **Fair Ranking Integration**: The integration of fair ranking mechanisms into RAG systems ensures equitable exposure of relevant items, thus promoting fairness and reducing potential biases. This integration is crucial for maintaining system effectiveness while ensuring that all stakeholders are considered [3].\n",
            "  \n",
            "- **Benefit vs. Detriment Trade-Off**: One of the key challenges in RAG systems is balancing the benefit of providing valuable external knowledge with the risk of misleading LLMs through noisy or incorrect texts. A theory-based approach, as proposed in recent studies, offers a framework for understanding and managing this trade-off, which is essential for reliable and consistent improvements in RAG systems [4].\n",
            "\n",
            "#### Practical Implications and Future Directions\n",
            "\n",
            "The modular approach of RAG systems offers significant practical advantages, particularly in production-level applications where flexibility and maintenance are paramount. By facilitating end-to-end training and enabling more complex, integrated workflows, Modular RAG sets the stage for more robust AI applications [2].\n",
            "\n",
            "Moreover, the ongoing research into theoretical frameworks and fair ranking integration highlights the dynamic nature of RAG systems' development. As these areas continue to evolve, Modular RAG is poised to play a pivotal role in shaping the future of AI system architectures, with potential applications spanning various domains from customer service to advanced knowledge systems [1].\n",
            "\n",
            "In conclusion, the shift towards Modular RAG represents a promising advancement in AI technology, offering enhanced capabilities and addressing many of the limitations of traditional RAG systems. However, realizing the full potential of this approach will require continued innovation and research, particularly in areas related to fairness and theoretical foundations.\n",
            "\n",
            "### Sources\n",
            "\n",
            "[1] http://arxiv.org/abs/2407.21059v1  \n",
            "[2] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
            "[3] https://github.com/kimdanny/Fair-RAG  \n",
            "[4] http://arxiv.org/abs/2406.00944v2  \n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mconduct_interview\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Modular RAG: Enhancing Flexibility and Maintenance in AI Systems\n",
            "\n",
            "### Summary\n",
            "\n",
            "In the evolving landscape of artificial intelligence, Retrieval-Augmented Generation (RAG) has emerged as a potent tool for empowering Large Language Models (LLMs) in handling complex, knowledge-intensive tasks. However, as the demands on these systems increase, the traditional RAG framework, known as Naive RAG, struggles to keep pace due to its rigid architecture and inefficiencies. In response, the concept of Modular RAG has been introduced, offering a transformative approach to RAG system design.\n",
            "\n",
            "Modular RAG distinguishes itself by decomposing the traditional RAG process into independent modules and specialized operators. This modularity allows for greater flexibility and adaptability, akin to LEGO-like reconfigurable frameworks, which significantly enhances system flexibility and maintenance at a production level [1]. This novel approach moves beyond the linear architecture of Naive RAG, integrating advanced design features such as routing, scheduling, and fusion mechanisms to address the increasing complexity of application scenarios.\n",
            "\n",
            "The innovative aspect of Modular RAG lies in its ability to identify and implement various RAG patterns—linear, conditional, branching, and looping—each with its specific nuances. This modularity not only facilitates the conceptualization and deployment of more sophisticated RAG systems but also opens up possibilities for new operators and paradigms that could further the evolution of RAG technologies [1]. Moreover, Modular RAG's adaptability supports end-to-end training across its components, marking a significant advancement over its predecessors [2].\n",
            "\n",
            "Despite the potential benefits, the transition to Modular RAG is not without challenges. Ensuring the integration of fair ranking systems and maintaining high generation quality are critical concerns that need to be addressed to achieve responsible and equitable RAG applications [3]. Furthermore, the theoretical underpinnings of RAG systems, particularly the balance between the benefit and detriment of retrieved texts, remain areas for ongoing research [4].\n",
            "\n",
            "In summary, the transformation from Naive to Modular RAG represents a significant leap in AI system architecture, offering enhanced flexibility and maintenance capabilities. This shift holds promise for more efficient and effective AI applications, although it necessitates further research to fully realize its potential.\n",
            "\n",
            "Sources:\n",
            "1. [1] http://arxiv.org/abs/2407.21059v1\n",
            "2. [2] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "3. [3] https://github.com/kimdanny/Fair-RAG\n",
            "4. [4] http://arxiv.org/abs/2406.00944v2\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### Evolution from Naive to Modular RAG\n",
            "\n",
            "Naive RAG systems, while foundational, have been limited by their rigid structure, which often results in inefficiencies and difficulties in adapting to dynamic datasets. Modular RAG emerges as a sophisticated evolution, designed to overcome these limitations through modularity and reconfigurability [1]. This approach allows for independent modules to be developed and integrated, providing the flexibility needed to handle complex queries and diverse datasets [2].\n",
            "\n",
            "- **Modular Architecture**: By breaking down RAG systems into discrete modules, Modular RAG enables the seamless integration of advanced technologies such as routing and scheduling mechanisms. This modularity not only enhances flexibility but also simplifies the maintenance and upgrading of RAG systems, making them more adaptable to evolving demands [1].\n",
            "  \n",
            "- **Implementation Patterns**: The identification of prevalent RAG patterns—linear, conditional, branching, and looping—allows for targeted implementation strategies. Each pattern presents unique implementation nuances, which Modular RAG addresses through specialized operators, thereby optimizing the performance of RAG systems across various scenarios [1].\n",
            "\n",
            "#### Addressing Challenges in RAG Systems\n",
            "\n",
            "The transition to Modular RAG is not without its challenges. Ensuring fair ranking and maintaining high generation quality are critical areas that require careful consideration and development [3].\n",
            "\n",
            "- **Fair Ranking Integration**: The integration of fair ranking mechanisms into RAG systems ensures equitable exposure of relevant items, thus promoting fairness and reducing potential biases. This integration is crucial for maintaining system effectiveness while ensuring that all stakeholders are considered [3].\n",
            "  \n",
            "- **Benefit vs. Detriment Trade-Off**: One of the key challenges in RAG systems is balancing the benefit of providing valuable external knowledge with the risk of misleading LLMs through noisy or incorrect texts. A theory-based approach, as proposed in recent studies, offers a framework for understanding and managing this trade-off, which is essential for reliable and consistent improvements in RAG systems [4].\n",
            "\n",
            "#### Practical Implications and Future Directions\n",
            "\n",
            "The modular approach of RAG systems offers significant practical advantages, particularly in production-level applications where flexibility and maintenance are paramount. By facilitating end-to-end training and enabling more complex, integrated workflows, Modular RAG sets the stage for more robust AI applications [2].\n",
            "\n",
            "Moreover, the ongoing research into theoretical frameworks and fair ranking integration highlights the dynamic nature of RAG systems' development. As these areas continue to evolve, Modular RAG is poised to play a pivotal role in shaping the future of AI system architectures, with potential applications spanning various domains from customer service to advanced knowledge systems [1].\n",
            "\n",
            "In conclusion, the shift towards Modular RAG represents a promising advancement in AI technology, offering enhanced capabilities and addressing many of the limitations of traditional RAG systems. However, realizing the full potential of this approach will require continued innovation and research, particularly in areas related to fairness and theoretical foundations.\n",
            "\n",
            "### Sources\n",
            "\n",
            "[1] http://arxiv.org/abs/2407.21059v1  \n",
            "[2] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
            "[3] https://github.com/kimdanny/Fair-RAG  \n",
            "[4] http://arxiv.org/abs/2406.00944v2  \n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mwrite_section\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Enhancing AI Scalability and Efficiency: Modular RAG Systems\n",
            "\n",
            "### Summary\n",
            "\n",
            "In the landscape of artificial intelligence, particularly with the surge of large language models (LLMs), the need for scalable and efficient AI systems has become paramount. Prof. Hinton's focus on the scalability and adaptability of AI systems in real-world environments underscores the relevance of modular Retrieval-Augmented Generation (RAG) systems. These systems integrate information retrieval with language generation, presenting a versatile solution for AI scalability challenges. The insights from recent developments in modular RAG frameworks reveal groundbreaking shifts in how AI systems can be structured for enhanced adaptability and sustainability. \n",
            "\n",
            "The evolving field of RAG systems offers several novel insights:\n",
            "\n",
            "1. **Naive RAG Systems**: Initially combined document retrieval with language generation, laying the groundwork for future advancements. These systems use retrieval models to generate contextually relevant responses but face challenges in handling diverse and dynamic datasets effectively [1].\n",
            "\n",
            "2. **Advanced RAG Techniques**: By fine-tuning embeddings for task-specific semantics, advanced RAG models improve the quality of retrieved information. They utilize dynamic embedding techniques to adaptively adjust during inference, enhancing the relevance and coherence of responses [1].\n",
            "\n",
            "3. **Modular RAG Frameworks**: Introduced as a transformative approach, these frameworks break down complex RAG systems into independent modules, allowing for more flexible and efficient design. This modularity supports scalability and adaptability in meeting the dynamic demands of modern AI applications [2][3][4].\n",
            "\n",
            "4. **Toolkit Developments**: FlashRAG, a modular toolkit, facilitates the standardized implementation and comparison of RAG methods. It addresses the need for a unified framework, offering a customizable environment for researchers to develop and test RAG algorithms [3].\n",
            "\n",
            "5. **Graph-Based RAG Systems**: These systems enhance traditional RAG models by utilizing graph technology to improve knowledge-based question-answering capabilities. This approach mitigates limitations related to real-time data incorporation and contextual understanding [5].\n",
            "\n",
            "6. **Practical Application and Challenges**: The modular approach offers significant flexibility and scalability benefits but also presents challenges in managing diverse data types and maintaining seamless integration within AI systems [6][7].\n",
            "\n",
            "The modular RAG system, with its reconfigurable framework, presents exciting opportunities for the conceptualization and deployment of AI systems. It aligns with Prof. Hinton's vision of sustainable and adaptable AI models, ensuring they can effectively scale and evolve in production environments.\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### 1. Evolution and Limitations of Naive RAG\n",
            "\n",
            "Naive RAG systems formed the foundation for retrieval-augmented AI models by integrating document retrieval with natural language generation. This approach enables AI systems to draw on external information sources, thereby enhancing the contextual relevance of generated responses. However, naive RAG systems face notable limitations, particularly in their handling of dynamic datasets and their inflexibility in adapting to tasks with specific semantic requirements [1][2].\n",
            "\n",
            "- **Inflexibility and Inefficiency**: Traditional RAG models often struggle with inflexibility, making it difficult to efficiently process diverse datasets. This can lead to inefficiencies when systems are tasked with responding to complex or varied queries [1][3].\n",
            "- **Semantic Relevance**: The reliance on static embeddings in naive RAG models can hinder their ability to capture task-specific semantics, impacting the quality of information retrieval and response generation [1].\n",
            "\n",
            "#### 2. Advancements in RAG Techniques\n",
            "\n",
            "Advanced RAG models address many of the limitations inherent in naive systems by employing techniques that enhance semantic understanding and adaptability:\n",
            "\n",
            "- **Dynamic Embedding Techniques**: By fine-tuning embeddings to capture specific domain knowledge, advanced RAG models improve the quality of responses. This allows for more precise retrieval of data relevant to the input query [1].\n",
            "- **Integration with LLMs**: The use of vector databases and advanced retrievers in conjunction with LLMs enables RAG systems to perform precise similarity searches. This integration significantly enhances the accuracy and context-awareness of generated outputs, making these systems more effective for knowledge-intensive applications [2].\n",
            "\n",
            "#### 3. Modular RAG Frameworks\n",
            "\n",
            "The introduction of modular RAG frameworks marks a significant advancement in AI system design. By decomposing complex RAG systems into independent modules, these frameworks offer a highly reconfigurable and scalable architecture [4][5].\n",
            "\n",
            "- **Reconfigurable Design**: Modular RAG frameworks transcend traditional linear architectures by incorporating routing, scheduling, and fusion mechanisms. This flexibility allows for the adaptation of AI systems to a wide range of application scenarios [4].\n",
            "- **Implementation Nuances**: The modular architecture supports various RAG patterns, including linear, conditional, branching, and looping. Each pattern has its own implementation nuances, offering tailored solutions for specific task requirements [4].\n",
            "\n",
            "#### 4. FlashRAG Toolkit\n",
            "\n",
            "The FlashRAG toolkit represents a significant step forward in standardizing RAG research and development. It provides a comprehensive and modular framework that addresses the challenges of implementing and comparing RAG methods in a consistent environment [3].\n",
            "\n",
            "- **Unified Research Platform**: FlashRAG supports the reproduction of existing RAG methods and the development of new algorithms within a cohesive framework. This facilitates methodological development and comparative studies among researchers [3].\n",
            "- **Comprehensive Resources**: The toolkit includes 12 advanced RAG methods and 32 benchmark datasets, offering a rich resource for researchers aiming to optimize their RAG processes [3].\n",
            "\n",
            "#### 5. Graph-Based RAG Systems\n",
            "\n",
            "The integration of graph technology into RAG systems enhances their capability to handle knowledge-intensive tasks more effectively. By leveraging graph structures, these systems can improve the accuracy and reliability of information retrieval and generation processes [5].\n",
            "\n",
            "- **Real-Time Data Integration**: Graph-based RAG systems can incorporate real-time data, addressing limitations related to static knowledge bases and improving contextual understanding [5].\n",
            "- **Example Implementations**: The use of LangGraph to evaluate information reliability and synthesize diverse data showcases the potential for graph-based approaches to enhance RAG systems' performance [5].\n",
            "\n",
            "#### 6. Practical Considerations and Challenges\n",
            "\n",
            "While modular RAG systems offer significant advantages in terms of flexibility and scalability, they also present challenges related to data management and integration:\n",
            "\n",
            "- **Data Diversity**: Managing diverse data types and ensuring seamless integration within the RAG system require sophisticated data processing and retrieval strategies [6][7].\n",
            "- **Implementation Complexity**: The modular approach necessitates careful design and optimization of individual modules to ensure overall system efficiency and effectiveness [6].\n",
            "\n",
            "In summary, the evolution of modular RAG systems is pivotal in enhancing the scalability and adaptability of AI models, aligning with Prof. Hinton's vision of sustainable AI systems in production environments.\n",
            "\n",
            "### Sources\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[3] http://arxiv.org/abs/2405.13576v1  \n",
            "[4] http://arxiv.org/abs/2407.21059v1  \n",
            "[5] http://arxiv.org/abs/2407.19994v3  \n",
            "[6] https://medium.com/aingineer/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e  \n",
            "[7] https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372  \n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mconduct_interview\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Enhancing AI Scalability and Efficiency: Modular RAG Systems\n",
            "\n",
            "### Summary\n",
            "\n",
            "In the landscape of artificial intelligence, particularly with the surge of large language models (LLMs), the need for scalable and efficient AI systems has become paramount. Prof. Hinton's focus on the scalability and adaptability of AI systems in real-world environments underscores the relevance of modular Retrieval-Augmented Generation (RAG) systems. These systems integrate information retrieval with language generation, presenting a versatile solution for AI scalability challenges. The insights from recent developments in modular RAG frameworks reveal groundbreaking shifts in how AI systems can be structured for enhanced adaptability and sustainability. \n",
            "\n",
            "The evolving field of RAG systems offers several novel insights:\n",
            "\n",
            "1. **Naive RAG Systems**: Initially combined document retrieval with language generation, laying the groundwork for future advancements. These systems use retrieval models to generate contextually relevant responses but face challenges in handling diverse and dynamic datasets effectively [1].\n",
            "\n",
            "2. **Advanced RAG Techniques**: By fine-tuning embeddings for task-specific semantics, advanced RAG models improve the quality of retrieved information. They utilize dynamic embedding techniques to adaptively adjust during inference, enhancing the relevance and coherence of responses [1].\n",
            "\n",
            "3. **Modular RAG Frameworks**: Introduced as a transformative approach, these frameworks break down complex RAG systems into independent modules, allowing for more flexible and efficient design. This modularity supports scalability and adaptability in meeting the dynamic demands of modern AI applications [2][3][4].\n",
            "\n",
            "4. **Toolkit Developments**: FlashRAG, a modular toolkit, facilitates the standardized implementation and comparison of RAG methods. It addresses the need for a unified framework, offering a customizable environment for researchers to develop and test RAG algorithms [3].\n",
            "\n",
            "5. **Graph-Based RAG Systems**: These systems enhance traditional RAG models by utilizing graph technology to improve knowledge-based question-answering capabilities. This approach mitigates limitations related to real-time data incorporation and contextual understanding [5].\n",
            "\n",
            "6. **Practical Application and Challenges**: The modular approach offers significant flexibility and scalability benefits but also presents challenges in managing diverse data types and maintaining seamless integration within AI systems [6][7].\n",
            "\n",
            "The modular RAG system, with its reconfigurable framework, presents exciting opportunities for the conceptualization and deployment of AI systems. It aligns with Prof. Hinton's vision of sustainable and adaptable AI models, ensuring they can effectively scale and evolve in production environments.\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### 1. Evolution and Limitations of Naive RAG\n",
            "\n",
            "Naive RAG systems formed the foundation for retrieval-augmented AI models by integrating document retrieval with natural language generation. This approach enables AI systems to draw on external information sources, thereby enhancing the contextual relevance of generated responses. However, naive RAG systems face notable limitations, particularly in their handling of dynamic datasets and their inflexibility in adapting to tasks with specific semantic requirements [1][2].\n",
            "\n",
            "- **Inflexibility and Inefficiency**: Traditional RAG models often struggle with inflexibility, making it difficult to efficiently process diverse datasets. This can lead to inefficiencies when systems are tasked with responding to complex or varied queries [1][3].\n",
            "- **Semantic Relevance**: The reliance on static embeddings in naive RAG models can hinder their ability to capture task-specific semantics, impacting the quality of information retrieval and response generation [1].\n",
            "\n",
            "#### 2. Advancements in RAG Techniques\n",
            "\n",
            "Advanced RAG models address many of the limitations inherent in naive systems by employing techniques that enhance semantic understanding and adaptability:\n",
            "\n",
            "- **Dynamic Embedding Techniques**: By fine-tuning embeddings to capture specific domain knowledge, advanced RAG models improve the quality of responses. This allows for more precise retrieval of data relevant to the input query [1].\n",
            "- **Integration with LLMs**: The use of vector databases and advanced retrievers in conjunction with LLMs enables RAG systems to perform precise similarity searches. This integration significantly enhances the accuracy and context-awareness of generated outputs, making these systems more effective for knowledge-intensive applications [2].\n",
            "\n",
            "#### 3. Modular RAG Frameworks\n",
            "\n",
            "The introduction of modular RAG frameworks marks a significant advancement in AI system design. By decomposing complex RAG systems into independent modules, these frameworks offer a highly reconfigurable and scalable architecture [4][5].\n",
            "\n",
            "- **Reconfigurable Design**: Modular RAG frameworks transcend traditional linear architectures by incorporating routing, scheduling, and fusion mechanisms. This flexibility allows for the adaptation of AI systems to a wide range of application scenarios [4].\n",
            "- **Implementation Nuances**: The modular architecture supports various RAG patterns, including linear, conditional, branching, and looping. Each pattern has its own implementation nuances, offering tailored solutions for specific task requirements [4].\n",
            "\n",
            "#### 4. FlashRAG Toolkit\n",
            "\n",
            "The FlashRAG toolkit represents a significant step forward in standardizing RAG research and development. It provides a comprehensive and modular framework that addresses the challenges of implementing and comparing RAG methods in a consistent environment [3].\n",
            "\n",
            "- **Unified Research Platform**: FlashRAG supports the reproduction of existing RAG methods and the development of new algorithms within a cohesive framework. This facilitates methodological development and comparative studies among researchers [3].\n",
            "- **Comprehensive Resources**: The toolkit includes 12 advanced RAG methods and 32 benchmark datasets, offering a rich resource for researchers aiming to optimize their RAG processes [3].\n",
            "\n",
            "#### 5. Graph-Based RAG Systems\n",
            "\n",
            "The integration of graph technology into RAG systems enhances their capability to handle knowledge-intensive tasks more effectively. By leveraging graph structures, these systems can improve the accuracy and reliability of information retrieval and generation processes [5].\n",
            "\n",
            "- **Real-Time Data Integration**: Graph-based RAG systems can incorporate real-time data, addressing limitations related to static knowledge bases and improving contextual understanding [5].\n",
            "- **Example Implementations**: The use of LangGraph to evaluate information reliability and synthesize diverse data showcases the potential for graph-based approaches to enhance RAG systems' performance [5].\n",
            "\n",
            "#### 6. Practical Considerations and Challenges\n",
            "\n",
            "While modular RAG systems offer significant advantages in terms of flexibility and scalability, they also present challenges related to data management and integration:\n",
            "\n",
            "- **Data Diversity**: Managing diverse data types and ensuring seamless integration within the RAG system require sophisticated data processing and retrieval strategies [6][7].\n",
            "- **Implementation Complexity**: The modular approach necessitates careful design and optimization of individual modules to ensure overall system efficiency and effectiveness [6].\n",
            "\n",
            "In summary, the evolution of modular RAG systems is pivotal in enhancing the scalability and adaptability of AI models, aligning with Prof. Hinton's vision of sustainable AI systems in production environments.\n",
            "\n",
            "### Sources\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[3] http://arxiv.org/abs/2405.13576v1  \n",
            "[4] http://arxiv.org/abs/2407.21059v1  \n",
            "[5] http://arxiv.org/abs/2407.19994v3  \n",
            "[6] https://medium.com/aingineer/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e  \n",
            "[7] https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372  \n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mwrite_conclusion\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32mconclusion\u001b[0m:\n",
            "## Conclusion\n",
            "\n",
            "The evolution from Naive to Modular Retrieval-Augmented Generation (RAG) systems represents a significant leap forward in the scalability and adaptability of artificial intelligence. Naive RAG laid the initial groundwork by integrating document retrieval with language generation, but its limitations in handling dynamic datasets and inflexibility necessitated advancements. Advanced RAG techniques addressed these issues by employing dynamic embeddings and vector databases, enhancing semantic understanding and adaptability.\n",
            "\n",
            "Modular RAG frameworks mark a transformative advancement by decomposing complex systems into independent modules, allowing for a reconfigurable and scalable architecture. This modularity not only supports diverse RAG patterns, such as linear, conditional, branching, and looping, but also facilitates the integration of advanced technologies like routing and scheduling mechanisms. Toolkits like FlashRAG further standardize RAG research, offering a comprehensive environment for algorithm development and comparison.\n",
            "\n",
            "Graph-based RAG systems and theoretical insights into benefit-detriment trade-offs provide additional layers of sophistication, improving real-time data integration and contextual understanding. However, challenges persist, particularly in data management and ensuring fair ranking.\n",
            "\n",
            "Overall, Modular RAG aligns with the vision of creating sustainable and adaptable AI systems, promising significant benefits in production environments. Continued research and innovation in this field are essential to fully realizing its potential and overcoming existing challenges.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mwrite_introduction\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32mintroduction\u001b[0m:\n",
            "# Modular RAG: A Paradigm Shift in AI System Design\n",
            "\n",
            "## Introduction\n",
            "\n",
            "In an era where the demand for scalable, efficient, and adaptable artificial intelligence (AI) systems continues to rise, Modular Retrieval-Augmented Generation (RAG) systems are emerging as a transformative solution. The landscape of AI, particularly with the evolution of large language models (LLMs), is rapidly advancing, necessitating novel approaches to overcome the limitations of traditional systems. This report delves into the significant advancements brought by Modular RAG, contrasting it with Naive RAG systems and exploring its benefits at the production level.\n",
            "\n",
            "We begin by examining the foundational role of Naive RAG systems, which integrate document retrieval with language generation but often falter due to their inflexibility and inefficiency in handling diverse datasets. Advanced RAG techniques address these challenges by leveraging dynamic embeddings and vector databases, enhancing semantic understanding and retrieval accuracy.\n",
            "\n",
            "The heart of our analysis lies in the introduction of Modular RAG frameworks, which decompose complex RAG systems into independent modules. This modularity allows for reconfigurable designs, improving scalability and facilitating the integration of advanced technologies such as routing and scheduling mechanisms. We also explore the FlashRAG toolkit, a modular resource that standardizes RAG research, and the integration of graph-based systems to enhance knowledge-based tasks.\n",
            "\n",
            "Lastly, we discuss the practical implications and challenges of deploying Modular RAG at scale, highlighting its potential to revolutionize AI applications across various domains. Through this exploration, we aim to illustrate how Modular RAG aligns with the vision of sustainable and adaptable AI systems, paving the way for future innovations.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mwrite_report\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32mcontent\u001b[0m:\n",
            "## Insights\n",
            "\n",
            "### Background\n",
            "\n",
            "The emergence of Retrieval-Augmented Generation (RAG) systems has significantly influenced the development of AI, particularly in enhancing the capabilities of large language models (LLMs) for handling complex, knowledge-intensive tasks. Initially, RAG systems were exemplified by Naive RAG, which combined document retrieval with language generation to provide contextually relevant responses. However, these systems often struggled with dynamic datasets and specific semantic requirements, leading to inefficiencies and challenges in scalability [1][2]. The evolution of RAG into Modular RAG represents a groundbreaking shift, introducing a framework that decomposes RAG systems into flexible and independent modules. This modularity is key to enhancing scalability and adaptability, aligning with the vision of sustainable AI systems that can efficiently evolve in production environments [3][4]. The development of toolkits like FlashRAG further supports this evolution by offering a standardized platform for implementing and comparing RAG methods, thereby facilitating research and practical deployment [5].\n",
            "\n",
            "### Related Work\n",
            "\n",
            "Prior studies laid the groundwork for RAG systems, with Naive RAG establishing the foundational integration of retrieval and generation. These early models demonstrated the potential of AI systems to draw on external information sources, yet they faced notable limitations, including inflexibility and inefficiency in processing diverse datasets [1]. Advanced RAG models addressed some of these challenges by employing dynamic embedding techniques and vector databases, which improved the semantic understanding and adaptability of the systems [2]. The introduction of graph-based RAG systems further enhanced these capabilities by leveraging graph structures for more accurate information retrieval and generation [5]. The transition to Modular RAG builds on these advancements by offering a reconfigurable framework that supports various RAG patterns, such as linear, conditional, branching, and looping, each with its specific implementation nuances [4].\n",
            "\n",
            "### Problem Definition\n",
            "\n",
            "The primary challenge addressed by this research is the limitations of traditional Naive RAG systems in adapting to diverse and dynamic datasets. Specifically, these systems struggle with inflexibility, inefficiency, and a lack of semantic relevance, which impact their scalability and effectiveness in real-world applications [1][2][3]. The research aims to explore how Modular RAG can overcome these limitations by providing a flexible and scalable architecture that supports the evolving demands of AI systems. This involves developing modular frameworks that decompose complex RAG systems into independent modules, allowing for more efficient design and maintenance [4][5]. Additionally, the research seeks to address practical challenges related to data management and integration, ensuring that Modular RAG systems can be effectively deployed at the production level [6][7].\n",
            "\n",
            "### Methodology\n",
            "\n",
            "Modular RAG systems are designed by breaking down traditional RAG processes into independent modules and specialized operators, facilitating greater flexibility and adaptability. This approach enables the integration of advanced design features such as routing, scheduling, and fusion mechanisms, which are crucial for handling complex application scenarios [3][4]. The methodology involves implementing various RAG patterns—linear, conditional, branching, and looping—each with its specific nuances, allowing for tailored solutions to specific task requirements [4][5]. The comprehensive framework provided by toolkits like FlashRAG supports the reproduction of existing RAG methods and the development of new algorithms, ensuring consistency and facilitating comparative studies among researchers [5]. Additionally, theoretical frameworks are explored to understand the trade-offs between the benefits and detriments of retrieved texts, offering a structured approach to optimizing RAG performance [4].\n",
            "\n",
            "### Implementation Details\n",
            "\n",
            "The practical implementation of Modular RAG involves utilizing software frameworks and computational resources that support modular architecture. FlashRAG, a modular toolkit, provides a standardized platform for implementing and comparing RAG methods, offering a customizable environment for researchers to develop and test RAG algorithms [5]. The toolkit includes 12 advanced RAG methods and 32 benchmark datasets, enabling researchers to optimize their RAG processes and ensuring consistency in evaluations [3][5]. Additionally, the use of vector databases and dynamic embedding techniques enhances the retrieval and generation processes, making them more context-aware and accurate [2]. The modular architecture also supports end-to-end training across its components, marking a significant advancement over traditional RAG systems [3].\n",
            "\n",
            "### Experiments\n",
            "\n",
            "Experimental protocols for Modular RAG systems involve evaluating their performance across various application scenarios and datasets. The use of comprehensive toolkits like FlashRAG facilitates the implementation of standardized evaluation metrics and procedures, ensuring consistency in testing and comparison [5]. Experiments focus on assessing the scalability, adaptability, and efficiency of Modular RAG systems, particularly in handling diverse and dynamic datasets. Evaluation metrics include measures of retrieval accuracy, response coherence, and system scalability. Additionally, experiments explore the integration of fair ranking mechanisms to ensure equitable exposure of relevant items, thereby promoting fairness and reducing potential biases in RAG systems [3]. Theoretical studies further support experimental findings by modeling the trade-offs between the benefits and detriments of retrieved texts, offering insights into optimizing RAG performance [4].\n",
            "\n",
            "### Results\n",
            "\n",
            "The results of implementing Modular RAG systems demonstrate significant improvements in flexibility, scalability, and efficiency compared to traditional Naive RAG models. Modular RAG's reconfigurable design allows for seamless integration and optimization of independent modules, resulting in enhanced adaptability to diverse application scenarios [4][5]. The use of dynamic embeddings and vector databases further improves retrieval accuracy and context-awareness, making Modular RAG systems more effective for knowledge-intensive applications [2]. The incorporation of graph-based approaches enhances real-time data integration and contextual understanding, addressing limitations related to static knowledge bases [5]. Moreover, the integration of fair ranking mechanisms ensures equitable exposure of relevant items, promoting fairness and reducing biases [3]. Overall, Modular RAG presents a compelling evolution in AI system design, offering a more adaptable and efficient framework for deploying AI solutions at scale.\n",
            "\n",
            "### Sources\n",
            "\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[3] http://arxiv.org/abs/2407.21059v1  \n",
            "[4] http://arxiv.org/abs/2406.00944v2  \n",
            "[5] http://arxiv.org/abs/2405.13576v1  \n",
            "[6] https://medium.com/aingineer/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e  \n",
            "[7] https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372  \n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mfinalize_report\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32mfinal_report\u001b[0m:\n",
            "# Modular RAG: A Paradigm Shift in AI System Design\n",
            "\n",
            "## Introduction\n",
            "\n",
            "In an era where the demand for scalable, efficient, and adaptable artificial intelligence (AI) systems continues to rise, Modular Retrieval-Augmented Generation (RAG) systems are emerging as a transformative solution. The landscape of AI, particularly with the evolution of large language models (LLMs), is rapidly advancing, necessitating novel approaches to overcome the limitations of traditional systems. This report delves into the significant advancements brought by Modular RAG, contrasting it with Naive RAG systems and exploring its benefits at the production level.\n",
            "\n",
            "We begin by examining the foundational role of Naive RAG systems, which integrate document retrieval with language generation but often falter due to their inflexibility and inefficiency in handling diverse datasets. Advanced RAG techniques address these challenges by leveraging dynamic embeddings and vector databases, enhancing semantic understanding and retrieval accuracy.\n",
            "\n",
            "The heart of our analysis lies in the introduction of Modular RAG frameworks, which decompose complex RAG systems into independent modules. This modularity allows for reconfigurable designs, improving scalability and facilitating the integration of advanced technologies such as routing and scheduling mechanisms. We also explore the FlashRAG toolkit, a modular resource that standardizes RAG research, and the integration of graph-based systems to enhance knowledge-based tasks.\n",
            "\n",
            "Lastly, we discuss the practical implications and challenges of deploying Modular RAG at scale, highlighting its potential to revolutionize AI applications across various domains. Through this exploration, we aim to illustrate how Modular RAG aligns with the vision of sustainable and adaptable AI systems, paving the way for future innovations.\n",
            "\n",
            "---\n",
            "\n",
            "## Main Idea\n",
            "\n",
            "\n",
            "\n",
            "### Background\n",
            "\n",
            "The emergence of Retrieval-Augmented Generation (RAG) systems has significantly influenced the development of AI, particularly in enhancing the capabilities of large language models (LLMs) for handling complex, knowledge-intensive tasks. Initially, RAG systems were exemplified by Naive RAG, which combined document retrieval with language generation to provide contextually relevant responses. However, these systems often struggled with dynamic datasets and specific semantic requirements, leading to inefficiencies and challenges in scalability [1][2]. The evolution of RAG into Modular RAG represents a groundbreaking shift, introducing a framework that decomposes RAG systems into flexible and independent modules. This modularity is key to enhancing scalability and adaptability, aligning with the vision of sustainable AI systems that can efficiently evolve in production environments [3][4]. The development of toolkits like FlashRAG further supports this evolution by offering a standardized platform for implementing and comparing RAG methods, thereby facilitating research and practical deployment [5].\n",
            "\n",
            "### Related Work\n",
            "\n",
            "Prior studies laid the groundwork for RAG systems, with Naive RAG establishing the foundational integration of retrieval and generation. These early models demonstrated the potential of AI systems to draw on external information sources, yet they faced notable limitations, including inflexibility and inefficiency in processing diverse datasets [1]. Advanced RAG models addressed some of these challenges by employing dynamic embedding techniques and vector databases, which improved the semantic understanding and adaptability of the systems [2]. The introduction of graph-based RAG systems further enhanced these capabilities by leveraging graph structures for more accurate information retrieval and generation [5]. The transition to Modular RAG builds on these advancements by offering a reconfigurable framework that supports various RAG patterns, such as linear, conditional, branching, and looping, each with its specific implementation nuances [4].\n",
            "\n",
            "### Problem Definition\n",
            "\n",
            "The primary challenge addressed by this research is the limitations of traditional Naive RAG systems in adapting to diverse and dynamic datasets. Specifically, these systems struggle with inflexibility, inefficiency, and a lack of semantic relevance, which impact their scalability and effectiveness in real-world applications [1][2][3]. The research aims to explore how Modular RAG can overcome these limitations by providing a flexible and scalable architecture that supports the evolving demands of AI systems. This involves developing modular frameworks that decompose complex RAG systems into independent modules, allowing for more efficient design and maintenance [4][5]. Additionally, the research seeks to address practical challenges related to data management and integration, ensuring that Modular RAG systems can be effectively deployed at the production level [6][7].\n",
            "\n",
            "### Methodology\n",
            "\n",
            "Modular RAG systems are designed by breaking down traditional RAG processes into independent modules and specialized operators, facilitating greater flexibility and adaptability. This approach enables the integration of advanced design features such as routing, scheduling, and fusion mechanisms, which are crucial for handling complex application scenarios [3][4]. The methodology involves implementing various RAG patterns—linear, conditional, branching, and looping—each with its specific nuances, allowing for tailored solutions to specific task requirements [4][5]. The comprehensive framework provided by toolkits like FlashRAG supports the reproduction of existing RAG methods and the development of new algorithms, ensuring consistency and facilitating comparative studies among researchers [5]. Additionally, theoretical frameworks are explored to understand the trade-offs between the benefits and detriments of retrieved texts, offering a structured approach to optimizing RAG performance [4].\n",
            "\n",
            "### Implementation Details\n",
            "\n",
            "The practical implementation of Modular RAG involves utilizing software frameworks and computational resources that support modular architecture. FlashRAG, a modular toolkit, provides a standardized platform for implementing and comparing RAG methods, offering a customizable environment for researchers to develop and test RAG algorithms [5]. The toolkit includes 12 advanced RAG methods and 32 benchmark datasets, enabling researchers to optimize their RAG processes and ensuring consistency in evaluations [3][5]. Additionally, the use of vector databases and dynamic embedding techniques enhances the retrieval and generation processes, making them more context-aware and accurate [2]. The modular architecture also supports end-to-end training across its components, marking a significant advancement over traditional RAG systems [3].\n",
            "\n",
            "### Experiments\n",
            "\n",
            "Experimental protocols for Modular RAG systems involve evaluating their performance across various application scenarios and datasets. The use of comprehensive toolkits like FlashRAG facilitates the implementation of standardized evaluation metrics and procedures, ensuring consistency in testing and comparison [5]. Experiments focus on assessing the scalability, adaptability, and efficiency of Modular RAG systems, particularly in handling diverse and dynamic datasets. Evaluation metrics include measures of retrieval accuracy, response coherence, and system scalability. Additionally, experiments explore the integration of fair ranking mechanisms to ensure equitable exposure of relevant items, thereby promoting fairness and reducing potential biases in RAG systems [3]. Theoretical studies further support experimental findings by modeling the trade-offs between the benefits and detriments of retrieved texts, offering insights into optimizing RAG performance [4].\n",
            "\n",
            "### Results\n",
            "\n",
            "The results of implementing Modular RAG systems demonstrate significant improvements in flexibility, scalability, and efficiency compared to traditional Naive RAG models. Modular RAG's reconfigurable design allows for seamless integration and optimization of independent modules, resulting in enhanced adaptability to diverse application scenarios [4][5]. The use of dynamic embeddings and vector databases further improves retrieval accuracy and context-awareness, making Modular RAG systems more effective for knowledge-intensive applications [2]. The incorporation of graph-based approaches enhances real-time data integration and contextual understanding, addressing limitations related to static knowledge bases [5]. Moreover, the integration of fair ranking mechanisms ensures equitable exposure of relevant items, promoting fairness and reducing biases [3]. Overall, Modular RAG presents a compelling evolution in AI system design, offering a more adaptable and efficient framework for deploying AI solutions at scale.\n",
            "\n",
            "### Sources\n",
            "\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[3] http://arxiv.org/abs/2407.21059v1  \n",
            "[4] http://arxiv.org/abs/2406.00944v2  \n",
            "[5] http://arxiv.org/abs/2405.13576v1  \n",
            "[6] https://medium.com/aingineer/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e  \n",
            "[7] https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372\n",
            "\n",
            "---\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "The evolution from Naive to Modular Retrieval-Augmented Generation (RAG) systems represents a significant leap forward in the scalability and adaptability of artificial intelligence. Naive RAG laid the initial groundwork by integrating document retrieval with language generation, but its limitations in handling dynamic datasets and inflexibility necessitated advancements. Advanced RAG techniques addressed these issues by employing dynamic embeddings and vector databases, enhancing semantic understanding and adaptability.\n",
            "\n",
            "Modular RAG frameworks mark a transformative advancement by decomposing complex systems into independent modules, allowing for a reconfigurable and scalable architecture. This modularity not only supports diverse RAG patterns, such as linear, conditional, branching, and looping, but also facilitates the integration of advanced technologies like routing and scheduling mechanisms. Toolkits like FlashRAG further standardize RAG research, offering a comprehensive environment for algorithm development and comparison.\n",
            "\n",
            "Graph-based RAG systems and theoretical insights into benefit-detriment trade-offs provide additional layers of sophistication, improving real-time data integration and contextual understanding. However, challenges persist, particularly in data management and ensuring fair ranking.\n",
            "\n",
            "Overall, Modular RAG aligns with the vision of creating sustainable and adaptable AI systems, promising significant benefits in production environments. Continued research and innovation in this field are essential to fully realizing its potential and overcoming existing challenges.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Resume graph execution\n",
        "invoke_graph(graph, None, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f589f94",
      "metadata": {},
      "source": [
        "Here's how to display the final research report: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "f076b709",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "# Modular RAG: A Paradigm Shift in AI System Design\n",
              "\n",
              "## Introduction\n",
              "\n",
              "In an era where the demand for scalable, efficient, and adaptable artificial intelligence (AI) systems continues to rise, Modular Retrieval-Augmented Generation (RAG) systems are emerging as a transformative solution. The landscape of AI, particularly with the evolution of large language models (LLMs), is rapidly advancing, necessitating novel approaches to overcome the limitations of traditional systems. This report delves into the significant advancements brought by Modular RAG, contrasting it with Naive RAG systems and exploring its benefits at the production level.\n",
              "\n",
              "We begin by examining the foundational role of Naive RAG systems, which integrate document retrieval with language generation but often falter due to their inflexibility and inefficiency in handling diverse datasets. Advanced RAG techniques address these challenges by leveraging dynamic embeddings and vector databases, enhancing semantic understanding and retrieval accuracy.\n",
              "\n",
              "The heart of our analysis lies in the introduction of Modular RAG frameworks, which decompose complex RAG systems into independent modules. This modularity allows for reconfigurable designs, improving scalability and facilitating the integration of advanced technologies such as routing and scheduling mechanisms. We also explore the FlashRAG toolkit, a modular resource that standardizes RAG research, and the integration of graph-based systems to enhance knowledge-based tasks.\n",
              "\n",
              "Lastly, we discuss the practical implications and challenges of deploying Modular RAG at scale, highlighting its potential to revolutionize AI applications across various domains. Through this exploration, we aim to illustrate how Modular RAG aligns with the vision of sustainable and adaptable AI systems, paving the way for future innovations.\n",
              "\n",
              "---\n",
              "\n",
              "## Main Idea\n",
              "\n",
              "\n",
              "\n",
              "### Background\n",
              "\n",
              "The emergence of Retrieval-Augmented Generation (RAG) systems has significantly influenced the development of AI, particularly in enhancing the capabilities of large language models (LLMs) for handling complex, knowledge-intensive tasks. Initially, RAG systems were exemplified by Naive RAG, which combined document retrieval with language generation to provide contextually relevant responses. However, these systems often struggled with dynamic datasets and specific semantic requirements, leading to inefficiencies and challenges in scalability [1][2]. The evolution of RAG into Modular RAG represents a groundbreaking shift, introducing a framework that decomposes RAG systems into flexible and independent modules. This modularity is key to enhancing scalability and adaptability, aligning with the vision of sustainable AI systems that can efficiently evolve in production environments [3][4]. The development of toolkits like FlashRAG further supports this evolution by offering a standardized platform for implementing and comparing RAG methods, thereby facilitating research and practical deployment [5].\n",
              "\n",
              "### Related Work\n",
              "\n",
              "Prior studies laid the groundwork for RAG systems, with Naive RAG establishing the foundational integration of retrieval and generation. These early models demonstrated the potential of AI systems to draw on external information sources, yet they faced notable limitations, including inflexibility and inefficiency in processing diverse datasets [1]. Advanced RAG models addressed some of these challenges by employing dynamic embedding techniques and vector databases, which improved the semantic understanding and adaptability of the systems [2]. The introduction of graph-based RAG systems further enhanced these capabilities by leveraging graph structures for more accurate information retrieval and generation [5]. The transition to Modular RAG builds on these advancements by offering a reconfigurable framework that supports various RAG patterns, such as linear, conditional, branching, and looping, each with its specific implementation nuances [4].\n",
              "\n",
              "### Problem Definition\n",
              "\n",
              "The primary challenge addressed by this research is the limitations of traditional Naive RAG systems in adapting to diverse and dynamic datasets. Specifically, these systems struggle with inflexibility, inefficiency, and a lack of semantic relevance, which impact their scalability and effectiveness in real-world applications [1][2][3]. The research aims to explore how Modular RAG can overcome these limitations by providing a flexible and scalable architecture that supports the evolving demands of AI systems. This involves developing modular frameworks that decompose complex RAG systems into independent modules, allowing for more efficient design and maintenance [4][5]. Additionally, the research seeks to address practical challenges related to data management and integration, ensuring that Modular RAG systems can be effectively deployed at the production level [6][7].\n",
              "\n",
              "### Methodology\n",
              "\n",
              "Modular RAG systems are designed by breaking down traditional RAG processes into independent modules and specialized operators, facilitating greater flexibility and adaptability. This approach enables the integration of advanced design features such as routing, scheduling, and fusion mechanisms, which are crucial for handling complex application scenarios [3][4]. The methodology involves implementing various RAG patterns—linear, conditional, branching, and looping—each with its specific nuances, allowing for tailored solutions to specific task requirements [4][5]. The comprehensive framework provided by toolkits like FlashRAG supports the reproduction of existing RAG methods and the development of new algorithms, ensuring consistency and facilitating comparative studies among researchers [5]. Additionally, theoretical frameworks are explored to understand the trade-offs between the benefits and detriments of retrieved texts, offering a structured approach to optimizing RAG performance [4].\n",
              "\n",
              "### Implementation Details\n",
              "\n",
              "The practical implementation of Modular RAG involves utilizing software frameworks and computational resources that support modular architecture. FlashRAG, a modular toolkit, provides a standardized platform for implementing and comparing RAG methods, offering a customizable environment for researchers to develop and test RAG algorithms [5]. The toolkit includes 12 advanced RAG methods and 32 benchmark datasets, enabling researchers to optimize their RAG processes and ensuring consistency in evaluations [3][5]. Additionally, the use of vector databases and dynamic embedding techniques enhances the retrieval and generation processes, making them more context-aware and accurate [2]. The modular architecture also supports end-to-end training across its components, marking a significant advancement over traditional RAG systems [3].\n",
              "\n",
              "### Experiments\n",
              "\n",
              "Experimental protocols for Modular RAG systems involve evaluating their performance across various application scenarios and datasets. The use of comprehensive toolkits like FlashRAG facilitates the implementation of standardized evaluation metrics and procedures, ensuring consistency in testing and comparison [5]. Experiments focus on assessing the scalability, adaptability, and efficiency of Modular RAG systems, particularly in handling diverse and dynamic datasets. Evaluation metrics include measures of retrieval accuracy, response coherence, and system scalability. Additionally, experiments explore the integration of fair ranking mechanisms to ensure equitable exposure of relevant items, thereby promoting fairness and reducing potential biases in RAG systems [3]. Theoretical studies further support experimental findings by modeling the trade-offs between the benefits and detriments of retrieved texts, offering insights into optimizing RAG performance [4].\n",
              "\n",
              "### Results\n",
              "\n",
              "The results of implementing Modular RAG systems demonstrate significant improvements in flexibility, scalability, and efficiency compared to traditional Naive RAG models. Modular RAG's reconfigurable design allows for seamless integration and optimization of independent modules, resulting in enhanced adaptability to diverse application scenarios [4][5]. The use of dynamic embeddings and vector databases further improves retrieval accuracy and context-awareness, making Modular RAG systems more effective for knowledge-intensive applications [2]. The incorporation of graph-based approaches enhances real-time data integration and contextual understanding, addressing limitations related to static knowledge bases [5]. Moreover, the integration of fair ranking mechanisms ensures equitable exposure of relevant items, promoting fairness and reducing biases [3]. Overall, Modular RAG presents a compelling evolution in AI system design, offering a more adaptable and efficient framework for deploying AI solutions at scale.\n",
              "\n",
              "### Sources\n",
              "\n",
              "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
              "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
              "[3] http://arxiv.org/abs/2407.21059v1  \n",
              "[4] http://arxiv.org/abs/2406.00944v2  \n",
              "[5] http://arxiv.org/abs/2405.13576v1  \n",
              "[6] https://medium.com/aingineer/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e  \n",
              "[7] https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372\n",
              "\n",
              "---\n",
              "\n",
              "## Conclusion\n",
              "\n",
              "The evolution from Naive to Modular Retrieval-Augmented Generation (RAG) systems represents a significant leap forward in the scalability and adaptability of artificial intelligence. Naive RAG laid the initial groundwork by integrating document retrieval with language generation, but its limitations in handling dynamic datasets and inflexibility necessitated advancements. Advanced RAG techniques addressed these issues by employing dynamic embeddings and vector databases, enhancing semantic understanding and adaptability.\n",
              "\n",
              "Modular RAG frameworks mark a transformative advancement by decomposing complex systems into independent modules, allowing for a reconfigurable and scalable architecture. This modularity not only supports diverse RAG patterns, such as linear, conditional, branching, and looping, but also facilitates the integration of advanced technologies like routing and scheduling mechanisms. Toolkits like FlashRAG further standardize RAG research, offering a comprehensive environment for algorithm development and comparison.\n",
              "\n",
              "Graph-based RAG systems and theoretical insights into benefit-detriment trade-offs provide additional layers of sophistication, improving real-time data integration and contextual understanding. However, challenges persist, particularly in data management and ensuring fair ranking.\n",
              "\n",
              "Overall, Modular RAG aligns with the vision of creating sustainable and adaptable AI systems, promising significant benefits in production environments. Continued research and innovation in this field are essential to fully realizing its potential and overcoming existing challenges."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "# Get final graph state\n",
        "final_state = graph.get_state(config)\n",
        "\n",
        "# Retrieve final report\n",
        "report = final_state.values.get(\"final_report\")\n",
        "\n",
        "# Display report in markdown format\n",
        "display(Markdown(report))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "d81f0bfe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Modular RAG: A Paradigm Shift in AI System Design\n",
            "\n",
            "## Introduction\n",
            "\n",
            "In an era where the demand for scalable, efficient, and adaptable artificial intelligence (AI) systems continues to rise, Modular Retrieval-Augmented Generation (RAG) systems are emerging as a transformative solution. The landscape of AI, particularly with the evolution of large language models (LLMs), is rapidly advancing, necessitating novel approaches to overcome the limitations of traditional systems. This report delves into the significant advancements brought by Modular RAG, contrasting it with Naive RAG systems and exploring its benefits at the production level.\n",
            "\n",
            "We begin by examining the foundational role of Naive RAG systems, which integrate document retrieval with language generation but often falter due to their inflexibility and inefficiency in handling diverse datasets. Advanced RAG techniques address these challenges by leveraging dynamic embeddings and vector databases, enhancing semantic understanding and retrieval accuracy.\n",
            "\n",
            "The heart of our analysis lies in the introduction of Modular RAG frameworks, which decompose complex RAG systems into independent modules. This modularity allows for reconfigurable designs, improving scalability and facilitating the integration of advanced technologies such as routing and scheduling mechanisms. We also explore the FlashRAG toolkit, a modular resource that standardizes RAG research, and the integration of graph-based systems to enhance knowledge-based tasks.\n",
            "\n",
            "Lastly, we discuss the practical implications and challenges of deploying Modular RAG at scale, highlighting its potential to revolutionize AI applications across various domains. Through this exploration, we aim to illustrate how Modular RAG aligns with the vision of sustainable and adaptable AI systems, paving the way for future innovations.\n",
            "\n",
            "---\n",
            "\n",
            "## Main Idea\n",
            "\n",
            "\n",
            "\n",
            "### Background\n",
            "\n",
            "The emergence of Retrieval-Augmented Generation (RAG) systems has significantly influenced the development of AI, particularly in enhancing the capabilities of large language models (LLMs) for handling complex, knowledge-intensive tasks. Initially, RAG systems were exemplified by Naive RAG, which combined document retrieval with language generation to provide contextually relevant responses. However, these systems often struggled with dynamic datasets and specific semantic requirements, leading to inefficiencies and challenges in scalability [1][2]. The evolution of RAG into Modular RAG represents a groundbreaking shift, introducing a framework that decomposes RAG systems into flexible and independent modules. This modularity is key to enhancing scalability and adaptability, aligning with the vision of sustainable AI systems that can efficiently evolve in production environments [3][4]. The development of toolkits like FlashRAG further supports this evolution by offering a standardized platform for implementing and comparing RAG methods, thereby facilitating research and practical deployment [5].\n",
            "\n",
            "### Related Work\n",
            "\n",
            "Prior studies laid the groundwork for RAG systems, with Naive RAG establishing the foundational integration of retrieval and generation. These early models demonstrated the potential of AI systems to draw on external information sources, yet they faced notable limitations, including inflexibility and inefficiency in processing diverse datasets [1]. Advanced RAG models addressed some of these challenges by employing dynamic embedding techniques and vector databases, which improved the semantic understanding and adaptability of the systems [2]. The introduction of graph-based RAG systems further enhanced these capabilities by leveraging graph structures for more accurate information retrieval and generation [5]. The transition to Modular RAG builds on these advancements by offering a reconfigurable framework that supports various RAG patterns, such as linear, conditional, branching, and looping, each with its specific implementation nuances [4].\n",
            "\n",
            "### Problem Definition\n",
            "\n",
            "The primary challenge addressed by this research is the limitations of traditional Naive RAG systems in adapting to diverse and dynamic datasets. Specifically, these systems struggle with inflexibility, inefficiency, and a lack of semantic relevance, which impact their scalability and effectiveness in real-world applications [1][2][3]. The research aims to explore how Modular RAG can overcome these limitations by providing a flexible and scalable architecture that supports the evolving demands of AI systems. This involves developing modular frameworks that decompose complex RAG systems into independent modules, allowing for more efficient design and maintenance [4][5]. Additionally, the research seeks to address practical challenges related to data management and integration, ensuring that Modular RAG systems can be effectively deployed at the production level [6][7].\n",
            "\n",
            "### Methodology\n",
            "\n",
            "Modular RAG systems are designed by breaking down traditional RAG processes into independent modules and specialized operators, facilitating greater flexibility and adaptability. This approach enables the integration of advanced design features such as routing, scheduling, and fusion mechanisms, which are crucial for handling complex application scenarios [3][4]. The methodology involves implementing various RAG patterns—linear, conditional, branching, and looping—each with its specific nuances, allowing for tailored solutions to specific task requirements [4][5]. The comprehensive framework provided by toolkits like FlashRAG supports the reproduction of existing RAG methods and the development of new algorithms, ensuring consistency and facilitating comparative studies among researchers [5]. Additionally, theoretical frameworks are explored to understand the trade-offs between the benefits and detriments of retrieved texts, offering a structured approach to optimizing RAG performance [4].\n",
            "\n",
            "### Implementation Details\n",
            "\n",
            "The practical implementation of Modular RAG involves utilizing software frameworks and computational resources that support modular architecture. FlashRAG, a modular toolkit, provides a standardized platform for implementing and comparing RAG methods, offering a customizable environment for researchers to develop and test RAG algorithms [5]. The toolkit includes 12 advanced RAG methods and 32 benchmark datasets, enabling researchers to optimize their RAG processes and ensuring consistency in evaluations [3][5]. Additionally, the use of vector databases and dynamic embedding techniques enhances the retrieval and generation processes, making them more context-aware and accurate [2]. The modular architecture also supports end-to-end training across its components, marking a significant advancement over traditional RAG systems [3].\n",
            "\n",
            "### Experiments\n",
            "\n",
            "Experimental protocols for Modular RAG systems involve evaluating their performance across various application scenarios and datasets. The use of comprehensive toolkits like FlashRAG facilitates the implementation of standardized evaluation metrics and procedures, ensuring consistency in testing and comparison [5]. Experiments focus on assessing the scalability, adaptability, and efficiency of Modular RAG systems, particularly in handling diverse and dynamic datasets. Evaluation metrics include measures of retrieval accuracy, response coherence, and system scalability. Additionally, experiments explore the integration of fair ranking mechanisms to ensure equitable exposure of relevant items, thereby promoting fairness and reducing potential biases in RAG systems [3]. Theoretical studies further support experimental findings by modeling the trade-offs between the benefits and detriments of retrieved texts, offering insights into optimizing RAG performance [4].\n",
            "\n",
            "### Results\n",
            "\n",
            "The results of implementing Modular RAG systems demonstrate significant improvements in flexibility, scalability, and efficiency compared to traditional Naive RAG models. Modular RAG's reconfigurable design allows for seamless integration and optimization of independent modules, resulting in enhanced adaptability to diverse application scenarios [4][5]. The use of dynamic embeddings and vector databases further improves retrieval accuracy and context-awareness, making Modular RAG systems more effective for knowledge-intensive applications [2]. The incorporation of graph-based approaches enhances real-time data integration and contextual understanding, addressing limitations related to static knowledge bases [5]. Moreover, the integration of fair ranking mechanisms ensures equitable exposure of relevant items, promoting fairness and reducing biases [3]. Overall, Modular RAG presents a compelling evolution in AI system design, offering a more adaptable and efficient framework for deploying AI solutions at scale.\n",
            "\n",
            "### Sources\n",
            "\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[3] http://arxiv.org/abs/2407.21059v1  \n",
            "[4] http://arxiv.org/abs/2406.00944v2  \n",
            "[5] http://arxiv.org/abs/2405.13576v1  \n",
            "[6] https://medium.com/aingineer/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e  \n",
            "[7] https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372\n",
            "\n",
            "---\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "The evolution from Naive to Modular Retrieval-Augmented Generation (RAG) systems represents a significant leap forward in the scalability and adaptability of artificial intelligence. Naive RAG laid the initial groundwork by integrating document retrieval with language generation, but its limitations in handling dynamic datasets and inflexibility necessitated advancements. Advanced RAG techniques addressed these issues by employing dynamic embeddings and vector databases, enhancing semantic understanding and adaptability.\n",
            "\n",
            "Modular RAG frameworks mark a transformative advancement by decomposing complex systems into independent modules, allowing for a reconfigurable and scalable architecture. This modularity not only supports diverse RAG patterns, such as linear, conditional, branching, and looping, but also facilitates the integration of advanced technologies like routing and scheduling mechanisms. Toolkits like FlashRAG further standardize RAG research, offering a comprehensive environment for algorithm development and comparison.\n",
            "\n",
            "Graph-based RAG systems and theoretical insights into benefit-detriment trade-offs provide additional layers of sophistication, improving real-time data integration and contextual understanding. However, challenges persist, particularly in data management and ensuring fair ranking.\n",
            "\n",
            "Overall, Modular RAG aligns with the vision of creating sustainable and adaptable AI systems, promising significant benefits in production environments. Continued research and innovation in this field are essential to fully realizing its potential and overcoming existing challenges.\n"
          ]
        }
      ],
      "source": [
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-opentutorial-A2cWC-y3-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
